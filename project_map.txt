# Project Directory Map: Thesis


================================================================================
## Folder: /
------------------------------------------------------------
### Other Files:
- linux_commands.txt
- requirements.txt


================================================================================
## Folder: laboratory
------------------------------------------------------------
### Python Source Files:

#### File: `bifurcation_analysis.py`
```python
import numpy as np
import jax
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import copy
from src.hr_model.model import HindmarshRose, DEFAULT_PARAMS, DEFAULT_STATE0
from scipy.signal import find_peaks
import multiprocessing as mp
from functools import partial


# ==============================================================================
# WORKER FUNCTION (for multiprocessing)
# ==============================================================================
def run_one_simulation(current_bif_value, model_instance, initial_state,
                       bifurcation_param_name, start_time, end_time, dt_initial,
                       n_points, max_steps, solver, stepsize_controller,
                       transient_fraction, positive_peaks_only):
    """
    Runs a single simulation for one specific bifurcation parameter value.
    This function is designed to be called by a multiprocessing worker.

    Returns:
        tuple: (parameter_value, list_of_peaks)
    """
    # --- Setup Instance ---
    # Create a deep copy to ensure each process has its own model instance
    current_instance = copy.deepcopy(model_instance)

    # --- FIX IS HERE ---
    # Modify the parameter inside the 'params' dictionary directly.
    current_instance.params[bifurcation_param_name] = current_bif_value

    current_instance.initial_state = np.array(initial_state, dtype=np.float64)

    # --- Run Simulation ---
    current_instance.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    # --- Process Results ---
    if current_instance.failed:
        return (current_bif_value, [np.nan])

    results = current_instance.get_results_dict(transient_fraction)
    x_curve = results['x1']

    # Find peaks in the steady-state portion
    peak_idx, _ = find_peaks(x_curve, height=0 if positive_peaks_only else None)
    peaks = x_curve[peak_idx]

    # Mask out negative peaks if requested
    if positive_peaks_only:
        positive_mask = peaks > 0
        peaks = peaks[positive_mask]

    # If no peaks are found, return NaN to maintain array structure
    if peaks.size == 0:
        peaks = np.array([np.nan])

    return (current_bif_value, peaks.tolist())


# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
def main():
    """
    Main function to set up and run the parallel bifurcation analysis.
    """
    # --- Simulation Parameters ---
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['rho'] = 0.7
    sim_params['m'] = 1
    model_instance = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)
    bifurcation_param_name = 'k'
    param_range = (-1.3, 0.6, 400)  # Increased steps to show benefit of parallelization

    # --- Integration Settings ---
    start_time = 0
    end_time = 2000
    dt_initial = 0.05
    n_points = int(end_time / dt_initial)
    transient_fraction = 0.5
    max_steps = int((end_time - start_time) / dt_initial) * 20
    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)

    # --- Parallel Execution ---
    param_start, param_end, param_steps = param_range
    bifurcation_values = np.linspace(param_start, param_end, int(param_steps))
    MAX_WORKERS = mp.cpu_count()
    # MAX_WORKERS = 8

    print(f"Starting parallel bifurcation analysis for '{bifurcation_param_name}'...")
    print(f"Running {param_steps} simulations on {MAX_WORKERS} cores.")

    # Use functools.partial to "freeze" the arguments that are the same for all simulations
    worker_func = partial(run_one_simulation,
                          model_instance=model_instance,
                          initial_state=DEFAULT_STATE0,
                          bifurcation_param_name=bifurcation_param_name,
                          start_time=start_time,
                          end_time=end_time,
                          dt_initial=dt_initial,
                          n_points=n_points,
                          max_steps=max_steps,
                          solver=solver,
                          stepsize_controller=stepsize_controller,
                          transient_fraction=transient_fraction,
                          positive_peaks_only=True)

    # Create a pool of worker processes and map the tasks

    with mp.Pool(processes=MAX_WORKERS) as pool:
        # pool.map will distribute bifurcation_values among the workers
        # and collect the results in a list once all are complete.
        results = pool.map(worker_func, bifurcation_values)

    print("All simulations finished. Processing results...")

    # --- Aggregate Results ---
    all_param_values = []
    all_peak_values = []
    for param_val, peaks in results:
        all_param_values.extend([param_val] * len(peaks))
        all_peak_values.extend(peaks)

    # --- Plotting ---
    from visualization.plotting import plot_bifurcation_diagram

    plot_bifurcation_diagram(
        param_values=all_param_values,
        peak_values=all_peak_values,
        bifurcation_param_name=bifurcation_param_name,
        title="Bifurcation Diagram",
        xlabel=None,
        ylabel=r'$x_{max}$',
        marker='.',
        s=2,
        save_fig=1)


    # # --- Optional: Save Data ---
    # import os
    # path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'Bifurcation/')
    # os.makedirs(path, exist_ok=True)
    #
    # gen_data = np.vstack([all_param_values, all_peak_values]).T
    # np.save(
    #     path + 'Bif_'+bifurcation_param_name+'_k_'+str(sim_params['k'])+'_m_'+str(sim_params['m']),
    #     gen_data)


# This check is essential for multiprocessing to work correctly on all platforms
if __name__ == "__main__":
    # On Windows, the 'spawn' start method is the default and safest.
    # Explicitly setting it prevents issues on systems where 'fork' is the default.
    mp.set_start_method("spawn", force=True)
    main()

```

#### File: `generate_data_for_PINN.py`
```python
import jax
import jax.numpy as jnp
import diffrax as dfx
import pickle
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.error_system import DEFAULT_HR_STATES0
from src.hr_model.physics import calculate_H
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS

def generate_data(
    num_runs: int,
    dynamics: str = 'complete',
    param_dict: dict = DEFAULT_PARAMS,
    initial_state_range: tuple = (-2, 2),
    seed: int = None,
    start_time: float = 0.0,
    end_time: float = 1000.0,
    dt_initial: float = 0.01,
    n_points: int = 10000,
    transient_ratio: float = 0,
    max_steps: int = None,
    solver = dfx.Tsit5(),
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12),
    I_ext = [0.8, 0.8],
    xi = [[0, 1], [1, 0]],
    output_file: str = "error_system_data.pkl"
):
    """
    Generate data by running the Error_System with multiple random initial conditions.
    For each run, compute the Hamiltonian, its time derivative, and the Lyapunov derivative.

    Parameters:
    - num_runs: Number of different initial conditions to simulate.
    - dynamics: 'complete' or 'simplified' dynamics for the error system.
    - param_dict: Dictionary of model parameters.
    - initial_state_range: Tuple (low, high) for uniform random initial states.
    - seed: Random seed for reproducibility.
    - start_time, end_time: Simulation time span.
    - dt_initial: Initial time step for the solver.
    - n_points: Number of points to save in the time series.
    - transient_ratio: Fraction of initial time series to discard as transient.
    - max_steps: Maximum number of steps for the solver.
    - solver: Diffrax solver to use.
    - stepsize_controller: Controller for adaptive step sizing.
    - I_ext: External currents for the HR neurons.
    - xi: Coupling matrix for the HR neurons.
    - output_file: File path to save the results.

    Returns:
    - Saves a list of result dictionaries to the specified output file.
    """
    # Set random seed
    if seed is not None:
        key = jax.random.PRNGKey(seed)
    else:
        key = jax.random.PRNGKey(0)  # Default seed

    # Generate keys for each run
    keys = jax.random.split(key, num_runs)

    # Prepare list to store results
    all_results = []

    # Compute max_steps if not provided
    if max_steps is None:
        max_steps = int((end_time - start_time) / dt_initial) * 20

    # Time points for saving
    t_save = jnp.linspace(start_time, end_time, n_points)

    for i in range(num_runs):
        print(f"Running simulation {i+1}/{num_runs}...")

        # Generate random hr_initial_state
        low, high = initial_state_range
        hr_initial_state = jax.random.uniform(keys[i], shape=(10,), minval=low, maxval=high)

        # Create simulator instance
        simulator = HRNetworkErrorSystem(
            params=param_dict,
            dynamics=dynamics,
            hr_initial_state=hr_initial_state,
            I_ext=I_ext,
            hr_xi=xi
        )

        # Run simulation
        simulator.solve(
            solver=solver,
            t0=start_time,
            t1=end_time,
            dt0=dt_initial,
            n_points=dfx.SaveAt(ts=t_save, dense=True),
            stepsize_controller=stepsize_controller,
            max_steps=max_steps
        )

        if not simulator.failed:
            results = simulator.get_results_dict(transient_ratio)

            # Compute Hamiltonian, dHdt, dVdt
            H = calculate_H(results, param_dict)
            dHdt = calculate_dHdt(results, param_dict)
            dVdt = calculate_dVdt(results, param_dict)

            # Add to results
            results['Hamiltonian'] = H
            results['dHdt'] = dHdt
            results['dVdt'] = dVdt

            # Also store initial state
            results['initial_state'] = hr_initial_state

            all_results.append(results)
        else:
            print(f"Simulation {i+1} failed.")

    # Save all results to file
    with open(output_file, 'wb') as f:
        pickle.dump(all_results, f)

    print(f"Data generation complete. Results saved to {output_file}")



if __name__ == '__main__':
    from visualization.plotting import plot_pinn_data

    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.62

    import os
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'PINN Data/')
    os.makedirs(output_dir, exist_ok=True)

    output_file = os.path.join(output_dir, f'error_system_data.pkl')
    # Generate data for 5 runs
    generate_data(num_runs=1,
                  dynamics = 'complete',
                  param_dict = sim_params,
                  initial_state_range = (-0.1, 0.1),
                  end_time = 1000,
                  n_points = 100000,
                  seed=42,
                  output_file=output_file
                  )

    # Load the generated data
    with open(output_file, 'rb') as f:
        all_results = pickle.load(f)
    print("Saved on file")

    # Use the imported plotting function to visualize the results
    plot_pinn_data(all_results, save_fig=True)

    print(f"Generated 5 separate plots, each with 8 subplots, saved as 'run_<number>_plots.png'")

```

#### File: `run_loop.py`
```python
import subprocess, time, sys, pathlib

CMD = [sys.executable, "v_dot_parameter_sweep_2D.py"]
# CMD = [r"C:\Virtual Environments\Neuron\.venv\Scripts\python.exe", "v_dot_parameter_sweep_2D.py"]


while True:
    print("\n=== Starting simulation at", time.ctime())
    exit_code = subprocess.call(CMD)
    if exit_code == 0:
        print("=== Simulation finished successfully!")
        break
    print(f"*** Child exited with code {exit_code}.  Restarting in 15 s ‚Ä¶")
    time.sleep(5)

```

#### File: `v_dot_parameter_sweep_1D.py`
```python

"""
v_dot_parameter_sweep_1D.py
------------------------------------------
Sweep a chosen Hindmarsh-Rose model parameter, simulate for each value,
compute the mean post-transient dV/dt, and plot the result ‚Ä¶ in parallel.
"""

from __future__ import annotations
import os
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import multiprocessing as mp
# ‚îÄ‚îÄ extra imports (2-D features) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import gc
from typing import Any
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONFIGURATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TARGET_PARAM = "k"  # name of the parameter to sweep
PARAM_MIN, PARAM_MAX = -2.5, 2
NUM_PARAM_POINTS = 5
PARAM_VALUES = jnp.linspace(PARAM_MIN, PARAM_MAX, NUM_PARAM_POINTS)

# ‚îÄ‚îÄ extra config copied from 2-D script ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CORE_NUM   = 5          # number of worker processes
BATCH_SIZE = 5          # params per checkpoint-batch
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# integration settings
START_TIME = 0
END_TIME = 250
DT_INITIAL = 0.01
POINT_NUM = 250
TRANSIENT_RATIO = 0.75
N_POINTS = dfx.SaveAt(ts=jnp.linspace(START_TIME, END_TIME, POINT_NUM), dense=True)
MAX_STEPS = int((END_TIME - START_TIME) / DT_INITIAL) * 20
SOLVER = dfx.Tsit5()
STEPSIZE_CONTROLLER = dfx.PIDController(rtol=1e-10, atol=1e-12)
# stepsize_controller = dfx.ConstantStepSize()

# Initial conditions (10 HR-state variables + 5 error-state variables)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,    # neuron1
    0.2, 0.3, 0.4, 0.5, 0.2     # neuron2
]

I_EXT = [0.8, 0.8]  # external currents
XI = [[0, 1], [1, 0]]  # electrical coupling

# where to save results (root directory)
SAVE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'V_H_DOT_1D/')
os.makedirs(SAVE_DIR, exist_ok=True)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# WORKER FUNCTION (runs in each process)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_one_param(param: float) -> float:
    """Simulate for a single parameter value and return mean post-transient dV/dt."""
    # 1‚Äì copy default parameters and set the swept one
    current_params = DEFAULT_PARAMS.copy()
    current_params[TARGET_PARAM] = param

    # 2‚Äì create simulator
    simulator = HRNetworkErrorSystem(
        params=current_params,
        dynamics='simplified',
        hr_initial_state=INITIAL_HR_STATE0,
        I_ext=I_EXT,
        hr_xi=XI)

    # 3‚Äì integrate
    simulator.solve(
        solver=SOLVER,
        t0=START_TIME,
        t1=END_TIME,
        dt0=DT_INITIAL,
        n_points=N_POINTS,
        stepsize_controller=STEPSIZE_CONTROLLER,
        max_steps=MAX_STEPS
    )

    # 4‚Äì analyse
    if not simulator.failed:
        results_dict = simulator.get_results_dict(TRANSIENT_RATIO)
        dVdt_timeseries = calculate_dVdt(results_dict, current_params)
        dHdt_timeseries = calculate_dHdt(results_dict, current_params)

        final_result = [float(jnp.nanmean(dVdt_timeseries)), float(jnp.nanmean(dHdt_timeseries))]

        # --- AGGRESSIVE CLEANUP ---
        del simulator, results_dict, dVdt_timeseries, dHdt_timeseries, current_params
        jax.clear_caches()
        gc.collect()

        return final_result
    else:
        # --- AGGRESSIVE CLEANUP (for the failure case too) ---
        del simulator, current_params
        jax.clear_caches()
        gc.collect()
        return [np.nan, np.nan]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN DRIVER
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main() -> None:
    print(f"Starting parallel simulation loop for {TARGET_PARAM} "
          f"from {PARAM_MIN} to {PARAM_MAX}‚Ä¶")

    # ‚îÄ‚îÄ new resume / checkpoint logic ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    outfile = os.path.join(
        SAVE_DIR, f"V_H_DOT_{TARGET_PARAM}_{PARAM_MIN}_{PARAM_MAX}.npz"
    )

    if os.path.exists(outfile):
        print(f"--- Found existing results file: {outfile}")
        with np.load(outfile) as data:
            if data["mean_dVdt"].shape == (NUM_PARAM_POINTS,):
                mean_dVdt = data["mean_dVdt"].copy()
                mean_dHdt = data["mean_dHdt"].copy()
                print("--- Resuming previous sweep")
            else:
                print("--- Shape mismatch ‚Äì starting fresh run")
                mean_dVdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
                mean_dHdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
    else:
        print("--- No checkpoint found ‚Äì starting fresh run")
        mean_dVdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)
        mean_dHdt = np.full(NUM_PARAM_POINTS, np.nan, dtype=np.float64)

    # build todo list
    tasks: list[dict[str, Any]] = []
    for idx, p in enumerate(PARAM_VALUES):
        if np.isnan(mean_dVdt[idx]) or np.isnan(mean_dHdt[idx]):
            tasks.append({"param": p, "index": idx})

    if not tasks:
        print("--- All simulations already complete! ---")
        return mean_dVdt, mean_dHdt

    print(f"Total simulations to run: {len(tasks)}")

    # ‚îÄ‚îÄ batch execution with RAM-safe pool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    with mp.Pool(processes=CORE_NUM, maxtasksperchild=1) as pool:
        for b0 in range(0, len(tasks), BATCH_SIZE):
            batch      = tasks[b0:b0+BATCH_SIZE]
            params     = [d["param"]  for d in batch]
            indices    = [d["index"]  for d in batch]

            print(f"\n--- Batch {b0//BATCH_SIZE + 1}/{-(-len(tasks)//BATCH_SIZE)} "
                  f"({b0+1}‚Äì{b0+len(batch)}) ---")

            results    = pool.map(run_one_param, params)

            for (dV, dH), idx in zip(results, indices):
                mean_dVdt[idx] = dV
                mean_dHdt[idx] = dH

            # save checkpoint every batch
            print("--- Saving checkpoint ---")
            np.savez_compressed(
                outfile,
                PARAM_VALUES=PARAM_VALUES,
                mean_dVdt=mean_dVdt,
                mean_dHdt=mean_dHdt,
                TARGET_PARAM=TARGET_PARAM
            )

    print("\nSimulation done")
    print("Saved on a file")

    return mean_dVdt, mean_dHdt




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ENTRY POINT  (required for multiprocessing on Windows & macOS)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    # On Windows the default start-method is "spawn" already.
    # Explicitly setting it once avoids accidental "fork" on some *nix installs.
    mp.freeze_support()             # makes scripts double-clickable on Windows
    mp.set_start_method("spawn", force=True)
    mean_dVdt, mean_dHdt = main()

    # ‚îÄ‚îÄ Plot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from visualization.plotting import plot_v_h_dot_1d

    # Ensure the simulation ran and returned valid data before plotting
    if mean_dVdt is not None and mean_dHdt is not None:
        print("Generating plot...")

        # # Define the output path for the plot, same as the data file but with .png
        # import os
        # plot_filename = os.path.join(
        #     SAVE_DIR, f"V_H_DOT_{TARGET_PARAM}_{PARAM_MIN}_{PARAM_MAX}.png"
        # )

        plot_v_h_dot_1d(
            param_values=PARAM_VALUES,
            mean_dvdt=mean_dVdt,
            mean_dhdt=mean_dHdt,
            param_name=TARGET_PARAM,
            title=f"Mean Derivatives vs. Parameter '{TARGET_PARAM}'",
            save_fig=False
        )



```

#### File: `v_dot_parameter_sweep_2D.py`
```python

"""
v_dot_parameter_sweep_2D.py
--------------------------------------------
Sweep two Hindmarsh‚ÄëRose model parameters, simulate for each pair,
compute the mean post‚Äëtransient dV/dt, and plot the result as a colour map.
"""

from __future__ import annotations
import os
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
import multiprocessing as mp
import gc
from typing import Any
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.physics import calculate_dHdt
from src.hr_model.physics import calculate_dVdt
from src.hr_model.model import DEFAULT_PARAMS


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CONFIGURATION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚Äë‚Äë choose the two parameters to sweep ‚Äë‚Äë
PARAM_X, PARAM_Y = "ge", "k"  # ‚Üê change names as needed

# ranges (inclusive)
X_MIN, X_MAX, NX = 0, 1, 3  # 11 points ‚Üí 0.3 ‚Ä¶ 0.9 step 0.06
Y_MIN, Y_MAX, NY = -2.5, 2, 3  # 9  points ‚Üí 0.1 ‚Ä¶ 0.5 step 0.05
x_vals = jnp.linspace(X_MIN, X_MAX, NX)
y_vals = jnp.linspace(Y_MIN, Y_MAX, NY)

# Multiprocessing params
CORE_NUM = 3
BATCH_SIZE = 9

# integration settings
START_TIME = 0
END_TIME = 250
DT_INITIAL = 0.01
POINT_NUM = 250
TRANSIENT_RATIO = 0.75
N_POINTS = dfx.SaveAt(ts=jnp.linspace(START_TIME, END_TIME, POINT_NUM), dense=True)
MAX_STEPS = int((END_TIME - START_TIME) / DT_INITIAL) * 20
SOLVER = dfx.Tsit5()
STEPSIZE_CONTROLLER = dfx.PIDController(rtol=1e-10, atol=1e-12)
# stepsize_controller = dfx.ConstantStepSize()

# Initial conditions (10 HR‚Äëstate variables + 5 error‚Äëstate variables)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,    # neuron1
    0.2, 0.3, 0.4, 0.5, 0.2     # neuron2
]

I_EXT = [0.8, 0.8]  # external currents
XI = [[0, 1], [1, 0]]  # electrical coupling

# where to save results (root directory)
SAVE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'V_H_DOT_2D/')
os.makedirs(SAVE_DIR, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# WORKER FUNCTION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_one_pair(params: tuple[float, float]) -> float:
    """Return mean post‚Äëtransient dV/dt for (x, y) parameter pair."""
    # 1‚Äì copy default parameters and set the swept one
    x_val, y_val = params
    current_params = DEFAULT_PARAMS.copy()
    current_params[PARAM_X] = x_val
    current_params[PARAM_Y] = y_val

    # 2‚Äì create simulator
    simulator = HRNetworkErrorSystem(
        params=current_params,
        dynamics='simplified',
        hr_initial_state=INITIAL_HR_STATE0,
        I_ext=I_EXT,
        hr_xi=XI)

    # 3‚Äì integrate
    # print('param pair: ',[x_val, y_val])
    simulator.solve(
        solver=SOLVER,
        t0=START_TIME,
        t1=END_TIME,
        dt0=DT_INITIAL,
        n_points=N_POINTS,
        stepsize_controller=STEPSIZE_CONTROLLER,
        max_steps=MAX_STEPS
    )

    # 4‚Äì analyse
    if not simulator.failed:
        results_dict = simulator.get_results_dict(TRANSIENT_RATIO)
        dVdt_timeseries = calculate_dVdt(results_dict, current_params)
        dHdt_timeseries = calculate_dHdt(results_dict, current_params)

        final_result = [float(jnp.nanmean(dVdt_timeseries)), float(jnp.nanmean(dHdt_timeseries))]

        # --- AGGRESSIVE CLEANUP ---
        del simulator, results_dict, dVdt_timeseries, dHdt_timeseries, current_params
        jax.clear_caches()
        gc.collect()

        return final_result
    else:
        # --- AGGRESSIVE CLEANUP (for the failure case too) ---
        del simulator, current_params
        jax.clear_caches()
        gc.collect()
        return [np.nan, np.nan]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN DRIVER
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main() -> tuple[np.ndarray, np.ndarray]:
    """
    Sweep PARAM_X √ó PARAM_Y, resume from partial runs, and keep RAM bounded.

    Returns
    -------
    (mean_dVdt, mean_dHdt)  ‚Äì both shape (NX, NY)
    """
    # ‚îÄ‚îÄ 1. Where to store results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    output_filename = os.path.join(
        SAVE_DIR,
        f"VDOT_{PARAM_X}_{X_MIN}-{X_MAX}_{PARAM_Y}_{Y_MIN}-{Y_MAX}.npz"
    )

    # ‚îÄ‚îÄ 2. Load / initialise result arrays ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if os.path.exists(output_filename):
        print(f"--- Found existing results file: {output_filename}")
        with np.load(output_filename) as data:
            if data["mean_dVdt"].shape == (NX, NY):
                # .copy() ‚áí writable even if the NPZ is mem-mapped read-only
                mean_dVdt = data["mean_dVdt"].copy()
                mean_dHdt = data["mean_dHdt"].copy()
                print("--- Resuming previous sweep")
            else:
                print("--- Shape mismatch ‚Äì starting fresh run")
                mean_dVdt = np.full((NX, NY), np.nan, dtype=np.float64)
                mean_dHdt = np.full((NX, NY), np.nan, dtype=np.float64)
    else:
        print("--- No checkpoint found ‚Äì starting fresh run")
        mean_dVdt = np.full((NX, NY), np.nan, dtype=np.float64)
        mean_dHdt = np.full((NX, NY), np.nan, dtype=np.float64)

    # ‚îÄ‚îÄ 3. Build todo list (need *both* matrices filled) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tasks_to_do: list[dict[str, Any]] = []
    for i in range(NX):           # rows  ‚Üí PARAM_X
        for j in range(NY):       # cols  ‚Üí PARAM_Y
            if np.isnan(mean_dVdt[i, j]) or np.isnan(mean_dHdt[i, j]):
                tasks_to_do.append({"params": (x_vals[i], y_vals[j]),
                                   "indices": (i, j)})

    if not tasks_to_do:
        print("--- All simulations already complete! ---")
        return mean_dVdt, mean_dHdt

    print(f"Total simulations to run: {len(tasks_to_do)}")

    # ‚îÄ‚îÄ 4. Batch-wise execution with ONE persistent worker pool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    with mp.Pool(processes=CORE_NUM, maxtasksperchild=1) as pool:
        for batch_no, start in enumerate(range(0, len(tasks_to_do), BATCH_SIZE), 1):
            current_batch  = tasks_to_do[start:start + BATCH_SIZE]
            params_batch   = [task["params"]   for task in current_batch]
            indices_batch  = [task["indices"]  for task in current_batch]

            print(f"\n--- Batch {batch_no}/{int(np.ceil(len(tasks_to_do)/BATCH_SIZE))} "
                  f"({start+1}‚Äì{min(start+BATCH_SIZE, len(tasks_to_do))}) ---")

            results_batch = pool.map(run_one_pair, params_batch)

            # insert into the big matrices
            for (dV, dH), (row, col) in zip(results_batch, indices_batch):
                mean_dVdt[row, col] = dV
                mean_dHdt[row, col] = dH

            # ‚îÄ‚îÄ 5. Checkpoint after every batch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            print("--- Saving checkpoint ---")
            np.savez_compressed(
                output_filename,
                x_vals=x_vals,
                y_vals=y_vals,
                mean_dVdt=mean_dVdt,
                mean_dHdt=mean_dHdt,
                PARAM_X=PARAM_X,
                PARAM_Y=PARAM_Y
            )

    print("\n--- All simulations finished successfully! ---")
    return mean_dVdt, mean_dHdt



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    mp.freeze_support()
    mp.set_start_method("spawn", force=True)
    mean_dVdt, mean_dHdt = main()

    # ‚îÄ‚îÄ Plot colour map ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    from visualization.plotting import plot_v_h_dot_2d


    # Ensure the simulation ran and returned valid data before plotting
    if mean_dVdt is not None and mean_dHdt is not None:
        print("Generating plots...")

        # # Define the output path for the plots, using the data filename as a base
        # import os
        # plot_path_prefix = os.path.join(
        #     SAVE_DIR,
        #     f"VDOT_{PARAM_X}_{X_MIN}-{X_MAX}_{PARAM_Y}_{Y_MIN}-{Y_MAX}"
        # )

        plot_v_h_dot_2d(
            x_vals=x_vals,
            y_vals=y_vals,
            mean_dvdt=mean_dVdt,
            mean_dhdt=mean_dHdt,
            param_x_name=PARAM_X,
            param_y_name=PARAM_Y,
            save_fig=False
        )
```

================================================================================
## Folder: utils
------------------------------------------------------------
### Python Source Files:

#### File: `print_pinn_pickle_keys.py`
```python
# utils/show_first_values.py
from __future__ import annotations

import argparse
import pickle
import sys
from pathlib import Path
from typing import Iterable, List, Any

import numpy as np


DEFAULT_KEYS: List[str] = [
    "x1", "y1", "z1", "u1", "phi1",
    "x2", "y2", "z2", "u2", "phi2",
]


def find_repo_root(start: Path) -> Path:
    for p in [start] + list(start.parents):
        if (p / "results").is_dir():
            return p
    return start.parent


def default_pickle_path(start: Path) -> Path:
    root = find_repo_root(start)
    return root / "results" / "PINN Data" / "data_for_PIN.pkl"


def to_numpy(a: Any) -> np.ndarray:
    # Gracefully handle jax, torch, lists, etc.
    try:
        import jax.numpy as jnp  # type: ignore
        if isinstance(a, jnp.ndarray):
            a = np.array(a)
    except Exception:
        pass
    try:
        import torch  # type: ignore
        if isinstance(a, torch.Tensor):
            a = a.detach().cpu().numpy()
    except Exception:
        pass
    if isinstance(a, np.ndarray):
        return a
    if isinstance(a, (list, tuple)):
        try:
            return np.asarray(a)
        except Exception:
            return np.array(a, dtype=object)
    # Last resort: wrap scalars
    return np.asarray(a)


def summarize_sample(sample: Any) -> str:
    arr = to_numpy(sample)
    if arr.ndim == 0:
        return _fmt(arr.item())
    if arr.ndim >= 1:
        flat = arr.ravel()
        n = min(6, flat.size)
        head = ", ".join(_fmt(x) for x in flat[:n])
        suffix = " ‚Ä¶" if flat.size > n else ""
        return f"[{head}{suffix}] shape={tuple(arr.shape)}"
    return repr(sample)


def _fmt(x: Any) -> str:
    if isinstance(x, (float, np.floating)):
        return f"{x:.6g}"
    return str(x)


def print_first_values(data: dict, keys: Iterable[str], idx: int) -> int:
    width = max(len(k) for k in keys) if keys else 3
    status = 0
    for k in keys:
        if k not in data:
            print(f"{k:<{width}} : (missing)")
            status = 1
            continue
        series = data[k]
        arr = to_numpy(series)
        if arr.ndim == 0:
            # scalar stored (unusual for a time series), just print it
            print(f"{k:<{width}} : {summarize_sample(arr)}")
            continue
        if arr.shape[0] <= idx:
            print(f"{k:<{width}} : Index {idx} out of range (length={arr.shape[0]})")
            status = 2
            continue
        print(f"{k:<{width}} : {summarize_sample(arr[idx])}")
    return status


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Show the first (or Nth) value of selected time series from data_for_PIN.pkl"
    )
    parser.add_argument(
        "-f", "--file", type=Path, default=None,
        help="Path to the pickle (defaults to (root)/results/PINN Data/data_for_PIN.pkl).",
    )
    parser.add_argument(
        "-k", "--keys", nargs="*", default=DEFAULT_KEYS,
        help=f"Keys to inspect (default: {', '.join(DEFAULT_KEYS)}).",
    )
    parser.add_argument(
        "-i", "--index", type=int, default=0,
        help="Time index to print (default: 0).",
    )
    args = parser.parse_args(argv)

    start = Path(__file__).resolve()
    pkl_path = args.file or default_pickle_path(start)

    if not pkl_path.exists():
        sys.stderr.write(f"‚ùå File not found: {pkl_path}\n")
        return 1

    try:
        with pkl_path.open("rb") as fh:
            data = pickle.load(fh)
    except Exception as e:
        sys.stderr.write(f"‚ùå Failed to load pickle: {e}\n")
        return 2

    if not isinstance(data, dict):
        sys.stderr.write(f"‚ö†Ô∏è Loaded object is {type(data).__name__}, not a dict‚Äîattempting best effort.\n")
        # Some users store a list with a single dict
        if isinstance(data, (list, tuple)) and data and isinstance(data[0], dict):
            data = data[0]
        else:
            sys.stderr.write("‚ùå Cannot interpret structure‚Äîexpected dict-like.\n")
            return 3

    print(f"üì¶ Loaded: {pkl_path}")
    print(f"üß≠ Index: {args.index}")
    return print_first_values(data, args.keys, args.index)


if __name__ == "__main__":
    raise SystemExit(main())

```

================================================================================
## Folder: visualization
------------------------------------------------------------
### Python Source Files:

#### File: `plotting.py`
```python
import os
import numpy as np
import matplotlib.pyplot as plt
# import matplotlib
# matplotlib.use('Qt5agg')


def plot_all_time_series(results, N, title="Hindmarsh-Rose Neuron States", save_fig=False):
    """
    Plots the time series of all state variables for each neuron in a grid.

    Args:
        results (dict): The dictionary of simulation results from get_results_dict.
        N (int): The number of neurons in the simulation.
        title (str): The main title for the entire figure.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    state_labels = [r'$x$', r'$y$', r'$z$', r'$u$', r'$\phi$']

    fig, axes = plt.subplots(5, N, figsize=(4 * N, 10), sharex=True, squeeze=False, dpi=300)
    fig.suptitle(title, fontsize=16)

    for i, (state_var, state_label) in enumerate(zip(states, state_labels)):
        for j in range(N):
            ax = axes[i, j]
            data_key = f'{state_var}{j + 1}'

            if data_key in results:
                ax.plot(results['t'], results[data_key])
                ax.grid(True, linestyle='--', alpha=0.6)

            if j == 0:
                ax.set_ylabel(state_label, fontsize=14)

            if i == 0:
                ax.set_title(f'Neuron {j + 1}')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = title.replace(' ', '_').replace(':', '') + '.png'
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_error_and_state_differences(results, title="Error System and State Difference Dynamics", save_fig=False):
    """
    Plots both the error system states (e.g., e_x) and the direct state
    differences (e.g., x2-x1) on the same subplots for comparison.
    Args:
        results (dict): The dictionary of simulation results.
        title (str): The main title for the figure.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    error_labels = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    diff_labels = [r'$x_2 - x_1$', r'$y_2 - y_1$', r'$z_2 - z_1$', r'$u_2 - u_1$', r'$\phi_2 - \phi_1$']

    fig, axes = plt.subplots(len(states), 1, figsize=(12, 15), sharex=True)
    fig.suptitle(title, fontsize=16)

    for i, state_var in enumerate(states):
        ax = axes[i]
        error_key = f'e_{state_var}'
        if error_key in results:
            ax.plot(results['t'], results[error_key], label=error_labels[i])

        key1 = f'{state_var}1'
        key2 = f'{state_var}2'
        if key1 in results and key2 in results:
            difference = results[key2] - results[key1]
            ax.plot(results['t'], difference, label=diff_labels[i], alpha=0.8)

        ax.set_ylabel(f"State '{state_var}'", fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(loc='upper right')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = title.replace(' ', '_').replace(':', '') + '.png'
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_bifurcation_diagram(param_values, peak_values, bifurcation_param_name, title="Bifurcation Diagram",
                             xlabel=None, ylabel=r'$x_{max}$', marker='.', s=1, save_fig=False):
    """Plots a bifurcation diagram."""

    if xlabel is None:
        greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'beta': r'$\beta$', 'gamma': r'$\gamma$'}
        xlabel = greek_letters.get(bifurcation_param_name.lower(), bifurcation_param_name)

    plt.figure(layout='tight')
    plt.scatter(param_values, peak_values, marker=marker, s=s, alpha=0.8)

    plt.xlabel(xlabel, fontsize=14)
    plt.ylabel(ylabel, fontsize=14)
    plt.title(f"{title} ({bifurcation_param_name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = f"bifurcation_{bifurcation_param_name}.png"
        save_path = os.path.join(output_dir, filename)
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_hamiltonian(t, H, save_fig=False):
    """Plots the Hamiltonian (H)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, H, label='Hamiltonian (H)')
    plt.xlabel('Time')
    plt.ylabel('H')
    plt.title('Hamiltonian vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'hamiltonian_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_hamiltonian_derivative(t, dHdt, save_fig=False):
    """Plots the Hamiltonian derivative (dH/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dHdt, label='dH/dt', color='orange')
    plt.xlabel('Time')
    plt.ylabel('dH/dt')
    plt.title('Hamiltonian Derivative vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'dHdt_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_lyapunov_derivative(t, dVdt, save_fig=False):
    """Plots the Lyapunov derivative (dV/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dVdt, label='dV/dt', color='green')
    plt.xlabel('Time')
    plt.ylabel('dV/dt')
    plt.title('Lyapunov Derivative vs. Time')
    plt.grid(True)
    plt.legend()

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, 'dVdt_vs_time.png')
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"Figure saved to {save_path}")
    else:
        plt.show(block=True)


def plot_pinn_data(all_results, save_fig=False):
    """
    Plots the results from the data generation for the PINN.
    Creates a separate figure for each simulation run, with subplots for key variables.
    Args:
        all_results (list): A list of result dictionaries for each run.
        save_fig (bool): If True, saves the figures to files. Otherwise, shows them interactively.
    """
    variables_to_plot = ['e_x', 'e_y', 'e_z', 'e_u', 'e_phi', 'Hamiltonian', 'dHdt', 'dVdt']
    titles = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$', r'$H$', r'$dH/dt$', r'$dV/dt$']

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)

    for run_idx, results in enumerate(all_results):
        t = results['t']
        fig, axes = plt.subplots(4, 2, figsize=(12, 8), sharex=True)
        fig.suptitle(f'Run {run_idx + 1}', fontsize=16)

        for ax, var, title in zip(axes.flat, variables_to_plot, titles):
            ax.plot(t, results[var])
            ax.set_xlabel('t')
            ax.set_ylabel(title)
            ax.grid(True)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_fig:
            filename = f'pinn_data_run_{run_idx + 1}.png'
            save_path = os.path.join(output_dir, filename)
            plt.savefig(save_path, dpi=300)
            plt.close(fig)
        else:
            plt.show(block=True)


    if save_fig:
        print(f"All PINN data plots saved to {output_dir}")


def plot_v_h_dot_1d(param_values, mean_dvdt, mean_dhdt, param_name, title, save_fig=False):
    """
    Plots the mean dV/dt and dH/dt against a single parameter.
    Args:
        param_values (np.ndarray): The values of the swept parameter.
        mean_dvdt (np.ndarray): The corresponding mean dV/dt values.
        mean_dhdt (np.ndarray): The corresponding mean dH/dt values.
        param_name (str): The name of the parameter (e.g., 'k', 'rho').
        title (str): The title for the plot.
        save_fig (bool): If True, saves the figure to a file. Otherwise, shows it interactively.
    """
    fig, ax = plt.subplots(constrained_layout=True)

    ax.plot(param_values, mean_dvdt, color="blue", label=r'$\langle dV/dt \rangle$', marker='.', linestyle='-')
    ax.plot(param_values, mean_dhdt, color="red", label=r'$\langle dH/dt \rangle$', marker='.', linestyle='-')

    greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'k': r'$k$'}
    xlabel = greek_letters.get(param_name.lower(), param_name)

    ax.set_xlabel(xlabel, fontsize=18)
    ax.set_ylabel(r'Mean Value', fontsize=18)
    ax.set_title(title, fontsize=20)
    ax.grid(True)
    ax.legend(loc="best", fontsize=14)
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)
        filename = f'v_h_dot_vs_{param_name}.png'
        save_path = os.path.join(output_dir, filename)
        fig.savefig(save_path, dpi=300)
        plt.close(fig)
        print(f"Plot saved to {save_path}")
    else:
        plt.show(block=True)


def plot_v_h_dot_2d(x_vals, y_vals, mean_dvdt, mean_dhdt, param_x_name, param_y_name, save_fig=False):
    """
    Plots 2D heatmaps for mean dV/dt and dH/dt from a parameter sweep.
    Generates and saves two separate figures.

    Args:
        x_vals (np.ndarray): Array of values for the parameter on the y-axis.
        y_vals (np.ndarray): Array of values for the parameter on the x-axis.
        mean_dvdt (np.ndarray): 2D array of mean dV/dt values.
        mean_dhdt (np.ndarray): 2D array of mean dH/dt values.
        param_x_name (str): Name of the parameter for the y-axis.
        param_y_name (str): Name of the parameter for the x-axis.
        save_fig (bool): If True, saves the plots to files. Otherwise, shows them interactively.
    """
    Xg, Yg = np.meshgrid(y_vals, x_vals)
    greek_map = {'ge': r'$g_e$', 'rho': r'$\rho$', 'k': r'$k$', 'm': r'$m$'}
    xlabel = greek_map.get(param_y_name.lower(), param_y_name)
    ylabel = greek_map.get(param_x_name.lower(), param_x_name)

    def five_ticks(grid):
        mn, mx = np.nanmin(grid), np.nanmax(grid)
        return np.round(np.linspace(mn, mx, 5), 2)

    # --- Plot 1: dV/dt ---
    fig_dv, ax_dv = plt.subplots(constrained_layout=True)
    pcm1 = ax_dv.pcolormesh(Xg, Yg, mean_dvdt, shading="auto", cmap='viridis')
    ax_dv.set_xlabel(xlabel, fontsize=14)
    ax_dv.set_ylabel(ylabel, fontsize=14)
    ax_dv.set_title(r"Mean $\langle dV/dt \rangle$ (post-transient)", fontsize=16)
    ax_dv.set_xticks(five_ticks(y_vals))
    ax_dv.set_yticks(five_ticks(x_vals))
    cbar1 = fig_dv.colorbar(pcm1, ax=ax_dv)
    cbar1.set_label(r'$\langle dV/dt \rangle$', fontsize=14)

    # --- Plot 2: dH/dt ---
    fig_dh, ax_dh = plt.subplots(constrained_layout=True)
    pcm2 = ax_dh.pcolormesh(Xg, Yg, mean_dhdt, shading="auto", cmap='inferno')
    ax_dh.set_xlabel(xlabel, fontsize=14)
    ax_dh.set_ylabel(ylabel, fontsize=14)
    ax_dh.set_title(r"Mean $\langle dH/dt \rangle$ (post-transient)", fontsize=16)
    ax_dh.set_xticks(five_ticks(y_vals))
    ax_dh.set_yticks(five_ticks(x_vals))
    cbar2 = fig_dh.colorbar(pcm2, ax=ax_dh)
    cbar2.set_label(r'$\langle dH/dt \rangle$', fontsize=14)

    if save_fig:
        output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'results', 'temp')
        os.makedirs(output_dir, exist_ok=True)

        dv_filename = f"v_dot_heatmap_{param_x_name}_vs_{param_y_name}.png"
        dh_filename = f"h_dot_heatmap_{param_x_name}_vs_{param_y_name}.png"

        dv_save_path = os.path.join(output_dir, dv_filename)
        dh_save_path = os.path.join(output_dir, dh_filename)

        fig_dv.savefig(dv_save_path, dpi=300)
        fig_dh.savefig(dh_save_path, dpi=300)
        print(f"Plots saved to {dv_save_path} and {dh_save_path}")
        plt.close(fig_dv)
        plt.close(fig_dh)
    else:
        plt.show(block=True)
```

================================================================================
## Folder: src
------------------------------------------------------------

================================================================================
## Folder: visualization/.ipynb_checkpoints
------------------------------------------------------------
### Python Source Files:

#### File: `plotting-checkpoint.py`
```python
# Visualization.py
# Defines simulation parameters, runs the solver, and plots the results.

import numpy as np
# import matplotlib
# matplotlib.use('TkAgg')
import matplotlib.pyplot as plt



def plot_all_time_series(results, N, title="Hindmarsh-Rose Neuron States"):
    """
    Plots the time series of all state variables for each neuron in a grid.

    Args:
        results (dict): The dictionary of simulation results from get_results_dict.
        N (int): The number of neurons in the simulation.
        title (str): The main title for the entire figure.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    state_labels = [r'$x$', r'$y$', r'$z$', r'$u$', r'$\phi$']

    # Create a figure with 5 rows (for states) and N columns (for neurons)
    # The squeeze=False argument ensures that 'axes' is always a 2D array,
    # even if N=1, which simplifies indexing.
    fig, axes = plt.subplots(5, N, figsize=(4 * N, 10), sharex=True, squeeze=False, dpi=300)
    fig.suptitle(title, fontsize=16)

    for i, (state_var, state_label) in enumerate(zip(states, state_labels)):  # Loop over states (rows)
        for j in range(N):  # Loop over neurons (columns)
            ax = axes[i, j]
            data_key = f'{state_var}{j + 1}'

            if data_key in results:
                ax.plot(results['t'], results[data_key])
                ax.grid(True, linestyle='--', alpha=0.6)

            # Set y-label for the first column
            if j == 0:
                ax.set_ylabel(state_label, fontsize=14)

            # Set title for the first row
            if i == 0:
                ax.set_title(f'Neuron {j + 1}')

    # Set common x-label
    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for suptitle
    plt.show(block=True)


def plot_error_and_state_differences(results, title="Error System and State Difference Dynamics"):
    """
    Plots both the error system states (e.g., e_x) and the direct state
    differences (e.g., x2-x1) on the same subplots for comparison.

    Args:
        results (dict): The dictionary of simulation results.
        title (str): The main title for the figure.
    """
    states = ['x', 'y', 'z', 'u', 'phi']
    error_labels = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    diff_labels = [r'$x_2 - x_1$', r'$y_2 - y_1$', r'$z_2 - z_1$', r'$u_2 - u_1$', r'$\phi_2 - \phi_1$']

    fig, axes = plt.subplots(len(states), 1, figsize=(12, 15), sharex=True)
    fig.suptitle(title, fontsize=16)

    for i, state_var in enumerate(states):
        ax = axes[i]

        # Plot the error system state (e.g., e_x)
        error_key = f'e_{state_var}'
        if error_key in results:
            ax.plot(results['t'], results[error_key], label=error_labels[i])

        # Plot the direct state difference (e.g., x2 - x1)
        key1 = f'{state_var}1'
        key2 = f'{state_var}2'
        if key1 in results and key2 in results:
            difference = results[key2] - results[key1]
            ax.plot(results['t'], difference, label=diff_labels[i], alpha=0.8)

        # Set labels and legend for each subplot
        ax.set_ylabel(f"State '{state_var}'", fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(loc='upper right')

    plt.xlabel('Time', fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show(block=True)



def plot_bifurcation_diagram(
    param_values,
    peak_values,
    bifurcation_param_name,
    title="Bifurcation Diagram",
    xlabel=None,
    ylabel=r'$x_{max}$', # Default ylabel for x peaks
    marker='.',
    s=1, # Marker size
    save_path=None):

    """Plots a bifurcation diagram."""

    if xlabel is None:
        # Use LaTeX representation if common greek letter, otherwise just the name
        greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'beta': r'$\beta$', 'gamma': r'$\gamma$'}
        xlabel = greek_letters.get(bifurcation_param_name.lower(), bifurcation_param_name)


    plt.figure(layout='tight') # Use tight layout

    # Filter out NaN values for plotting if necessary, or let scatter handle them
    # Scatter usually ignores NaN, which is often desired.
    plt.scatter(param_values, peak_values, marker=marker, s=s, alpha=0.8)

    plt.xlabel(xlabel, fontsize=14)
    plt.ylabel(ylabel, fontsize=14)
    plt.title(f"{title} ({bifurcation_param_name})", fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)

    if save_path:
        try:
            plt.savefig(save_path, dpi=300)
            print(f"Figure saved to {save_path}")
        except Exception as e:
            print(f"Error saving figure: {e}")
    else:
        plt.show(block=True)


def plot_hamiltonian(t, H):
    """Plots the Hamiltonian (H)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, H, label='Hamiltonian (H)')
    plt.xlabel('Time')
    plt.ylabel('H')
    plt.title('Hamiltonian vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_hamiltonian_derivative(t, dHdt):
    """Plots the Hamiltonian derivative (dH/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dHdt, label='dH/dt', color='orange')
    plt.xlabel('Time')
    plt.ylabel('dH/dt')
    plt.title('Hamiltonian Derivative vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_lyapunov_derivative(t, dVdt):
    """Plots the Lyapunov derivative (dV/dt)"""
    plt.figure(figsize=(10, 6))
    plt.plot(t, dVdt, label='dV/dt', color='green')
    plt.xlabel('Time')
    plt.ylabel('dV/dt')
    plt.title('Lyapunov Derivative vs. Time')
    plt.grid(True)
    plt.legend()
    plt.show(block=True)

def plot_pinn_data(all_results):
    """
    Plots the results from the data generation for the PINN.
    Creates a separate figure for each simulation run, with subplots for key variables.

    Args:
        all_results (list): A list of result dictionaries, where each dictionary
                            corresponds to a single simulation run.
    """
    # Define variables to plot and their corresponding LaTeX titles
    variables_to_plot = ['e_x', 'e_y', 'e_z', 'e_u', 'e_phi', 'Hamiltonian', 'dHdt', 'dVdt']
    titles = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$', r'$H$', r'$dH/dt$', r'$dV/dt$']

    # Create a separate plot for each run in the results
    for run_idx, results in enumerate(all_results):
        t = results['t']
        fig, axes = plt.subplots(4, 2, sharex=True)
        fig.suptitle(f'Run {run_idx + 1}', fontsize=16)

        # Plot each time series in its designated subplot
        for ax, var, title in zip(axes.flat, variables_to_plot, titles):
            ax.plot(t, results[var])
            ax.set_xlabel('t')
            ax.set_ylabel(var)
            ax.grid(True)

        plt.tight_layout()
        plt.show(block=True)


def plot_v_h_dot_1d(param_values, mean_dvdt, mean_dhdt, param_name, title, save_path=None):
    """
    Plots the mean dV/dt and dH/dt against a single parameter in a combined figure.

    Args:
        param_values (np.ndarray): The values of the swept parameter.
        mean_dvdt (np.ndarray): The corresponding mean dV/dt values.
        mean_dhdt (np.ndarray): The corresponding mean dH/dt values.
        param_name (str): The name of the parameter (e.g., 'k', 'rho').
        title (str): The title for the plot.
        save_path (str, optional): Path to save the figure. If None, shows the plot.
    """
    fig, ax = plt.subplots(
        constrained_layout=True
    )

    # Plot the two curves
    ax.plot(param_values, mean_dvdt, color="blue", label=r'$\langle dV/dt \rangle$', marker='.', linestyle='-')
    ax.plot(param_values, mean_dhdt, color="red", label=r'$\langle dH/dt \rangle$', marker='.', linestyle='-')

    # Use LaTeX representation for common greek letters
    greek_letters = {'rho': r'$\rho$', 'alpha': r'$\alpha$', 'k': r'$k$'}
    xlabel = greek_letters.get(param_name.lower(), param_name)

    # Set labels, title, grid, and legend
    ax.set_xlabel(xlabel, fontsize=18)
    ax.set_ylabel(r'Mean Value', fontsize=18)
    ax.set_title(title, fontsize=20)
    ax.grid(True)
    ax.legend(loc="best", fontsize=14)

    # Enforce a square aspect ratio and remove horizontal margins
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    # Save or show the plot
    if save_path:
        fig.savefig(save_path, dpi=300)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show(block=True)


def plot_v_h_dot_2d(x_vals, y_vals, mean_dvdt, mean_dhdt, param_x_name, param_y_name, save_path_prefix=None):
    """
    Plots 2D heatmaps for mean dV/dt and dH/dt from a parameter sweep.
    Generates and saves two separate figures.

    Args:
        x_vals (np.ndarray): Array of values for the parameter on the y-axis.
        y_vals (np.ndarray): Array of values for the parameter on the x-axis.
        mean_dvdt (np.ndarray): 2D array (shape Nx, Ny) of mean dV/dt values.
        mean_dhdt (np.ndarray): 2D array (shape Nx, Ny) of mean dH/dt values.
        param_x_name (str): Name of the parameter for the y-axis.
        param_y_name (str): Name of the parameter for the x-axis.
        save_path_prefix (str, optional): Base path for saving the plots.
                                          If None, plots are shown interactively.
    """
    # --- Common Setup ---
    # Create meshgrid. Note: The parameter for the plot's x-axis (PARAM_Y) comes first.
    Xg, Yg = np.meshgrid(y_vals, x_vals)

    # Define LaTeX labels for better formatting
    greek_map = {'ge': r'$g_e$', 'rho': r'$\rho$', 'k': r'$k$', 'm': r'$m$'}
    xlabel = greek_map.get(param_y_name.lower(), param_y_name)
    ylabel = greek_map.get(param_x_name.lower(), param_x_name)

    # Helper function to generate clean tick marks
    def five_ticks(grid):
        mn, mx = np.nanmin(grid), np.nanmax(grid)
        return np.round(np.linspace(mn, mx, 5), 2)

    # --- Plot 1: dV/dt ---
    fig_dv, ax_dv = plt.subplots(constrained_layout=True)
    pcm1 = ax_dv.pcolormesh(Xg, Yg, mean_dvdt, shading="auto", cmap='viridis')
    ax_dv.set_xlabel(xlabel, fontsize=14)
    ax_dv.set_ylabel(ylabel, fontsize=14)
    ax_dv.set_title(r"Mean $\langle dV/dt \rangle$ (post-transient)", fontsize=16)

    ax_dv.set_xticks(five_ticks(y_vals))
    ax_dv.set_yticks(five_ticks(x_vals))

    cbar1 = fig_dv.colorbar(pcm1, ax=ax_dv)
    cbar1.set_label(r'$\langle dV/dt \rangle$', fontsize=14)

    # --- Plot 2: dH/dt ---
    fig_dh, ax_dh = plt.subplots(constrained_layout=True)
    pcm2 = ax_dh.pcolormesh(Xg, Yg, mean_dhdt, shading="auto", cmap='inferno')
    ax_dh.set_xlabel(xlabel, fontsize=14)
    ax_dh.set_ylabel(ylabel, fontsize=14)
    ax_dh.set_title(r"Mean $\langle dH/dt \rangle$ (post-transient)", fontsize=16)

    ax_dh.set_xticks(five_ticks(y_vals))
    ax_dh.set_yticks(five_ticks(x_vals))

    cbar2 = fig_dh.colorbar(pcm2, ax=ax_dh)
    cbar2.set_label(r'$\langle dH/dt \rangle$', fontsize=14)

    # --- Save or Show Plots ---
    if save_path_prefix:
        dv_save_path = f"{save_path_prefix}_dVdt.png"
        dh_save_path = f"{save_path_prefix}_dHdt.png"
        fig_dv.savefig(dv_save_path, dpi=300)
        fig_dh.savefig(dh_save_path, dpi=300)
        print(f"Plots saved to {dv_save_path} and {dh_save_path}")
        plt.close(fig_dv)
        plt.close(fig_dh)
    else:
        plt.show(block=True)
```

================================================================================
## Folder: visualization/literature plots
------------------------------------------------------------
### Python Source Files:

#### File: `plot_PINN_from_pickle.py`
```python
"""
plot_PINN_from_pickle.py
-------------------------
This script loads the curated time-series and loss data from the 
'pinn_plot_data.pkl' file generated by pH_PINN.py and reproduces all
the final analysis and comparison plots.

This allows for visualization to be run independently from model training.
"""

import pickle
import os
import numpy as np
import matplotlib.pyplot as plt


def main():
    """
    Loads the pickle data and generates all plots.
    """
    # ==========================================================================
    # 1. SETUP PATHS AND LOAD DATA
    # ==========================================================================

    # Define paths relative to this script's location
    # This script is in (root)/visualization/literature plots/
    # The data is in (root)/results/PINN Data/
    script_dir = os.path.dirname(os.path.abspath(__file__))
    pickle_path = os.path.join(script_dir, '..', '..', 'results', 'PINN Data', 'pinn_plot_data.pkl')
    output_dir = os.path.join(script_dir, '..', '..', 'results', 'PINN Plots')
    os.makedirs(output_dir, exist_ok=True)

    print(f"Loading plotting data from: {pickle_path}")
    try:
        with open(pickle_path, 'rb') as f:
            data = pickle.load(f)
        print("Data loaded successfully.")
    except FileNotFoundError:
        print(f"ERROR: Data file not found at '{pickle_path}'")
        print("Please run the pH_PINN.py script first to generate the data file.")
        return

    # ==========================================================================
    # 2. UNPACK DATA FROM DICTIONARY
    # ==========================================================================

    # Unpack all variables from the dictionary for easier access
    t_test = data['t_test']
    # split_start = data['split_start']
    # split_end = data['split_end']

    split_start = 0
    split_end = 100000

    # Loss Histories
    train_losses = data['train_losses']
    val_losses = data['val_losses']
    hamiltonian_losses = data['hamiltonian_losses']
    phys_losses = data['phys_losses']
    conservative_losses = data['conservative_losses']
    dissipative_losses = data['dissipative_losses']
    data_unified_losses = data['data_unified_losses']
    physics_residual_losses = data['physics_residual_losses']
    j_structure_losses = data['j_structure_losses']
    r_structure_losses = data['r_structure_losses']

    # Plot Data
    H_analytical_vis = data['H_analytical_vis']
    H_learned_aligned = data['H_learned_aligned']
    e_dot_test = data['e_dot_test']
    e_dot_autodiff = data['e_dot_autodiff']
    e_dot_from_equations = data['e_dot_from_equations']
    e_dot_from_structure = data['e_dot_from_structure']
    x_dot_test = data['x_dot_test']
    x_dot_autodiff = data['x_dot_autodiff']
    x_dot_from_vectorfield_vis = data['x_dot_from_vectorfield_vis']
    e_test = data['e_test']
    e_pred = data['e_pred']
    x_test = data['x_test']
    x_pred = data['x_pred']
    dHdt_analytical_vis = data['dHdt_analytical_vis']
    dHdt_pred_true = data['dHdt_pred_true']
    dHdt_pred_structure = data['dHdt_pred_structure']
    dHdt_pred_autodiff = data['dHdt_pred_autodiff']
    dHdt_pred_pH = data['dHdt_pred_pH']

    print("All required data has been unpacked.")

    # ==========================================================================
    # 3. GENERATE PLOTS
    # ==========================================================================
    print("Generating plots...")

    # --- Plot 1: Learned vs Analytical Hamiltonian ---
    plt.figure(figsize=(11, 10))
    plt.plot(t_test[split_start:split_end], H_analytical_vis[split_start:split_end],
             label='Analytical Hamiltonian', color='blue', linewidth=2)
    plt.plot(t_test[split_start:split_end], H_learned_aligned[split_start:split_end],
             label='Learned Hamiltonian', color='red')
    plt.xlabel("$t$", fontsize=30)
    plt.ylabel("$H$", fontsize=30)
    plt.legend(fontsize=30, loc='upper right')
    plt.title("(a)", fontsize=30)
    plt.grid(True)
    plt.tick_params(axis='both', which='major', labelsize=25)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'hamiltonian_comparison.png'), dpi=300)


    # --- Plot 2: Detailed Loss Histories ---
    plt.figure(figsize=(16, 9))
    plt.plot(train_losses, label='Total Training', linewidth=2)
    plt.plot(val_losses, label='Total Validation', linewidth=2)
    plt.plot(data_unified_losses, label='Data Fidelity', color='dodgerblue', linewidth=2)
    plt.plot(physics_residual_losses, label='Physics Residual', color='darkorange', linewidth=2)
    plt.plot(phys_losses, label='PH Structure', color='purple', linewidth=2)
    plt.plot(j_structure_losses, label='J Fidelity', color='brown', linewidth=2)
    plt.plot(r_structure_losses, label='R Fidelity', color='magenta', linewidth=2)
    plt.plot(conservative_losses, label='Conservative', color='green', linewidth=2)
    plt.plot(dissipative_losses, label='Dissipative', color='darkcyan', linewidth=2)
    plt.plot(hamiltonian_losses, label='Hamiltonian (Monitor)', color='red')
    plt.yscale('log')
    plt.xlabel('Epoch', fontsize=25)
    plt.ylabel('Loss (Log Scale)', fontsize=25)
    plt.legend(fontsize=20, loc='upper right', ncol=3)
    plt.grid(True, which="both", ls="--", alpha=0.6)
    plt.tick_params(axis='both', which='major', labelsize=20)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'training_losses.png'), dpi=300)


    # --- Plot 3: Error Derivative Fidelity (e_dot) ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 12), sharex=True)
    state_labels_e_dot = [r'$\dot{e}_x$', r'$\dot{e}_y$', r'$\dot{e}_z$', r'$\dot{e}_u$', r'$\dot{e}_\phi$']
    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], e_dot_test[split_start:split_end, i],
                     label='True Derivative', color='green', linewidth=2)
        axes[i].plot(t_test[split_start:split_end], e_dot_autodiff[split_start:split_end, i],
                     label='Autodiff', color='orange')
        axes[i].plot(t_test[split_start:split_end], e_dot_from_equations[split_start:split_end, i],
                     label='Analytical Eq.', color='purple')
        axes[i].plot(t_test[split_start:split_end], e_dot_from_structure[split_start:split_end, i],
                     label='PH Structure', color='red')
        axes[i].set_ylabel(state_labels_e_dot[i], fontsize=25)
        axes[i].grid(True)
        axes[i].tick_params(axis='both', which='major', labelsize=16)
    axes[0].legend(loc='upper right', fontsize=18)
    axes[-1].set_xlabel("$t$", fontsize=25)
    plt.tight_layout()
    fig.savefig(os.path.join(output_dir, 'error_derivative_fidelity.png'), dpi=300)


    # --- Plot 4: HR Derivative Fidelity (x_dot) ---
    fig, axes = plt.subplots(x_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_x_dot = [r'$\dot{x}_1$', r'$\dot{y}_1$', r'$\dot{z}_1$', r'$\dot{u}_1$', r'$\dot{\phi}_1$',
                          r'$\dot{x}_2$', r'$\dot{y}_2$', r'$\dot{z}_2$', r'$\dot{u}_2$', r'$\dot{\phi}_2$']
    for i in range(x_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], x_dot_test[split_start:split_end, i],
                     label='True Derivative', color='green', linewidth=2)
        axes[i].plot(t_test[split_start:split_end], x_dot_autodiff[split_start:split_end, i],
                     label='Autodiff', color='orange')
        axes[i].plot(t_test[split_start:split_end], x_dot_from_vectorfield_vis[split_start:split_end, i],
                     label='Analytical Eq.', color='purple')
        axes[i].set_ylabel(state_labels_x_dot[i], fontsize=25)
        axes[i].grid(True)
        axes[i].tick_params(axis='both', which='major', labelsize=16)
    axes[0].legend(loc='upper right', fontsize=18)
    axes[-1].set_xlabel("$t$", fontsize=25)
    plt.tight_layout()
    fig.savefig(os.path.join(output_dir, 'hr_derivative_fidelity.png'), dpi=300)


    # --- Plot 5: Error State Trajectories (e) ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 12), sharex=True)
    state_labels_error = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], e_test[split_start:split_end, i],
                     'b', label='True State', linewidth=2)
        axes[i].plot(t_test[split_start:split_end], e_pred[split_start:split_end, i],
                     'r', label='Predicted State')
        axes[i].set_ylabel(state_labels_error[i], fontsize=25)
        axes[i].grid(True)
        axes[i].tick_params(axis='both', which='major', labelsize=16)
    axes[0].legend(loc='upper right', fontsize=18)
    axes[-1].set_xlabel("$t$", fontsize=25)
    plt.tight_layout()
    fig.savefig(os.path.join(output_dir, 'error_state_e_prediction.png'), dpi=300)


    # --- Plot 6: HR State Trajectories (x) ---
    fig, axes = plt.subplots(x_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_x = [r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\phi_1$',
                      r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$', r'$\phi_2$']
    for i in range(x_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], x_test[split_start:split_end, i],
                     'b', label='True State', linewidth=2)
        axes[i].plot(t_test[split_start:split_end], x_pred[split_start:split_end, i],
                     'r', label='Predicted State')
        axes[i].set_ylabel(state_labels_x[i], fontsize=25)
        axes[i].grid(True)
        axes[i].tick_params(axis='both', which='major', labelsize=16)
    axes[2].legend(loc='upper right', fontsize=18)
    axes[-1].set_xlabel("$t$", fontsize=14)
    plt.tight_layout()
    fig.savefig(os.path.join(output_dir, 'hr_state_x_prediction.png'), dpi=300)

    # --- Plot 7: dH/dt Comparison ---
    plt.figure(figsize=(11, 10))
    plt.plot(t_test[split_start:split_end], dHdt_analytical_vis[split_start:split_end],
             label=r'Analytical $dH/dt$', linewidth=2, color='blue')
    # plt.plot(t_test[split_start:split_end], dHdt_pred_true[split_start:split_end]*(-1),
    #          label=r'$\nabla H \cdot \dot e_{\mathrm{true}}$',color='red')
    # plt.plot(t_test[split_start:split_end], dHdt_pred_autodiff[split_start:split_end]*(-1),
    #          label=r'$\nabla H \cdot \dot e_{\mathrm{autodiff}}$', color='red')
    plt.plot(t_test[split_start:split_end], dHdt_pred_pH[split_start:split_end]*(-1),
             label=r'Learned $dH/dt = -(\nabla H)^\top R\,\nabla H$', color='red')
    plt.xlabel('$t$', fontsize=30);
    plt.ylabel(r'$dH/dt$', fontsize=30)
    plt.legend(fontsize=30, loc='upper right')
    plt.tight_layout()
    plt.grid(True)
    plt.tick_params(axis='both', which='major', labelsize=25)
    plt.title("(b)", fontsize=30)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'dHdt_comparison.png'), dpi=300)

    plt.close('all')
    print(f"\n‚úÖ All plots have been generated and saved to '{output_dir}'")


if __name__ == "__main__":
    main()
```

#### File: `plot_TS_PhP.py`
```python
import os
import diffrax as dfx
import matplotlib.pyplot as plt
from matplotlib.ticker import FormatStrFormatter
from src.hr_model.model import HindmarshRose, DEFAULT_PARAMS, DEFAULT_STATE0


# ==============================================================================
# 1. LOCAL PLOTTING FUNCTIONS
# ==============================================================================

def plot_time_series(t, x, title_suffix, save_path_prefix=None):
    """Plots the time series of variable x after a transient period."""

    plt.figure(figsize=(8, 8), layout='tight')
    plt.suptitle(f'{title_suffix}', fontsize=30, y=0.957)
    plt.plot(t, x, c='k')
    plt.xlabel(r'$t$', fontsize=25)
    plt.ylabel(r'$x$', fontsize=25)
    plt.tick_params(axis='both', which='major', labelsize=20)

    if len(t) > 0:
        t_min, t_max = t.min(), t.max()
        plt.xlim([t_min, t_max + (t_max - t_min) * 0.01])

    ax = plt.gca()
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    if save_path_prefix:
        plt.savefig(save_path_prefix + '.png', format='png', dpi=300)
        plt.savefig(save_path_prefix + '.eps', format='eps')
        plt.close()
    else:
        plt.show(block=True)


def plot_phase_portrait(x, y, title_suffix, save_path_prefix=None):
    """Plots the phase portrait (x vs y) after a transient period."""

    plt.figure(figsize=(8, 8), layout='tight')
    plt.suptitle(f'{title_suffix}', fontsize=30, y=0.957)
    plt.plot(x, y, c='k')
    plt.xlabel(r'$x$', fontsize=25)
    plt.ylabel(r'$y$', fontsize=25)
    plt.tick_params(axis='both', which='major', labelsize=20)
    ax = plt.gca()
    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))
    ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    if save_path_prefix:
        plt.savefig(save_path_prefix + '.png', format='png', dpi=300)
        plt.savefig(save_path_prefix + '.eps', format='eps')
        plt.close()
    else:
        plt.show(block=True)


# ==============================================================================
# 2. MAIN SCRIPT LOGIC
# ==============================================================================

# --- Define the two specific simulation setups ---
setups = [
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 0.25},
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 0.5},
    {'name': 'rho_fixed', 'rho': 0.7, 'k': 0.36, 'm': 1},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 0.25},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 0.5},
    {'name': 'k_fixed', 'rho': 0.7, 'k': 0.87, 'm': 1}
]

# --- Define Output Directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Phase Portrait')
os.makedirs(output_dir, exist_ok=True)
print(f"Plots will be saved to: {output_dir}")

# --- Loop Through Setups and Run Simulations ---
for setup_params in setups:
    m_val = setup_params['m']
    setup_name = setup_params['name']
    print(f"\n--- Running simulation for setup: '{setup_name}' (m = {m_val}) ---")

    # --- Use simulation example from model.py ---
    sim_params = DEFAULT_PARAMS.copy()
    sim_params.update(setup_params)  # Update params for the current setup

    # Create the model instance
    hr_model = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)

    # Integration settings
    start_time = 0
    end_time = 2000
    dt_initial = 0.05
    n_points = 20000
    max_steps = int((end_time - start_time) / dt_initial) * 20
    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)

    # Run the solver
    hr_model.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    # Get results, discarding the first 75% as transient
    transient_discard_ratio = 0.75
    results = hr_model.get_results_dict(transient_ratio=transient_discard_ratio)
    t_sol, x_sol, y_sol = results['t'], results['x1'], results['y1']

    # --- Use exact plotting logic from the original script ---
    if m_val == 1:
        xt_title_suffix, xy_title_suffix = '(a1)', '(a2)'
    elif m_val == 0.5:
        xt_title_suffix, xy_title_suffix = '(b1)', '(b2)'
    elif m_val == 0.25:
        xt_title_suffix, xy_title_suffix = '(c1)', '(c2)'
    else:
        xt_title_suffix, xy_title_suffix = '(x1)', '(x2)'

    # --- Create filenames and save plots ---
    base_filename_ts = f"timeseries_{setup_name}_{xt_title_suffix.strip('()')}"
    save_path_prefix_ts = os.path.join(output_dir, base_filename_ts)

    base_filename_pp = f"phase_portrait_{setup_name}_{xy_title_suffix.strip('()')}"
    save_path_prefix_pp = os.path.join(output_dir, base_filename_pp)

    # Plot and save the time series using the local function
    plot_time_series(t_sol, x_sol, xt_title_suffix, save_path_prefix=save_path_prefix_ts)

    # Plot and save the phase portrait using the local function
    plot_phase_portrait(x_sol, y_sol, xy_title_suffix, save_path_prefix=save_path_prefix_pp)

print("\nAll simulations and plots saved successfully.")
```

#### File: `plot_bif_lya.py`
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.ticker import FuncFormatter
import os # Add this import



# --- Define paths to the data directories relative to the data ---
path_bif = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Bifurcation/')
os.makedirs(path_bif, exist_ok=True)

path_lya = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Lyapanov 1D/')
os.makedirs(path_lya, exist_ok=True)

# --- Define output directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Bif_Lya_plots')
os.makedirs(output_dir, exist_ok=True)



bif_names = ['Bif_k_rho_0.7_m_1.npy', 'Bif_k_rho_0.7_m_0.5.npy', 'Bif_k_rho_0.7_m_0.25.npy',
             'Bif_rho_k_0.87_m_1.npy', 'Bif_rho_k_0.87_m_0.5.npy', 'Bif_rho_k_0.87_m_0.25.npy']
lya_names = ['Lya_k_rho_0.7_m_1.txt', 'Lya_k_rho_0.7_m_0.5.txt', 'Lya_k_rho_0.7_m_0.25.txt',
             'Lya_rho_k_0.87_m_1.txt', 'Lya_rho_k_0.87_m_0.5.txt', 'Lya_rho_k_0.87_m_0.25.txt']
save_names = ['k_m_1', 'k_m_0.5', 'k_m_0.25',
              'rho_m_1', 'rho_m_0.5', 'rho_m_0.25']
x_names = [r'$k$', r'$k$', r'$k$', r'$\rho$', r'$\rho$', r'$\rho$']
titles = ['(a)', '(b)', '(c)', '(a)', '(b)', '(c)']

leg_poss = [(0, 0), (0, 0), (0, 0), (0, 0), (0.75, 0), (0, 0)]

for bif_name, lya_name, save_name, title, leg_pos, x_name in zip(
        bif_names, lya_names, save_names, titles, leg_poss, x_names):

    # Load data
    data_lya = np.array(pd.read_csv(path_lya+lya_name, delim_whitespace=True, header=None))
    data_bif = np.load(path_bif+bif_name)


    # Function to format ticks to 2 decimal places
    def format_func(value, tick_number):
        return f'{value:.2f}'


    # Define the size of each subplot (square dimensions)
    subplot_size = 6  # in inches

    # Calculate the total figure size
    total_width = subplot_size
    total_height = 1.25 * subplot_size

    # Create the figure and subplots with adjusted size and spacing
    fig, (ax1, ax2) = plt.subplots(
        2, 1,
        figsize=(total_width, total_height),
        gridspec_kw={'height_ratios': [1, 1]}
    )
    fig.suptitle(title, fontsize=30, y=0.95)

    ### Subplot 1: Scatter Plot
    # Plot the data
    ax1.scatter(data_bif[:, 0], data_bif[:, 1], s=0.05, c='k')

    # Set labels and format
    ax1.set_ylabel(r'$x_{\text{max}}$', fontsize=25)
    ax1.set_xticks([])
    ax1.tick_params(axis='both', which='major', labelsize=16)
    ax1.set_xlim([data_bif[:, 0].min(), data_bif[:, 0].max()])
    ax1.xaxis.set_major_formatter(FuncFormatter(format_func))
    ax1.yaxis.set_major_formatter(FuncFormatter(format_func))

    # Set y-limits to data range
    y_min_ax1 = data_bif[:, 1].min()
    y_max_ax1 = data_bif[:, 1].max()
    ax1.set_ylim([y_min_ax1, y_max_ax1])

    # Set y-ticks
    ax1.set_yticks(np.linspace(y_min_ax1, y_max_ax1, 5))

    # Adjust aspect ratio
    ax1.set_aspect('auto')

    ### Subplot 2: Line Plot
    # Plot the data
    ax2.plot(data_lya[:, 0], data_lya[:, 1], label="LE 1", linewidth=1.5, c='b')
    ax2.plot(data_lya[:, 0], data_lya[:, 3], label="LE 2", linewidth=1.5, c='r')

    # Set labels and format

    ax2.set_xlabel(x_name, fontsize=25)
    ax2.set_ylabel(r'$LEs$', fontsize=25)
    ax2.tick_params(axis='both', which='major', labelsize=16)
    ax2.set_xlim([data_lya[:, 0].min(), data_lya[:, 0].max()])
    ax2.set_xticks(np.linspace(data_lya[:, 0].min(), data_lya[:, 0].max(), 5))
    ax2.xaxis.set_major_formatter(FuncFormatter(format_func))
    ax2.yaxis.set_major_formatter(FuncFormatter(format_func))
    ax2.legend(fontsize=12, loc='lower left', bbox_to_anchor=leg_pos)
    ax2.grid(True)

    # Set y-limits to data range

    y_min_ax2 = min(data_lya[:, 1][~np.isnan(data_lya[:, 1])].min(),
                    data_lya[:, 3][~np.isnan(data_lya[:, 3])].min())
    y_max_ax2 = max(data_lya[:, 1][~np.isnan(data_lya[:, 1])].max(),
                    data_lya[:, 3][~np.isnan(data_lya[:, 3])].max())
    ax2.set_ylim([y_min_ax2, y_max_ax2])

    # Set y-ticks
    ax2.set_yticks(np.linspace(y_min_ax2, y_max_ax2, 5))

    # Adjust aspect ratio
    ax2.set_aspect('auto')

    # **Adjust layout to prevent labels from being cut off**
    plt.tight_layout()

    # If labels are still cut off, adjust the margins manually
    # fig.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.1, hspace=0.3)

    # --- Save the figure in both PNG and EPS formats ---
    base_output_path = os.path.join(output_dir, save_name)
    plt.savefig(base_output_path + '.png', format='png', dpi=100)
    plt.savefig(base_output_path + '.eps', format='eps')
    plt.close()
```

#### File: `plot_error_system.py`
```python
import os
import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt
import diffrax as dfx
import jax.numpy as jnp
from src.hr_model.error_system import HRNetworkErrorSystem
from src.hr_model.model import DEFAULT_PARAMS

# ======================================================================
# 1. SIMULATION SETUP & EXECUTION
# ======================================================================

# --- Initial Conditions & Parameters ---
# Initial state (x, y, z, u, œÜ for each neuron)
INITIAL_HR_STATE0 = [
    0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
    0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
]

# [cite_start]External currents and coupling matrix [cite: 151]
I_ext = [0.8, 0.8]
xi = [[0, 1], [1, 0]]
sim_params = DEFAULT_PARAMS.copy()

# --- Create Simulator Instance ---
# Using 'complete' dynamics as in the example
simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                 hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

# --- Integration Settings ---
start_time = 0
end_time = 1000
dt_initial = 0.01
point_num = 10000
transient_ratio = 0
n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
max_steps = int((end_time - start_time) / dt_initial) * 20
solver = dfx.Tsit5()
stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)

# --- Run Simulation ---
print("Running simulation...")
simulator.solve(
    solver=solver,
    t0=start_time,
    t1=end_time,
    dt0=dt_initial,
    n_points=n_points,
    stepsize_controller=stepsize_controller,
    max_steps=max_steps
)
print("Simulation finished.")

# --- Get Results ---
results = simulator.get_results_dict(transient_ratio)


# ======================================================================
# 2. PLOTTING
# This section is preserved from the original plot_error_system.py
# ======================================================================

# --- Define the Directory ---
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Error System')
os.makedirs(output_dir, exist_ok=True)
print(f"Saving figures to '{output_dir}/'")

# --- Define Plotting Series ---
series = [
    ('e_x', r'$e_x$', 'darkblue', '(a)'),
    ('e_y', r'$e_y$', 'C2', '(b)'),
    ('e_z', r'$e_z$', 'C1', '(c)'),
    ('e_u', r'$e_u$', 'tomato', '(d)'),
    ('e_phi', r'$e_\phi$', 'purple', '(e)'),
]

# --- Generate and Save Plots ---
for key, label, color, title in series:
    # Create a new square figure with high resolution and tight layout
    fig, ax = plt.subplots(
        figsize=(4, 4),
        dpi=200,
        constrained_layout=True)

    # Plot the data
    ax.plot(results['t'], results[key], c=color, linewidth=1.5)

    # Set labels, title, grid, aspect ratio, and margins
    ax.set_xlabel(r'$t$',   fontsize=18)
    ax.set_ylabel(label,    fontsize=18)
    ax.set_title(f'{title.strip("$")}', fontsize=20)
    ax.grid(True)
    ax.set_box_aspect(1.0)
    ax.margins(x=0)

    # Define save paths and save the figure in both PNG and EPS formats
    png_path = os.path.join(output_dir, f'{key}.png')
    eps_path = os.path.join(output_dir, f'{key}.eps')
    fig.savefig(png_path, format='png', dpi=200)
    fig.savefig(eps_path, format='eps')

    plt.close(fig)

print("All plots have been generated and saved successfully.")
```

#### File: `plot_lya2D_heatmap.py`
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors as mcolors
import os



import matplotlib
matplotlib.use("Qt5Agg")  # <-- assuming you meant Qt5Agg (q5tgg)
import matplotlib.pyplot as plt


# --- Define the Directory ---
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'Lyapanov 2D/')
os.makedirs(path, exist_ok=True)

file_names = ['Max_Lya_k_m.txt', 'Max_Lya_rho_m.txt', 'Max_Lya_rho_k_m_1.txt',
              'Max_Lya_rho_k_m_0.5.txt', 'Max_Lya_rho_k_m_0.25.txt']

save_names = ['K_M', 'Rho_M', 'Rho_K0_M_1',
              'Rho_K0_M_0.5', 'Rho_K0_M_0.25']

x_names = [r'$k$', r'$\rho$', r'$\rho$', r'$\rho$', r'$\rho$']
y_names = [r'$m$', r'$m$', r'$k$', r'$k$', r'$k$']
titles = ['(a)', '(b)', '(c1)', '(c2)', '(c3)']

global_min = float('inf')
global_max = float('-inf')

# First pass: Determine global min and max across all files
for file_name in file_names:
    df = pd.read_csv(path + file_name, delim_whitespace=True, header=None)
    pivot_table = df.pivot(index=0, columns=2, values=1).fillna(0)
    array_2d = np.rot90(pivot_table.to_numpy(), k=1)
    global_min = min(global_min, np.nanmin(array_2d))
    global_max = max(global_max, np.nanmax(array_2d))

# Iterate through files again to create plots with consistent colorbar
for file_name, save_name, x_name, y_name, title in zip(
        file_names, save_names, x_names, y_names, titles):

    # Read the file into a DataFrame
    df = pd.read_csv(path + file_name, delim_whitespace=True, header=None)

    # Pivot the DataFrame to reshape it into a 2D array
    pivot_table = df.pivot(index=0, columns=2, values=1)

    # Fill NaNs with a value if necessary, h.g., 0
    pivot_table = pivot_table.fillna(0)

    # Convert to a 2D numpy array
    array_2d = pivot_table.to_numpy().T

    # Define custom colormap: black to yellow to red
    colors = ['black', 'yellow', 'red']
    n_bins = int((global_max * 100 / (
                global_max - global_min)) * 128) if global_min != global_max else 256  # Handle case where min == max
    cmap_custom = mcolors.LinearSegmentedColormap.from_list("custom", colors, N=n_bins)

    # Combine white for negative values with the custom colormap
    if global_min < 0:
        cmap_pos = cmap_custom(np.linspace(0, 1, n_bins))
        cmap_min_len = int(
            (abs(global_min) * 100 / (global_max - global_min)) * 128) if global_min != global_max else 128
        cmap_min = np.array([[1, 1, 1, 1]] * cmap_min_len)
        cmap_combined = np.vstack((cmap_min, cmap_custom(np.linspace(0, 1, n_bins))))
        cmap = mcolors.ListedColormap(cmap_combined)
        vmin_plot = global_min
        vmax_plot = global_max
    else:
        cmap = cmap_custom
        vmin_plot = global_min
        vmax_plot = global_max

    # Plot the heatmap
    fig, ax = plt.subplots(figsize=(8, 8), dpi=100)
    plt.suptitle(title, fontsize=30, y=0.90)
    img = ax.imshow(array_2d, aspect='auto', cmap=cmap, origin='lower',
                    extent=[pivot_table.index.min(), pivot_table.index.max(),
                            pivot_table.columns.min(), pivot_table.columns.max()],
                    vmin=vmin_plot, vmax=vmax_plot)

    # --- Mouse readout: x, y, z ---
    # Use the same array you plotted:
    arr = array_2d  # if you plotted 'array_2d' in imshow
    # arr = masked.filled(np.nan)  # use this instead if you plotted 'masked'

    # Get the plotted extent from the image (left, right, bottom, top)
    x0, x1, y0, y1 = img.get_extent()

    # Array shape: rows (y), cols (x)
    ny, nx = arr.shape
    dx = (x1 - x0) / (nx - 1) if nx > 1 else 1.0
    dy = (y1 - y0) / (ny - 1) if ny > 1 else 1.0


    def format_coord(x, y):
        col = int(round((x - x0) / dx))
        row = int(round((y - y0) / dy))
        if 0 <= row < ny and 0 <= col < nx:
            z = arr[row, col]
            if np.isnan(z):
                return f"x={x:.3f}, y={y:.3f}, z=NaN"
            return f"x={x:.3f}, y={y:.3f}, z={z:.5g}"
        return f"x={x:.3f}, y={y:.3f}"


    ax.format_coord = format_coord
    # <<< end add this >>>

    # Add colorbar with reduced distance from plot
    cbar = fig.colorbar(img, ax=ax, pad=0.02)

    # Set colorbar ticks
    colorbar_ticks = []
    if global_min < 0:
        colorbar_ticks.append(round(global_min, 2))
    colorbar_ticks.append(0.00)
    if global_max > 0:
        num_intervals = 3
        positive_ticks = np.linspace(0, global_max, num_intervals + 1)[1:]  # Exclude 0
        colorbar_ticks.extend([round(tick, 2) for tick in positive_ticks])
        colorbar_ticks.append(round(global_max, 2))
    elif global_max == 0 and global_min < 0:
        colorbar_ticks.append(round(global_max, 2))

    colorbar_ticks = sorted(list(set(colorbar_ticks)))  # Remove duplicates and sort
    cbar.set_ticks(colorbar_ticks)
    cbar.set_ticklabels([f'{tick:.2f}' for tick in colorbar_ticks])

    # Set colorbar label at the top
    cbar.ax.set_title(r'$\lambda_{\mathrm{max}}$', pad=20, fontsize=25)

    # Set axis labels
    ax.set_xlabel(x_name, fontsize=25)
    ax.set_ylabel(y_name, fontsize=25)

    # Set font size for axis ticks
    ax.tick_params(axis='both', which='major', labelsize=20)

    # Set font size for colorbar ticks
    cbar.ax.tick_params(labelsize=20)

    # --- Set x-axis ticks ---
    x_min, x_max = pivot_table.index.min(), pivot_table.index.max()
    x_ticks = np.linspace(x_min, x_max, 5)
    ax.set_xticks(x_ticks)
    ax.set_xticklabels([f'{tick:.2f}' for tick in x_ticks])

    # --- Set y-axis ticks ---
    y_min, y_max = pivot_table.columns.min(), pivot_table.columns.max()
    y_ticks = np.linspace(y_min, y_max, 5)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels([f'{tick:.2f}' for tick in y_ticks])

    # Adjust layout to prevent cropping
    plt.tight_layout()

    # --- Save the figure in both PNG and EPS formats ---
    # base_output_path = os.path.join(path, save_name)
    # plt.savefig(base_output_path + '.png', format='png', dpi=300)
    # plt.savefig(base_output_path + '.eps', format='eps')
    # plt.close()

    plt.show(block=True)  # show in interactive window (PyCharm)
    plt.close()

```

#### File: `plot_v_h_dot_1D.py`
```python
import matplotlib
matplotlib.use("Qt5Agg")
import matplotlib.pyplot as plt
import numpy as np
import os

# --- Define Directory ---
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'V_H_DOT_1D/')
os.makedirs(path, exist_ok=True)

file_name = 'V_H_DOT_k_-2.5_2.npz'

data = np.load(path + file_name)
PARAM_VALUES = data['PARAM_VALUES']
mean_dVdt = data['mean_dVdt']
mean_dHdt = data['mean_dHdt']
TARGET_PARAM = data['TARGET_PARAM']

# ‚îÄ‚îÄ Combined Plot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

import matplotlib
matplotlib.use('Qt5Agg')
import matplotlib.pyplot as plt

# mask out any non-finite values
x_vals = np.asarray(PARAM_VALUES, dtype=float)

# compute x-limits (assuming PARAM_VALUES has no NaN/Inf)
x_min, x_max = np.nanmin(x_vals), np.nanmax(x_vals)

# create the figure and axes
fig, ax = plt.subplots(
    figsize=(4, 4),
    dpi=200,
    constrained_layout=True
)

# plot your two curves
ax.plot(PARAM_VALUES, mean_dVdt, color="blue", label=r'$dV/dt$')
ax.plot(PARAM_VALUES, mean_dHdt, color="red", label=r'$dH/dt$')

# optional: ensure ticks include endpoints
ax.set_xticks([x_min] + list(ax.get_xticks()) + [x_max])

# labels, title, grid, legend
ax.set_xlabel(r'$k$', fontsize=18)
ax.set_ylabel(r'$dV/dt, dH/dt$', fontsize=18)
ax.set_title("(b)", fontsize=20)
ax.grid(True)
ax.legend(loc="best", fontsize=14)

# enforce square aspect and remove x-margins
ax.set_box_aspect(1.0)
ax.set_xlim(x_min, x_max)
ax.margins(x=0)

png_path = os.path.join(path, f'k.png')
eps_path = os.path.join(path, f'k.eps')
fig.savefig(png_path, format='png', dpi=200)
fig.savefig(eps_path, format='eps')



```

#### File: `plot_v_h_dot_2D.py`
```python
# plot_vdot_2d_swapped.py  ‚Äì PARAM_X on x-axis, PARAM_Y on y-axis
# --------------------------------------------------------------

# import matplotlib
# matplotlib.use("Qt5Agg")
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import os

# ‚îÄ‚îÄ Edit these two lines to your file location ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'V_H_DOT_2D/')
os.makedirs(path, exist_ok=True)

file_name = 'VDOT_ge_0-1_rho_0.3-0.9.npz'

# ‚îÄ‚îÄ Load data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
data = np.load(path + file_name)
x_vals     = data['x_vals']                 # PARAM_X values
y_vals     = data['y_vals']                 # PARAM_Y values
mean_dVdt  = data['mean_dVdt']              # shape (NX, NY)
mean_dHdt  = data['mean_dHdt']

# maskout some values
mean_dHdt[np.where(mean_dHdt < -1)] = np.nan
mean_dHdt[np.where(mean_dHdt > 350)] = np.nan

PARAM_X = data['PARAM_X'].item()
PARAM_Y = data['PARAM_Y'].item()

# ‚îÄ‚îÄ Build grid with x on rows, y on columns (needs transpose) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Xg, Yg = np.meshgrid(x_vals, y_vals)        # shapes (NY, NX)

# Helper to compute 5 ticks from data grid
def five_ticks(grid):
    mn, mx = np.min(grid), np.max(grid)
    ticks = np.linspace(mn, mx, 5)
    ticks = np.round(ticks, 1)
    return ticks

# ‚îÄ‚îÄ Plot 1 : „ÄàdV/dt„Äâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
fig_dV, ax_dV = plt.subplots(figsize=(4, 4), dpi=200, constrained_layout=True)

ax_dV.set_box_aspect(1.0)
pcm1 = ax_dV.pcolormesh(Xg, Yg, mean_dVdt.T, shading='auto', cmap='seismic')
ax_dV.set_xlabel(r'$ge$', fontsize=18)
ax_dV.set_ylabel(r'$\rho$', fontsize=18)
ax_dV.set_title('(a3)', fontsize=18)

# set 5 ticks on x and y
xticks = five_ticks(Xg)
yticks = five_ticks(Yg)
ax_dV.set_xticks(xticks)
ax_dV.set_yticks(yticks)
ax_dV.set_xticklabels([f"{t:.1f}" for t in xticks])
ax_dV.set_yticklabels([f"{t:.1f}" for t in yticks])

# colorbar with title‚Äêlabel on top
cbar1 = fig_dV.colorbar(pcm1, ax=ax_dV, pad=0.0001, fraction=0.05)
cbar1.ax.set_title(r'$dV/dt$', pad=7, fontdict={'fontsize':15})

# ‚îÄ‚îÄ Plot 2 : „ÄàdH/dt„Äâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
fig_dH, ax_dH = plt.subplots(figsize=(4, 4), dpi=200, constrained_layout=True)

ax_dH.set_box_aspect(1.0)
pcm2 = ax_dH.pcolormesh(Xg, Yg, mean_dHdt.T, shading='auto', cmap='seismic')
ax_dH.set_xlabel(r'$ge$', fontsize=18)
ax_dH.set_ylabel(r'$\rho$', fontsize=18)
ax_dH.set_title('(b3)', fontsize=18)

# set 5 ticks on x and y
ax_dH.set_xticks(xticks)
ax_dH.set_yticks(yticks)
ax_dH.set_xticklabels([f"{t:.1f}" for t in xticks])
ax_dH.set_yticklabels([f"{t:.1f}" for t in yticks])

cbar2 = fig_dH.colorbar(pcm2, ax=ax_dH, pad=0.0001, fraction=0.05)
cbar2.ax.set_title(r'$dH/dt$', pad=7, fontdict={'fontsize':15})

png_dV_path = os.path.join(path, f'rho_V.png')
eps_dV_path = os.path.join(path, f'rho_V.eps')
fig_dV.savefig(png_dV_path, format='png', dpi=200)
fig_dV.savefig(eps_dV_path, format='eps')

png_dH_path = os.path.join(path, f'rho_H.png')
eps_dH_path = os.path.join(path, f'rho_H.eps')
fig_dH.savefig(png_dH_path, format='png', dpi=200)
fig_dH.savefig(eps_dH_path, format='eps')



```

================================================================================
## Folder: src/hr_model
------------------------------------------------------------
### Python Source Files:

#### File: `error_system.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)
from src.hr_model.model import HindmarshRose as HR
from src.hr_model.model import DEFAULT_PARAMS

# Default initial state for two neurons
DEFAULT_HR_STATES0 = [0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4, 0.5, 0.2]

class HRNetworkErrorSystem:
    """
    Simulates a system combining a 2‚Äëneuron Hindmarsh‚ÄëRose network
    (using the HindmarshRose class) and an associated error system.
    """
    NUM_ERROR_VARS = 5

    def __init__(self, params=None, dynamics=None, hr_initial_state=None, I_ext=None, hr_xi=None):
        self.dynamics = dynamics
        self.params = params.copy()
        self.hr_initial_state = jnp.array(hr_initial_state, dtype=jnp.float64)
        self.error_initial_state = (self.hr_initial_state[len(self.hr_initial_state) // 2:]
                                    - self.hr_initial_state[:len(self.hr_initial_state) // 2])
        self.xi = jnp.array(hr_xi, dtype=jnp.float64)
        self.I_ext = jnp.array(I_ext, dtype=jnp.float64)

        # --- Create the HindmarshRose instance ---
        self.hr_network = HR(
            N=2,
            params=self.params,
            initial_state=self.hr_initial_state,
            I_ext=self.I_ext,
            xi=self.xi
        )

        # --- Combine initial states for the overall system ---
        self.combined_state0 = jnp.concatenate([self.hr_initial_state, self.error_initial_state], dtype=jnp.float64)

        # Attributes to store results
        self.t = None
        self.solution = None
        self.derivative = None
        self.failed = None

    @staticmethod
    def complete_vector_field(current_error_state, current_hr_state, params):
        """
        Calculates derivatives for the 'complete' error system dynamics.
        This static method can be called from anywhere without needing an instance.
        """
        # Unpack HR state variables needed by the error system
        x1, _, _, u1, phi1 = current_hr_state[0:5]
        _, _, _, u2, _ = current_hr_state[5:10]

        # Unpack current error state
        e_x, e_y, e_z, e_u, e_phi = current_error_state

        # --- Complete Error System Derivatives ---
        de_xdt = ((((e_y - (params['a'] * ((e_x ** 3) + (3 * (e_x ** 2) * x1) + (3 * e_x * (x1 ** 2))))
                     + (params['b'] * ((e_x ** 2) + (2 * e_x * x1))) + (params['k'] * params['h'] * e_x))
                    + (params['k'] * params['f'] * ((x1 * (e_u ** 2)) + (2 * u1 * x1 * e_u) + (e_x * (e_u ** 2)) +
                                                    (2 * u1 * e_x * e_u) + ((u1 ** 2) * e_x))))
                   + (params['rho'] * ((phi1 * e_x) + (e_x * e_phi) + (x1 * e_phi))) - (2 * params['ge'] * e_x))
        )
        de_ydt = -params['d'] * ((e_x ** 2) + (2 * e_x * x1)) - e_y
        de_zdt = params['r'] * ((params['s'] * e_x) - e_z)

        # --- piece-wise de_u/dt ---
        condlist = [
            (u1 >= 1) & (-1 < u2) & (u2 < 1), (u1 >= 1) & (u2 <= -1),
            (-1 < u1) & (u1 < 1) & (u2 >= 1), (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
            (-1 < u1) & (u1 < 1) & (u2 <= -1), (u1 <= -1) & (u2 >= 1),
            (u1 <= -1) & (-1 < u2) & (u2 < 1),
        ]
        choicelist = [
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 - 1), e_x - e_u - 4 * params['m'],
            e_x - e_u - 2 * params['m'] * (u1 - 1), e_x + (2 * params['m'] - 1) * e_u,
            e_x - e_u - 2 * params['m'] * (u1 + 1), e_x - e_u + 4 * params['m'],
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 + 1),
        ]
        de_udt = jnp.select(condlist, choicelist, default=e_x - e_u)
        de_phidt = e_x - params['q'] * e_phi

        return jnp.array([de_xdt, de_ydt, de_zdt, de_udt, de_phidt], dtype=jnp.float64)

    @staticmethod
    def simplified_vector_field(current_error_state, current_hr_state, params):
        """
        Calculates derivatives for the 'simplified' error system dynamics.
        This static method can be called from anywhere without needing an instance.
        """
        # Unpack HR state variables needed by the error system
        x1, _, _, u1, phi1 = current_hr_state[0:5]
        _, _, _, u2, _ = current_hr_state[5:10]

        # Unpack current error state
        e_x, e_y, e_z, e_u, e_phi = current_error_state

        # --- Simplified Error System Derivatives ---
        de_xdt = (
                (((-3 * params['a'] * (x1 ** 2)) + (2 * params['b'] * x1) + (params['k'] * params['h'])
                  + (params['k'] * params['f'] * (u1 ** 2)) + (params['rho'] * phi1) - (2 * params['ge'])) * e_x)
                + e_y + (2 * params['k'] * params['f'] * u1 * x1 * e_u) + (params['rho'] * x1 * e_phi)
        )
        de_ydt = (-2 * params['d'] * x1 * e_x) - e_y
        de_zdt = params['r'] * ((params['s'] * e_x) - e_z)

        # --- piece-wise de_u/dt ---
        condlist = [
            (u1 >= 1) & (-1 < u2) & (u2 < 1), (u1 >= 1) & (u2 <= -1),
            (-1 < u1) & (u1 < 1) & (u2 >= 1), (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
            (-1 < u1) & (u1 < 1) & (u2 <= -1), (u1 <= -1) & (u2 >= 1),
            (u1 <= -1) & (-1 < u2) & (u2 < 1),
        ]
        choicelist = [
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 - 1), e_x - e_u - 4 * params['m'],
            e_x - e_u - 2 * params['m'] * (u1 - 1), e_x + (2 * params['m'] - 1) * e_u,
            e_x - e_u - 2 * params['m'] * (u1 + 1), e_x - e_u + 4 * params['m'],
            e_x + (2 * params['m'] - 1) * e_u + 2 * params['m'] * (u1 + 1),
        ]
        de_udt = jnp.select(condlist, choicelist, default=e_x - e_u)
        de_phidt = e_x - (params['q'] * e_phi)

        return jnp.array([de_xdt, de_ydt, de_zdt, de_udt, de_phidt], dtype=jnp.float64)

    def _error_system_ode(self, t, current_error_state, current_hr_state):
        """Calculates derivatives ONLY for the error system variables by calling the appropriate static method."""
        if self.dynamics == "simplified":
            return HRNetworkErrorSystem.simplified_vector_field(current_error_state, current_hr_state, self.params)
        elif self.dynamics == "complete":
            return HRNetworkErrorSystem.complete_vector_field(current_error_state, current_hr_state, self.params)
        else:
            raise ValueError(f"Unknown dynamics mode: {self.dynamics}")

        # In your HRNetworkErrorSystem class...

    def _clipped_ode_func(self, t, combined_state, args):
        """
        Combined HR-network + error-system ODE for SIMPLIFIED dynamics.
        The ERROR variables are box-constrained to self.error_clip_bound.
        """
        # Add near the top of your class for clarity
        ERROR_SLICE = slice(10, 15)  # indices of e_x ‚Ä¶ e_phi
        LOW, HIGH = [-5, 5] # box limits

        # ‚îÄ‚îÄ 1. Unpack raw (unclipped) state ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        hr_state = combined_state[:10]  # x1‚Ä¶phi2
        err_state = combined_state[ERROR_SLICE]  # e_x‚Ä¶e_phi

        # ‚îÄ‚îÄ 2. Compute raw derivatives ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        hr_d = self.hr_network._ode_func_internal(t, hr_state)
        err_d = self._error_system_ode(t, err_state, hr_state)
        combined_derivatives = jnp.concatenate([hr_d, err_d])

        # ‚îÄ‚îÄ 3. Velocity-clamp ONLY the error-state derivatives ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        err_d_clamped = jnp.where(
            ((err_state >= HIGH) & (err_d > 0.0)) |  # trying to exit above +5
            ((err_state <= LOW) & (err_d < 0.0)),  # trying to exit below ‚àí5
            0.0,  # stop outward motion
            err_d  # keep everything else
        )

        final_derivatives = combined_derivatives.at[ERROR_SLICE].set(err_d_clamped)
        return final_derivatives

    def _unclipped_ode_func(self, t, combined_state, args):
        """
        Combined HR-network + error-system ODE for COMPLETE dynamics (no clipping).
        """
        # --- Unpack the combined state vector ---
        current_hr_state = combined_state[0:10]
        current_error_state = combined_state[10:15]

        # --- Calculate HR Derivatives using the HindmarshRose instance ---
        hr_derivatives_flat = self.hr_network._ode_func_internal(t, current_hr_state)

        # --- Calculate Error System Derivatives ---
        error_derivatives = self._error_system_ode(t, current_error_state, current_hr_state)

        # --- Combine derivatives into a single vector ---
        combined_derivatives = jnp.concatenate([hr_derivatives_flat, error_derivatives])
        return combined_derivatives

    # ------------------------------------------------------------------
    # Diffrax‚Äëbased solver
    # ------------------------------------------------------------------
    def solve(self, solver=None, t0=None, t1=None, dt0=None, n_points=None,
              stepsize_controller=None, max_steps=None):
        """
        Integrate the combined HR + error system with Diffrax.
        """
        # --- Choose the correct vector field function BEFORE calling the solver ---
        if self.dynamics == "simplified":
            vector_field = self._clipped_ode_func
        elif self.dynamics == "complete":
            vector_field = self._unclipped_ode_func
        else:
            # This case should not be reached with the current code
            raise ValueError(f"Unknown dynamics mode: {self.dynamics}")

        try:
            sol = dfx.diffeqsolve(
                terms=dfx.ODETerm(vector_field),  # Use the selected function
                solver=solver,
                t0=t0,
                t1=t1,
                dt0=dt0,
                y0=self.combined_state0,
                saveat=n_points,
                stepsize_controller=stepsize_controller,
                max_steps=max_steps,
                args=None
            )
        except Exception as exc:
            print(f"Solver failed with exception: {exc}")
            self.failed = True
            self.t = self.solution = None
            return np.nan, np.nan

        # Normal exit ------------------------------------------------------
        self.failed = False
        self.t = jnp.asarray(sol.ts)
        self.solution = jnp.asarray(sol.ys)
        # Recompute derivative using the selected vector field
        self.derivative = jnp.asarray(jax.vmap(vector_field, in_axes=(0, 0, None))(sol.ts, sol.ys, None))

    # ------------------------------------------------------------------
    # Extract results
    # ------------------------------------------------------------------
    def get_results_dict(self, transient_ratio: float = 0.0):
        """
        Return a dict containing the post-transient time vector ('t') and all
        state variables of the combined HR + error system.
        """
        # ‚îÄ‚îÄ locate cut-off index using the actual (non-uniform) time stamps ‚îÄ
        cutoff_time = self.t[0] + transient_ratio * (self.t[-1] - self.t[0])
        start_idx = jnp.searchsorted(self.t, cutoff_time, side="left")

        t_post   = self.t[start_idx:]
        sol_post = self.solution[start_idx:]
        deriv_post = self.derivative[start_idx:]

        var_names = [
            'x1', 'y1', 'z1', 'u1', 'phi1',
            'x2', 'y2', 'z2', 'u2', 'phi2',
            'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'
        ]

        deriv_names = [f'd_{name}' for name in var_names]

        result = {'t': t_post}
        result.update({name: sol_post[:, i] for i, name in enumerate(var_names)})
        result.update({name: deriv_post[:, i] for i, name in enumerate(deriv_names)}) # Use pre-saved derivatives
        return result


# --- Example Usage ---
if __name__ == '__main__':
    from visualization.plotting import plot_error_and_state_differences

    # initial state (x, y, z, u, œÜ for each neuron)
    INITIAL_HR_STATE0 = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext = [0.8, 0.8]
    xi = [[0, 1], [1, 0]]

    # Example modification of parameters
    sim_params = DEFAULT_PARAMS.copy()
    # sim_params['ge'] = 0.65

    # Create simulator instance
    simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                     hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.01
    point_num = 10000
    transient_ratio = 0
    n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
    max_steps = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run simulation ----------------------------------------------------
    print("Running simulation...")
    import time
    tic = time.perf_counter()

    simulator.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps
    )

    toc = time.perf_counter()
    print(f"Finished in {(toc - tic):.2f} s")

    # Get results dictionary
    results = simulator.get_results_dict(transient_ratio)

    # Plotting
    plot_error_and_state_differences(results)

    # # # Write to file
    # # import pickle
    # # with open('error_system.pkl', 'wb') as f:
    # #     pickle.dump(results, f)
```

#### File: `model.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Default parameters dictionary
DEFAULT_PARAMS = {
    # --- Neuron Params ---
    'a': 1.0,
    'b': 3.0,
    'c': 1.0,
    'd': 5.0,
    'f': 0.2,
    'h': 0.3,
    'k': 0.87,
    'm': 0.5,
    'q': 0.005,
    'r': 0.006,
    's': 5.2,
    'x0': -1.56,
    'rho': 0.7,
    'ge': 0.62,
}

# Default initial state for a single neuron
DEFAULT_STATE0 = [0.0, 0.0, 0.0, 1.0, 0.0]

class HindmarshRose:
    """Simulates a network of N coupled Hindmarsh-Rose neurons."""

    def __init__(self, N, params=None, initial_state=None, I_ext=None, xi=None):
        self.N = N
        self.params = params.copy()
        self.initial_state = jnp.array(initial_state, dtype=jnp.float64).flatten()

        # External current
        if isinstance(I_ext, (int, float)):
            self.I_ext = jnp.full(self.N, float(I_ext), dtype=jnp.float64)
        else:
            self.I_ext = jnp.array(I_ext, dtype=jnp.float64)

        # Electrical coupling matrix
        if isinstance(xi, (int, float)):
            self.xi = jnp.full((self.N, self.N), float(xi), dtype=jnp.float64)
        else:
            self.xi = jnp.array(xi, dtype=jnp.float64)
        if self.N > 0:
            self.xi = jnp.fill_diagonal(self.xi, 0, inplace=False)

        # Attributes to store results later
        self.t = None
        self.solution = None
        self.failed = None

    @staticmethod
    def vector_field(t, state, N, params, I_ext, xi):
        """
        Calculates the time derivatives for a network of N coupled Hindmarsh-Rose neurons.
        This static method contains the core physics and can be called from anywhere
        without needing an instance of the class, e.g., HindmarshRose.vector_field(...)
        """
        # Reshape the flat state vector into a 2D array (N neurons x 5 variables)
        state_matrix = state.reshape((N, 5))
        x, y, z, u, phi = state_matrix.T

        # Electrical Coupling
        x_diff = x[jnp.newaxis, :] - x[:, jnp.newaxis]
        electrical_coupling = params['ge'] * jnp.sum(jnp.asarray(xi) * x_diff, axis=1)

        # --- Calculate derivatives ---
        dxdt = (y - (params['a'] * x ** 3) + (params['b'] * x ** 2)
                + (params['k'] * (params['h'] + (params['f'] * (u ** 2))) * x)
                + (params['rho'] * phi * x) + I_ext
                + electrical_coupling
                )
        dydt = params['c'] - (params['d'] * x ** 2) - y
        dzdt = params['r'] * (params['s'] * (x + params['x0']) - z)
        dudt = -u + (params['m'] * (jnp.abs(u + 1.0) - jnp.abs(u - 1.0))) + x
        dphidt = x - (params['q'] * phi)

        # Assign calculated derivative vectors to the output matrix
        d_state_dt_matrix = jnp.zeros_like(state_matrix).at[:, 0].set(dxdt).at[:, 1].set(dydt) \
                                .at[:, 2].set(dzdt).at[:, 3].set(dudt) \
                                .at[:, 4].set(dphidt)

        return d_state_dt_matrix.flatten()

    def _ode_func_internal(self, t, state):
        # This instance method now calls the static method, passing its instance attributes.
        return HindmarshRose.vector_field(t, state, self.N, self.params, self.I_ext, self.xi)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def solve(self, solver=None, t0=None, t1=None, dt0=None, n_points=None,
              stepsize_controller=None, max_steps=None):
        """Integrate the model with Diffrax."""

        try:
            sol = dfx.diffeqsolve(
                terms=dfx.ODETerm(lambda t, y, args: self._ode_func_internal(t, y)),
                solver=solver,
                t0=t0,
                t1=t1,
                dt0=dt0,
                y0=self.initial_state,
                saveat=dfx.SaveAt(ts=jnp.linspace(t0, t1, n_points)),
                stepsize_controller=stepsize_controller,
                max_steps=max_steps
            )
        except Exception as exc:
            print(f"Solver failed with exception: {exc}")
            self.failed = True
            self.t = self.solution = None
            return np.nan, np.nan

        self.t = jnp.asarray(sol.ts)
        self.solution = jnp.asarray(sol.ys)
        self.failed = False
        self.derivative = jnp.asarray(jax.vmap(self._ode_func_internal)(sol.ts, sol.ys))

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def get_results_dict(self, transient_ratio: float = 0.0):
        """
        Return the simulated time series as a dict *after* removing the initial
        transient period.
        """
        # <<< 4. MODIFIED: Entire method updated to handle derivatives
        if self.t is None:
            print("Simulation has not been run or has failed. Returning empty dictionary.")
            return {}

        # ‚îÄ‚îÄ locate cut-off index using the actual time stamps ‚îÄ
        cutoff_time = self.t[0] + transient_ratio * (self.t[-1] - self.t[0])
        start_idx = jnp.searchsorted(self.t, cutoff_time, side="left")

        # --- Slice all result arrays to remove transient ---
        t_post = self.t[start_idx:]
        sol_post = self.solution[start_idx:]
        deriv_post = self.derivative[start_idx:]

        # --- Create names for state variables and their derivatives ---
        var_names = [f"{v}{i+1}" for i in range(self.N) for v in ("x", "y", "z", "u", "phi")]
        deriv_names = [f"d_{name}" for name in var_names]

        # ‚îÄ‚îÄ assemble and return dict ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        result = {'t': t_post}
        result.update({name: sol_post[:, i] for i, name in enumerate(var_names)})
        result.update({name: deriv_post[:, i] for i, name in enumerate(deriv_names)})

        return result









# --- Example Usage ---
if __name__ == '__main__':
    from visualization.plotting import plot_all_time_series
    import os

    #=======================================================================================
    # --- Example 1: Single Neuron ---
    #=======================================================================================

    print("Simulating Single Neuron...")
    # Initialize using defaults where possible
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.2

    # create the model
    hr_single = HindmarshRose(N=1, params=sim_params, initial_state=DEFAULT_STATE0, I_ext=0.8, xi=0)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.05
    n_points = 10000
    transient_ratio = 0.5
    max_steps  = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-8, atol=1e-10)
    # stepsize_controller = dfx.ConstantStepSize()

    # Run the solver
    import time
    start = time.perf_counter()

    hr_single.solve(
        solver=solver, t0=start_time, t1=end_time, dt0=dt_initial, n_points=n_points,
        stepsize_controller=stepsize_controller, max_steps=max_steps)

    finish = time.perf_counter()
    time = finish - start
    print(time)

    results_single = hr_single.get_results_dict(transient_ratio)

    plot_all_time_series(results_single, N=1, title="One Neuron Simulation", save_fig=0)

    #=======================================================================================
    # --- Example 2: Two Coupled Neurons ---
    #=======================================================================================

    print("\nSimulating Two Coupled Neurons...")

    # Initialize using defaults where possible
    sim_params = DEFAULT_PARAMS.copy()
    sim_params['ge'] = 0.2

    # initial state (x, y, z, u, œÜ for each neuron)
    state0_coupled = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext_coupled = [0.8, 0.8]
    xi_coupled = [[0, 1], [1, 0]]

    # create the model
    hr_coupled = HindmarshRose(
        N=2,
        params=sim_params,
        initial_state=state0_coupled,
        I_ext=I_ext_coupled,
        xi=xi_coupled
    )

    # integration settings
    start_time = 0
    end_time   = 1000
    dt_initial = 0.05
    n_points   = 10000
    transient_ratio = 0.5
    max_steps  = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run the solver
    import time
    start = time.perf_counter()
    hr_coupled.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps)
    finish = time.perf_counter()
    time = finish - start
    print(time)

    results_coupled = hr_coupled.get_results_dict(transient_ratio)

    plot_all_time_series(results_coupled, N=2, title="Two Coupled Neurons Simulation", save_fig=False)
```

#### File: `physics.py`
```python
import numpy as np
import jax, jax.numpy as jnp
import diffrax as dfx
jax.config.update("jax_enable_x64", True)


# ---------------------------------------------------------------------------
# Hamiltonian --------------------------------------------------
# ---------------------------------------------------------------------------
def calculate_H(
    results: dict[str, np.ndarray],
    params:  dict[str, float],
    C: float = 0.0,           # integration constant (defaults to 0)
) -> np.ndarray:
    """
    Return the time series of the Hamiltonian **H** evaluated along a trajectory.

    Parameters
    ----------
    results
        Output of ``Error_System.HRNetworkErrorSystem.get_results_dict``.
        Must contain the keys
        ``'x1', 'u1', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'``.
    params
        Parameter dictionary (e.g. ``NeuralModel.DEFAULT_PARAMS``) with at least
        ``a, b, d, f, h, k, m, q, r, rho, s, ge``.
        Only ``d, k, f, r, rho, s`` are used here.
    C
        Arbitrary additive constant.  Leave at 0 unless you need a specific
        reference level.

    Returns
    -------
    numpy.ndarray
        1-D array of length ``len(results['x1'])`` ‚Äì the Hamiltonian value at
        each recorded time step.
    """
    # ------------------------------------------------------------------
    # 1. Unpack parameters we actually need
    # ------------------------------------------------------------------
    d   = params["d"]
    k   = params["k"]
    f   = params["f"]
    r   = params["r"]
    rho = params["rho"]
    s   = params["s"]

    # ------------------------------------------------------------------
    # 2. Extract trajectory arrays
    # ------------------------------------------------------------------
    required = ("x1", "u1", "e_x", "e_y", "e_z", "e_u", "e_phi")
    try:
        x1, u1, e_x, e_y, e_z, e_u, e_phi = (
            jnp.asarray(results[k]) for k in required
        )
    except KeyError as missing:
        raise KeyError(f"results dict is missing key: {missing}") from None

    # Shape validation (same pattern as in calculate_dHdt)
    ref_shape = x1.shape
    for key, arr in zip(required, (x1, u1, e_x, e_y, e_z, e_u, e_phi)):
        if arr.shape != ref_shape:
            raise ValueError(
                f"Shape mismatch for results['{key}'] ‚Äì expected {ref_shape}, got {arr.shape}"
            )

    # ------------------------------------------------------------------
    # 3. Hamiltonian formula (vectorised)
    # ------------------------------------------------------------------
    H = (
        2 * d * x1 * e_x**2
        + e_y**2
        - 4 * d * k * f * u1 * x1**2 * e_u**2
        - 2 * d * rho * x1**2 * e_phi**2
        + e_z
        - r * s * e_phi
        + C
    )

    # Guard against numerical overflow/underflow
    H = jnp.where(jnp.isfinite(H), H, jnp.nan)
    return H

# ---------------------------------------------------------------------------
# Hamiltonian Rate of Change---------------------------------------
# ---------------------------------------------------------------------------
def calculate_dHdt(results: dict[str, np.ndarray], params: dict[str, float]) -> np.ndarray:  # noqa: N802 ‚Äì keep MATLAB-style name
    """Return the time series of **dH/dt** evaluated along a trajectory.

    Parameters
    ----------
    results
        Dictionary produced by :pymeth:`Error_System.HRNetworkErrorSystem.get_results_dict`.
        Must contain the keys ``'x1', 'u1', 'phi1', 'u2', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'``.
    params
        Parameter dictionary (e.g. :pydata:`NeuralModel.DEFAULT_PARAMS`).
        Must include at least the entries ::

            a, b, d, f, h, k, m, q, r, rho, s, ge

    Returns
    -------
    numpy.ndarray
        1-D array with ``len(results['x1'])`` elements ‚Äì the value of dH/dt at
        each recorded time step.
    """
    # ----------------------------------------------------------------------
    # 1. Unpack parameters
    # ----------------------------------------------------------------------
    a   = params["a"]
    b   = params["b"]
    d   = params["d"]
    f   = params["f"]
    h   = params["h"]
    k   = params["k"]
    m   = params["m"]
    q   = params["q"]
    r   = params["r"]
    rho = params["rho"]
    s   = params["s"]
    ge  = params["ge"]

    # ----------------------------------------------------------------------
    # 2. Extract trajectory arrays
    # ----------------------------------------------------------------------
    required = ("x1", "u1", "phi1", "u2", "e_x", "e_y", "e_z", "e_u", "e_phi")
    try:
        x1, u1, phi1, u2, e_x, e_y, e_z, e_u, e_phi = (
            jnp.asarray(results[k]) for k in required
        )
    except KeyError as missing:
        raise KeyError(f"results dict is missing key: {missing}") from None

    # Shape validation ------------------------------------------------------
    ref_shape = x1.shape
    for key, arr in zip(required, (x1, u1, phi1, u2, e_x, e_y, e_z, e_u, e_phi)):
        if arr.shape != ref_shape:
            raise ValueError(f"Shape mismatch for results['{key}'] ‚Äì expected {ref_shape}, got {arr.shape}")

    # ----------------------------------------------------------------------
    # 3. Helper functions N, alpha, beta (vectorised)
    # ----------------------------------------------------------------------
    N = (
        -3 * a * x1**2 + 2 * b * x1 + k * h + k * f * u1**2 + rho * phi1 - 2 * ge
    )

    conds = [
        (u1 >= 1) & (u2 > -1) & (u2 < 1),
        (u1 >= 1) & (u2 <= -1),
        (u1 > -1) & (u1 < 1) & (u2 >= 1),
        (u1 > -1) & (u1 < 1) & (u2 > -1) & (u2 < 1),
        (u1 > -1) & (u1 < 1) & (u2 <= -1),
        (u1 <= -1) & (u2 >= 1),
        (u1 <= -1) & (u2 > -1) & (u2 < 1),
    ]

    alpha_choices = [2 * m - 1, -1, -1, 2 * m - 1, -1, -1, 2 * m - 1]
    beta_choices  = [
        2 * m * (u1 - 1),
        -4 * m,
        -2 * m * (u1 - 1),
        0,
        -2 * m * (u1 + 1),
        4 * m,
        2 * m * (u1 + 1),
    ]

    alpha = jnp.select(conds, alpha_choices, default=-1)
    beta  = jnp.select(conds, beta_choices,  default=0)

    # ----------------------------------------------------------------------
    # 4. Assemble dH/dt (Eq. Hdot)
    # ----------------------------------------------------------------------
    dHdt = (
        4 * d * x1 * N * e_x**2
        - 2 * e_y**2
        - r * e_z
        - 8 * d * k * f * u1 * x1**2 * alpha * e_u**2
        - 8 * d * k * f * u1 * x1**2 * beta * e_u
        + 4 * d * rho * q * x1**2 * e_phi**2
        + q * r * s * e_phi
    )

    # Guard against numerical overflow/underflow
    dHdt = jnp.where(jnp.isfinite(dHdt), dHdt, jnp.nan)
    return dHdt


# ---------------------------------------------------------------------------
# Lyapunov Rate of change -----------------------------------------
# ---------------------------------------------------------------------------

def calculate_dVdt(results, params):
    """
    Calculates the time series of dV/dt based on simulation results.

    Args:
        results (dict): Dictionary containing the time series output from
                        HRNetworkErrorSystem.get_results_dict(). Must contain keys
                        'x1', 'u1', 'phi1', 'u2', 'e_x', 'e_y', 'e_z', 'e_u', 'e_phi'.
        params (dict): Dictionary containing the model parameters. Must contain
                       keys 'a', 'b', 'k', 'h', 'f', 'rho', 'ge', 'gc', 'lam',
                       'v_syn', 'theta', 'd', 'r', 's', 'q', 'm'.

    Returns:
        np.ndarray: Time series array of dV/dt values. Returns None if
                    required keys are missing.
    """

    # --- Extract Parameters ---
    a = params['a']
    b = params['b']
    k = params['k']
    h = params['h']
    f = params['f']
    rho = params['rho']
    ge = params['ge']
    d = params['d']
    r = params['r']
    s = params['s']
    q = params['q']
    m = params['m']

    # --- Extract Time Series Variables ---
    x1 = results['x1']
    u1 = results['u1']
    phi1 = results['phi1']
    u2 = results['u2']
    e_x = results['e_x']
    e_y = results['e_y']
    e_z = results['e_z']
    e_u = results['e_u']
    e_phi = results['e_phi']

    # Ensure inputs are numpy arrays
    x1, u1, phi1, u2 = jnp.asarray(x1), jnp.asarray(u1), jnp.asarray(phi1), jnp.asarray(u2)
    e_x, e_y, e_z, e_u, e_phi = jnp.asarray(e_x), jnp.asarray(e_y), jnp.asarray(e_z), jnp.asarray(e_u), jnp.asarray(e_phi)

    # --- Calculate dV/dt Components ---
    dVdt_term1 = (((-3 * a * (x1 ** 2)) + (2 * b * x1) + (k * h) +
                  (k * f * (u1 ** 2)) + (rho * phi1) - (2 * ge)) * (e_x ** 2))
    dVdt_term2 = -(e_y ** 2)
    dVdt_term3 = -(r * (e_z ** 2))
    dVdt_term4 = -(q * (e_phi ** 2))
    dVdt_term5 = 2 * k * f * u1 * x1 * e_x * e_u
    dVdt_term6 = (1 - (2 * d * x1)) * e_x * e_y
    dVdt_term7 = r * s * e_x * e_z
    dVdt_term8 = ((rho * x1) + 1) * e_x * e_phi

    # --- Calculate Piecewise Term ---

    conditions = [
        (u1 >= 1) & (-1 < u2) & (u2 < 1),
        (u1 >= 1) & (u2 <= -1),
        (-1 < u1) & (u1 < 1) & (u2 >= 1),
        (-1 < u1) & (u1 < 1) & (-1 < u2) & (u2 < 1),
        (-1 < u1) & (u1 < 1) & (u2 <= -1),
        (u1 <= -1) & (u2 >= 1),
        (u1 <= -1) & (-1 < u2) & (u2 < 1)
    ]
    choices = [
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)) + (2 * m * (u1 - 1) * e_u),
        (e_x * e_u) - (e_u ** 2) - (4 * m * e_u),
        (e_x * e_u) - (e_u ** 2) - (2 * m * (u1 - 1) * e_u),
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)),
        (e_x * e_u) - (e_u ** 2) - (2 * m * (u1 + 1) * e_u),
        (e_x * e_u) - (e_u ** 2) + (4 * m * e_u),
        (e_x * e_u) + ((2 * m - 1) * (e_u ** 2)) + (2 * m * (u1 + 1) * e_u)
    ]
    default_choice = (e_x * e_u) - (e_u ** 2)
    piecewise_term = jnp.select(conditions, choices, default=default_choice)


    # --- Combine all terms ---
    dVdt = (dVdt_term1 + dVdt_term2 + dVdt_term3 + dVdt_term4 +
            dVdt_term5 + dVdt_term6 + dVdt_term7 + dVdt_term8 +
            piecewise_term)

    dVdt = jnp.where(jnp.isfinite(dVdt), dVdt, jnp.nan)     # replace inf / nan by nan

    return dVdt


# --- Example Usage ---
if __name__ == '__main__':
    from src.hr_model.error_system import HRNetworkErrorSystem
    from src.hr_model.model import DEFAULT_PARAMS
    from visualization.plotting import (
        plot_hamiltonian,
        plot_hamiltonian_derivative,
        plot_lyapunov_derivative
    )

    # initial state (x, y, z, u, œÜ for each neuron)
    INITIAL_HR_STATE0 = [
        0.1, 0.2, 0.3, 0.4, 0.1,   # neuron 1
        0.2, 0.3, 0.4, 0.5, 0.2    # neuron 2
    ]

    # external currents and coupling matrix
    I_ext = [0.8, 0.8]
    xi = [[0, 1], [1, 0]]

    # Example modification of parameters
    sim_params = DEFAULT_PARAMS.copy()
    # sim_params['ge'] = 0.65

    # Create simulator instance
    simulator = HRNetworkErrorSystem(params=sim_params, dynamics='complete',
                                     hr_initial_state=INITIAL_HR_STATE0, I_ext=I_ext, hr_xi=xi)

    # integration settings
    start_time = 0
    end_time = 1000
    dt_initial = 0.01
    point_num = 10000
    transient_ratio = 0
    n_points = dfx.SaveAt(ts=jnp.linspace(start_time, end_time, point_num), dense=True)
    max_steps = int((end_time - start_time) / dt_initial) * 20

    solver = dfx.Tsit5()
    stepsize_controller = dfx.PIDController(rtol=1e-10, atol=1e-12)
    # stepsize_controller = dfx.ConstantStepSize()

    # run simulation ----------------------------------------------------
    print("Running simulation...")
    import time
    tic = time.perf_counter()

    simulator.solve(
        solver=solver,
        t0=start_time,
        t1=end_time,
        dt0=dt_initial,
        n_points=n_points,
        stepsize_controller=stepsize_controller,
        max_steps=max_steps
    )

    toc = time.perf_counter()
    print(f"Finished in {(toc - tic):.2f} s")

    # Get results dictionary
    results = simulator.get_results_dict(transient_ratio)

    # --- Calculate H, dH/dt, and dV/dt ---
    print("Calculating physical quantities...")
    H = calculate_H(results, sim_params)
    dHdt = calculate_dHdt(results, sim_params)
    dVdt = calculate_dVdt(results, sim_params)
    print("Calculations complete.")

    # --- Plotting ---
    print("Generating plots...")

    # --- Plotting ---
    print("Generating plots...")

    plot_hamiltonian(results['t'], H, save_fig=1)
    plot_hamiltonian_derivative(results['t'], dHdt, save_fig=1)
    plot_lyapunov_derivative(results['t'], dVdt,  save_fig=1)
```

================================================================================
## Folder: src/sph_pinn
------------------------------------------------------------
### Other Files:
- 3Stage_phpinn.ipynb
- phpinn.ipynb
- pinn.ipynb

### Python Source Files:

#### File: `3Stage_phpinn.py`
```python
#%%
import jax, jax.numpy as jnp
import equinox as eqx
import optax
import matplotlib.pyplot as plt
import pickle
import sys
from src.hr_model.model import DEFAULT_PARAMS

# JAX configuration to use 64-bit precision.
jax.config.update("jax_enable_x64", True)

# ==============================================================================
# 1. NEURAL NETWORK DEFINITIONS
# ==============================================================================

class FourierFeatures(eqx.Module):
    """Encodes a 1D input into a higher-dimensional space using Fourier features."""
    b_matrix: jax.Array

    def __init__(self, key, in_size=1, mapping_size=32, scale=1):
        self.b_matrix = jax.random.normal(key, (mapping_size // 2, in_size)) * scale

    def __call__(self, t):
        if t.ndim == 1:
            t = t[None, :]
        t_proj = t @ self.b_matrix.T
        return jnp.concatenate([jnp.sin(t_proj), jnp.cos(t_proj)], axis=-1).squeeze()


class StateNN(eqx.Module):
    """An MLP with Fourier Features to approximate the combined state [q(t), s(t)]."""
    layers: list

    def __init__(self, key, out_size=15, width=256, depth=3, mapping_size=32, scale=300):
        fourier_key, *layer_keys = jax.random.split(key, depth + 1)
        self.layers = [
            FourierFeatures(fourier_key, in_size=1, mapping_size=mapping_size, scale=scale),
            eqx.nn.Linear(mapping_size, width, key=layer_keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for i in range(1, depth - 1)],
            eqx.nn.Linear(width, out_size, key=layer_keys[-1])
        ]

    def __call__(self, t):
        x = self.layers[0](t)
        for layer in self.layers[1:-1]:
            x = jax.nn.tanh(layer(x))
        return self.layers[-1](x)


# --- sPHNN Component Networks (from sPHNN implementation) ---

class _FICNN(eqx.Module):
    """Internal helper class for a Fully Input Convex Neural Network."""
    w_layers: list
    u_layers: list
    final_layer: eqx.nn.Linear
    activation: callable = eqx.field(static=True)

    def __init__(self, key, in_size: int, out_size: int, width: int, depth: int):
        self.activation = jax.nn.softplus
        keys = jax.random.split(key, depth)
        self.w_layers = [eqx.nn.Linear(in_size, width, key=keys[0])]
        self.w_layers.extend([eqx.nn.Linear(in_size, width, key=key) for key in keys[1:-1]])
        self.u_layers = [eqx.nn.Linear(width, width, use_bias=False, key=key) for key in keys[1:-1]]
        self.final_layer = eqx.nn.Linear(width, out_size, use_bias=False, key=keys[-1])

    def __call__(self, s):
        z = self.activation(self.w_layers[0](s))
        for i in range(len(self.u_layers)):
            # Enforce non-negative weights for convexity
            u_layer_non_negative = eqx.tree_at(lambda l: l.weight, self.u_layers[i], jnp.abs(self.u_layers[i].weight))
            z = self.activation(u_layer_non_negative(z) + self.w_layers[i + 1](s))
        return self.final_layer(z)[0]


class HamiltonianNN(eqx.Module):
    """Learns a convex Hamiltonian function H(x) with a guaranteed minimum at x0."""
    ficnn: _FICNN
    x0: jax.Array
    epsilon: float = eqx.field(static=True)

    def __init__(self, key, in_size, width, depth, x0, epsilon):
        self.ficnn = _FICNN(key, in_size, out_size=1, width=width, depth=depth)
        self.x0 = x0
        self.epsilon = epsilon

    def __call__(self, x):
        # Implements Equation (10) from the paper
        f_x = self.ficnn(x)
        f_x0 = self.ficnn(self.x0)
        grad_f_x0 = jax.grad(self.ficnn)(self.x0)
        # Normalization term to set H(x0)=0 and grad H(x0)=0
        f_norm = f_x0 + jnp.dot(grad_f_x0, x - self.x0)
        # Regularization term to ensure a strict minimum
        f_reg = self.epsilon * jnp.sum((x - self.x0) ** 2)
        return f_x - f_norm + f_reg


class DissipationNN(eqx.Module):
    """Learns a positive semi-definite dissipation matrix R(s) = L(s)L(s)^T."""
    layers: list
    activation: callable
    state_dim: int

    def __init__(self, key, state_dim, width, depth, activation):
        self.state_dim = state_dim
        num_l_elements = state_dim * (state_dim + 1) // 2
        keys = jax.random.split(key, depth)
        self.layers = [
            eqx.nn.Linear(state_dim, width, key=keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for key in keys[1:-1]],
            eqx.nn.Linear(width, num_l_elements, key=keys[-1])
        ]
        self.activation = activation

    def __call__(self, s):
        for layer in self.layers[:-1]:
            s = self.activation(layer(s))
        l_elements = self.layers[-1](s)
        
        # Build the lower triangular matrix L
        L = jnp.zeros((self.state_dim, self.state_dim))
        tril_indices = jnp.tril_indices(self.state_dim)
        L = L.at[tril_indices].set(l_elements)
        
        # Enforce positive diagonal elements for positive definiteness
        positive_diag = jax.nn.softplus(jnp.diag(L))
        L = L.at[jnp.diag_indices(self.state_dim)].set(positive_diag)
        
        # Return R = L @ L.T
        return L @ L.T


class DynamicJ_NN(eqx.Module):
    """Learns a skew-symmetric structure matrix J(s)."""
    layers: list
    state_dim: int
    activation: callable

    def __init__(self, key, state_dim, width, depth, activation):
        self.state_dim = state_dim
        num_unique_elements = state_dim * (state_dim - 1) // 2
        keys = jax.random.split(key, depth + 1)
        self.layers = [
            eqx.nn.Linear(state_dim, width, key=keys[0]),
            *[eqx.nn.Linear(width, width, key=key) for key in keys[1:-1]],
            eqx.nn.Linear(width, num_unique_elements, key=keys[-1])
        ]
        self.activation = activation

    def __call__(self, s):
        for layer in self.layers[:-1]:
            s = self.activation(layer(s))
        upper_triangle_elements = self.layers[-1](s)
        
        # Build the matrix from its upper triangular elements
        J = jnp.zeros((self.state_dim, self.state_dim))
        triu_indices = jnp.triu_indices(self.state_dim, k=1)
        J = J.at[triu_indices].set(upper_triangle_elements)
        
        # Enforce skew-symmetry: J = J - J.T
        return J - J.T


# --- The Combined Model ---

class Combined_sPHNN_PINN(eqx.Module):
    """Main model combining a unified state predictor and sPHNN structure."""
    state_net: StateNN
    hamiltonian_net: HamiltonianNN
    dissipation_net: DissipationNN
    j_net: DynamicJ_NN

    def __init__(self, key, config):
        state_key, h_key, d_key, j_key = jax.random.split(key, 4)
        state_dim = config['state_dim']
        # The equilibrium point for the normalized error system is the origin.
        x0_norm = jnp.zeros(state_dim)

        self.state_net = StateNN(key=state_key)
        self.hamiltonian_net = HamiltonianNN(
            h_key, in_size=state_dim, width=config['h_width'], depth=config['h_depth'],
            x0=x0_norm, epsilon=config['h_epsilon']
        )
        self.dissipation_net = DissipationNN(
            d_key, state_dim=state_dim, width=config['d_width'],
            depth=config['d_depth'], activation=config['activation']
        )
        self.j_net = DynamicJ_NN(
            j_key, state_dim=state_dim, width=config['j_width'],
            depth=config['j_depth'], activation=config['activation']
        )


# ==============================================================================
# 2. DATA HANDLING
# ==============================================================================


def generate_data(file_path="error_system_data.pkl"):
    """
    Loads and prepares training data from a pre-generated pickle file containing
    multiple simulation runs.
    """
    print(f"Loading simulation data from {file_path}...")
    try:
        with open(file_path, 'rb') as f:
            # The file contains a list of result dictionaries
            all_runs_results = pickle.load(f)
    except FileNotFoundError:
        print(f"Error: Data file not found at {file_path}")
        print("Please run 'generate_data_for_PINN.py' to create the data file.")
        return None, None, None, None, None

    # Initialize lists to hold data from all runs
    all_t, all_s, all_q, all_s_dot, all_H = [], [], [], [], []

    # Process each simulation run
    for i, results in enumerate(all_runs_results):
        print(f"  ... processing run {i + 1}/{len(all_runs_results)}")

        # Extract data for the current run
        t = jnp.asarray(results['t'])
        s = jnp.vstack([
            results['e_x'], results['e_y'], results['e_z'],
            results['e_u'], results['e_phi']
        ]).T
        q = jnp.vstack([
            results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],
            results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']
        ]).T
        s_dot_true = jnp.vstack([
            results['d_e_x'], results['d_e_y'], results['d_e_z'],
            results['d_e_u'], results['d_e_phi']
        ]).T
        H_analytical = jnp.asarray(results['Hamiltonian'])

        # Append to the main lists
        all_t.append(t)
        all_s.append(s)
        all_q.append(q)
        all_s_dot.append(s_dot_true)
        all_H.append(H_analytical)

    # Concatenate all runs into single arrays
    final_t = jnp.concatenate(all_t)
    final_s = jnp.concatenate(all_s)
    final_q = jnp.concatenate(all_q)
    final_s_dot = jnp.concatenate(all_s_dot)
    final_H = jnp.concatenate(all_H)

    print("Data loading and aggregation complete.")
    return final_t, final_s, final_q, final_s_dot, final_H


def normalize(data, mean, std):
    """Normalizes data using pre-computed statistics."""
    return (data - mean) / (std + 1e-8)

def denormalize(data, mean, std):
    """Denormalizes data using pre-computed statistics."""
    return data * std + mean


# ==============================================================================
# 3. TRAINING LOGIC
# ==============================================================================

# --- Helper functions for the new physics-based loss terms ---

def _alpha(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [2*m - 1., -1., -1., 2*m - 1., -1., -1., 2*m - 1.]
    return jnp.select(conds, choices, default=-1.)

def _beta(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [
        2*m * (u1 - 1), -4*m, -2*m * (u1 - 1), 0.,
        -2*m * (u1 + 1), 4*m, 2*m * (u1 + 1),
    ]
    return jnp.select(conds, choices, default=0.)

def f_c_fn(e, q, hr_params):
    """Calculates the conservative vector field f_c(e)."""
    e_x, e_y, e_u, e_phi = e[0], e[1], e[3], e[4]
    x1, u1 = q[0], q[3]
    
    k, f, rho, d, r, s = \
        hr_params['k'], hr_params['f'], hr_params['rho'], hr_params['d'], hr_params['r'], hr_params['s']
    
    return jnp.array([
        e_y + 2*k*f*u1*x1*e_u + rho*x1*e_phi,
        -2*d*x1*e_x,
        r*s*e_x,
        e_x,
        e_x
    ])

def f_d_fn(e, q, hr_params):
    """Calculates the dissipative vector field f_d(e)."""
    e_x, e_y, e_z, e_u, e_phi = e[0], e[1], e[2], e[3], e[4]
    x1, u1, phi1, u2 = q[0], q[3], q[4], q[8]

    a, b, k, h, f, rho, g_e, r, q_param, m = \
        hr_params['a'], hr_params['b'], hr_params['k'], hr_params['h'], \
        hr_params['f'], hr_params['rho'], hr_params['ge'], hr_params['r'], \
        hr_params['q'], hr_params['m']

    N_val = -3*a*x1**2 + 2*b*x1 + k*h + k*f*u1**2 + rho*phi1 - 2*g_e
    alpha_val = _alpha(u1, u2, m)
    beta_val = _beta(u1, u2, m)

    return jnp.array([
        N_val * e_x,
        -e_y,
        -r * e_z,
        alpha_val * e_u + beta_val,
        -q_param * e_phi
    ])

@eqx.filter_jit
def loss_fn(model: Combined_sPHNN_PINN, t_batch_norm, s_true_batch_norm, q_true_batch_norm, s_dot_true_batch_norm, H_true_batch_norm,
            lambda_conservative: float, lambda_dissipative: float, lambda_physics: float, hr_params: dict,
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
            stage: int):
    """Calculates the composite loss based on the current training stage."""

    # --- Part 1: State Prediction and Common Calculations ---
    all_states_pred_norm = jax.vmap(model.state_net)(t_batch_norm)
    q_pred_batch_norm = all_states_pred_norm[:, :10]
    s_pred_batch_norm = all_states_pred_norm[:, 10:]

    s_pred = denormalize(s_pred_batch_norm, s_mean, s_std)
    q_pred = denormalize(q_pred_batch_norm, q_mean, q_std)
    s_dot_true_batch = denormalize(s_dot_true_batch_norm, s_dot_mean, s_dot_std)

    # inside loss_fn, after computing s_dot_true_batch
    dyn_scale = jnp.mean(jnp.linalg.norm(s_dot_true_batch, axis=1) ** 2) + 1e-8#######################


    # Calculate s_dot from autodiff of the state network
    get_autodiff_grad_s_slice = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[1]
    s_dot_autodiff_norm = jax.vmap(get_autodiff_grad_s_slice, in_axes=(None, 0))(model.state_net, t_batch_norm)
    s_dot_autodiff = s_dot_autodiff_norm * (s_std / (t_std + 1e-8))

    # --- Part 2: Loss Component Calculation ---

    # --- Stage 1 Loss Components ---
    all_states_true_norm = jnp.concatenate([q_true_batch_norm, s_true_batch_norm], axis=1)
    loss_data_unified = jnp.mean((all_states_pred_norm - all_states_true_norm) ** 2)
    loss_s_dot_autodiff = jnp.mean((s_dot_true_batch - s_dot_autodiff) ** 2)
    
    f_c_batch = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
    f_d_batch = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
    s_dot_diss_cons = f_c_batch + f_d_batch
    loss_s_dot_analytical = jnp.mean((s_dot_diss_cons - s_dot_autodiff) ** 2)

    # --- Stage 2 & 3 Loss Components ---
    grad_H_norm_fn = jax.vmap(jax.grad(model.hamiltonian_net))
    grad_H_norm = grad_H_norm_fn(s_pred_batch_norm)
    grad_H = grad_H_norm / (s_std + 1e-8)

    J_norm = jax.vmap(model.j_net)(s_pred_batch_norm)
    R_norm = jax.vmap(model.dissipation_net)(s_pred_batch_norm)
    s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
    s_dot_from_structure = s_dot_from_structure_norm * s_std
    # loss_phys = jnp.mean((s_dot_true_batch - s_dot_from_structure) ** 2)
    loss_phys = jnp.mean((s_dot_true_batch - s_dot_from_structure) ** 2) / dyn_scale################

    # lie_derivative = jax.vmap(jnp.dot)(f_c_batch, grad_H)
    # loss_conservative = jnp.mean(lie_derivative ** 2)

    # Conservative Loss#####################################################
    lie_derivative = jax.vmap(jnp.dot)(f_c_batch, grad_H)
    loss_conservative = jnp.mean(lie_derivative ** 2) / dyn_scale############

    # dHdt_from_autodiff = jax.vmap(jnp.dot)(grad_H, s_dot_autodiff)
    # dHdt_from_equations = jax.vmap(jnp.dot)(grad_H, f_d_batch)
    # loss_dissipative = jnp.mean((dHdt_from_autodiff - dHdt_from_equations) ** 2)

    # Dissipative Loss############################################################################
    dHdt_from_autodiff = jax.vmap(jnp.dot)(grad_H, s_dot_autodiff)
    dHdt_from_equations = jax.vmap(jnp.dot)(grad_H, f_d_batch)
    loss_dissipative = jnp.mean((dHdt_from_autodiff - dHdt_from_equations) ** 2) / dyn_scale#######
    
    # --- Hamiltonian Loss (for monitoring only) ---
    H_pred_norm = jax.vmap(model.hamiltonian_net)(s_pred_batch_norm)
    H_pred = denormalize(H_pred_norm, H_mean, H_std)
    H_true = denormalize(H_true_batch_norm, H_mean, H_std)
    correlation = jnp.corrcoef(H_true.flatten(), H_pred.flatten())[0, 1]
    sign = jnp.sign(correlation)
    H_pred_aligned = sign * H_pred - jnp.mean(sign * H_pred) + jnp.mean(H_true)
    loss_hamiltonian = jnp.mean((H_pred_aligned - H_true) ** 2)

    # --- Part 3: Select Total Loss Based on Stage ---
    #loss_stage1 = loss_data_unified + loss_s_dot_autodiff + loss_s_dot_analytical
    loss_stage1 = loss_data_unified + loss_s_dot_analytical
    loss_stage2 = ((lambda_conservative * loss_conservative) 
                   + (lambda_dissipative * loss_dissipative) 
                   + (lambda_physics * loss_phys))
    
    if stage == 1:
        total_loss = loss_stage1
    elif stage == 2:
        total_loss = loss_stage2
    else:  # Stage 3
        total_loss = loss_stage1 + loss_stage2

    loss_components = {
        "total": total_loss,
        "data_unified": loss_data_unified,
        "s_dot_autodiff": loss_s_dot_autodiff,
        "s_dot_analytical": loss_s_dot_analytical,
        "phys": loss_phys,
        "conservative": loss_conservative,
        "dissipative": loss_dissipative,
        "hamiltonian": loss_hamiltonian,
    }
    return total_loss, loss_components


@eqx.filter_jit
def train_step(model, opt_state, optimizer, trainable_filter, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
               lambda_conservative, lambda_dissipative, lambda_physics, hr_params, 
               t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std, stage):
    """Performs a single training step on the specified trainable parameters."""
    
    # Define a wrapper for the loss function to pass the trainable filter
    def filtered_loss_fn(filtered_model):
        return loss_fn(
            filtered_model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
            lambda_conservative, lambda_dissipative, lambda_physics, hr_params, 
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
            stage
        )

    # Calculate gradients only for the trainable parts of the model
    (loss_val, loss_components), grads = eqx.filter_value_and_grad(filtered_loss_fn, has_aux=True)(model)
    
    # Apply updates only to the trainable parameters
    updates, opt_state = optimizer.update(grads, opt_state, model)
    model = eqx.apply_updates(model, updates)
    
    return model, opt_state, loss_val, loss_components


@eqx.filter_jit
def evaluate_model(model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
                   lambda_conservative, lambda_dissipative, lambda_physics, hr_params, 
                   t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std):
    """Calculates the full loss (Stage 3) for the validation set for consistent evaluation."""
    loss_val, _ = loss_fn(
        model, t_batch_norm, s_batch_norm, q_batch_norm, s_dot_batch_norm, H_batch_norm,
        lambda_conservative, lambda_dissipative, lambda_physics, hr_params, 
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
        stage=3  # Always evaluate using the full loss
    )
    return loss_val

# ==============================================================================
# 4. MAIN EXECUTION LOGIC
# ==============================================================================

# --- Setup and Hyperparameters ---
key = jax.random.PRNGKey(42)
model_key, data_key = jax.random.split(key)

# Training hyperparameters
batch_size = 8000
validation_split = 0.2

# --- Three-Stage Training Setup ---
# Stage 1: Train StateNet only
epochs_stage1 = 100
lr_stage1 = 1e-3

# Stage 2: Freeze StateNet, train sPHNN components
epochs_stage2 = 200
lr_stage2 = 1e-3

# Stage 3: Fine-tune all networks
epochs_stage3 = 300
lr_stage3_initial = 1e-4
lr_stage3_end = 1e-5

# Physics loss hyperparameters with warmup
lambda_conservative_max = 1
lambda_dissipative_max = 5
lambda_physics_max = 15
# Warmup will happen during Stage 2
lambda_warmup_epochs = epochs_stage2 

# System parameters
hr_params = DEFAULT_PARAMS.copy()

# --- Generate and Prepare Data ---
import os

path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data/',
                    'error_system_data.pkl')
t, s, q, s_dot_true, H_analytical = generate_data(path)
if t is None:
    sys.exit("Exiting: Data loading failed.")

num_samples = s.shape[0]
perm = jax.random.permutation(data_key, num_samples)
t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = t[perm], s[perm], q[perm], s_dot_true[perm], H_analytical[perm]
t_shuffled = t_shuffled.reshape(-1, 1)

split_idx = int(num_samples * (1 - validation_split))
t_train, t_val = jnp.split(t_shuffled, [split_idx])
s_train, s_val = jnp.split(s_shuffled, [split_idx])
q_train, q_val = jnp.split(q_shuffled, [split_idx])
s_dot_train, s_dot_val = jnp.split(s_dot_shuffled, [split_idx])
H_train, H_val = jnp.split(H_shuffled, [split_idx])

# --- Normalize Data (using ONLY training set statistics) ---
t_mean, t_std = jnp.mean(t_train), jnp.std(t_train)
s_mean, s_std = jnp.mean(s_train, axis=0), jnp.std(s_train, axis=0)
q_mean, q_std = jnp.mean(q_train, axis=0), jnp.std(q_train, axis=0)
s_dot_mean, s_dot_std = jnp.mean(s_dot_train, axis=0), jnp.std(s_dot_train, axis=0)
H_mean, H_std = jnp.mean(H_train), jnp.std(H_train)

t_train_norm = normalize(t_train, t_mean, t_std)
s_train_norm = normalize(s_train, s_mean, s_std)
q_train_norm = normalize(q_train, q_mean, q_std)
s_dot_train_norm = normalize(s_dot_train, s_dot_mean, s_dot_std)
H_train_norm = normalize(H_train, H_mean, H_std)

t_val_norm = normalize(t_val, t_mean, t_std)
s_val_norm = normalize(s_val, s_mean, s_std)
q_val_norm = normalize(q_val, q_mean, q_std)
s_dot_val_norm = normalize(s_dot_val, s_dot_mean, s_dot_std)
H_val_norm = normalize(H_val, H_mean, H_std)

# --- Centralized Neural Network Configuration ---
s_dim = s_train.shape[1]
q_dim = q_train.shape[1]
nn_config = {
    "state_dim": s_dim, 
    "h_width": 128, "h_depth": 3, "h_epsilon": 0.525,
    "d_width": 2, "d_depth": 3, "j_width": 2, "j_depth": 3,
    "activation": jax.nn.softplus,
}

# Initialize the combined model
model = Combined_sPHNN_PINN(key=model_key, config=nn_config)

# --- Training Loop Setup ---
train_losses, val_losses = [], []
all_loss_components = []
best_model, best_val_loss = model, jnp.inf

num_batches = t_train_norm.shape[0] // batch_size
if num_batches == 0 and t_train_norm.shape[0] > 0:
    print(f"Warning: batch_size ({batch_size}) > num samples. Setting num_batches to 1.")
    num_batches = 1

total_epochs = epochs_stage1 + epochs_stage2 + epochs_stage3
global_epoch = 0


# ==============================================================================
# STAGE 1: TRAIN STATE NETWORK
# ==============================================================================
print("\n" + "="*50)
print(f"STARTING STAGE 1: Training State Network for {epochs_stage1} epochs.")
print("="*50)

# Filter to train only the state_net
trainable_filter_s1 = eqx.filter(model, eqx.is_array)
trainable_filter_s1 = eqx.tree_at(
    lambda m: (m.hamiltonian_net, m.dissipation_net, m.j_net), 
    trainable_filter_s1, 
    (
        jax.tree_util.tree_map(lambda _: False, model.hamiltonian_net),
        jax.tree_util.tree_map(lambda _: False, model.dissipation_net),
        jax.tree_util.tree_map(lambda _: False, model.j_net)
    )
)

optimizer_s1 = optax.adamw(learning_rate=lr_stage1)
opt_state = optimizer_s1.init(eqx.filter(model, eqx.is_array))

for epoch in range(epochs_stage1):
    key, shuffle_key = jax.random.split(key)
    perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])
    t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = \
        t_train_norm[perm], s_train_norm[perm], q_train_norm[perm], s_dot_train_norm[perm], H_train_norm[perm]

    epoch_losses = {k: 0.0 for k in ["total", "data_unified", "s_dot_autodiff", "s_dot_analytical", "phys", "conservative", "dissipative", "hamiltonian"]}
    
    for i in range(num_batches):
        start, end = i * batch_size, (i + 1) * batch_size
        t_b, s_b, q_b, s_dot_b, H_b = t_shuffled[start:end], s_shuffled[start:end], q_shuffled[start:end], s_dot_shuffled[start:end], H_shuffled[start:end]

        model, opt_state, train_loss_val, loss_comps = train_step(
            model, opt_state, optimizer_s1, trainable_filter_s1, t_b, s_b, q_b, s_dot_b, H_b,
            0.0, 0.0, 0.0, hr_params, # No physics lambdas in stage 1
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
            stage=1
        )
        for k in epoch_losses:
            if k in loss_comps:
                epoch_losses[k] += loss_comps[k]

    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}
    val_loss = evaluate_model(
        model, t_val_norm, s_val_norm, q_val_norm, s_dot_val_norm, H_val_norm,
        0.0, 0.0, 0.0, hr_params,
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
    )
    
    train_losses.append(avg_losses["total"])
    val_losses.append(val_loss)
    all_loss_components.append(avg_losses)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = model

    global_epoch += 1
    if (epoch + 1) % 100 == 0 or epoch == 0:
        print(f"Epoch {global_epoch}/{total_epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | "
              f"Data: {avg_losses['data_unified']:.4f} | S_dot_AD: {avg_losses['s_dot_autodiff']:.4f} | S_dot_Analytic: {avg_losses['s_dot_analytical']:.4f}")

# ==============================================================================
# STAGE 2: TRAIN sPHNN COMPONENTS
# ==============================================================================
print("\n" + "="*50)
print(f"STARTING STAGE 2: Training sPHNN Components for {epochs_stage2} epochs.")
print("="*50)

# Filter to train only the sPHNN components
trainable_filter_s2 = eqx.filter(model, eqx.is_array)
trainable_filter_s2 = eqx.tree_at(
    lambda m: m.state_net, 
    trainable_filter_s2, 
    jax.tree_util.tree_map(lambda _: False, model.state_net)
)

optimizer_s2 = optax.adamw(learning_rate=lr_stage2)
opt_state = optimizer_s2.init(eqx.filter(model, eqx.is_array))

for epoch in range(epochs_stage2):
    warmup_factor = jnp.minimum(1.0, (epoch + 1) / lambda_warmup_epochs)
    current_lambda_conservative = lambda_conservative_max * warmup_factor
    current_lambda_dissipative = lambda_dissipative_max * warmup_factor
    current_lambda_physics = lambda_physics_max * warmup_factor

    key, shuffle_key = jax.random.split(key)
    perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])
    t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = \
        t_train_norm[perm], s_train_norm[perm], q_train_norm[perm], s_dot_train_norm[perm], H_train_norm[perm]

    epoch_losses = {k: 0.0 for k in ["total", "data_unified", "s_dot_autodiff", "s_dot_analytical", "phys", "conservative", "dissipative", "hamiltonian"]}

    for i in range(num_batches):
        start, end = i * batch_size, (i + 1) * batch_size
        t_b, s_b, q_b, s_dot_b, H_b = t_shuffled[start:end], s_shuffled[start:end], q_shuffled[start:end], s_dot_shuffled[start:end], H_shuffled[start:end]

        model, opt_state, train_loss_val, loss_comps = train_step(
            model, opt_state, optimizer_s2, trainable_filter_s2, t_b, s_b, q_b, s_dot_b, H_b,
            current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
            stage=2
        )
        for k in epoch_losses:
            if k in loss_comps:
                epoch_losses[k] += loss_comps[k]
    
    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}
    val_loss = evaluate_model(
        model, t_val_norm, s_val_norm, q_val_norm, s_dot_val_norm, H_val_norm,
        current_lambda_conservative, current_lambda_dissipative, current_lambda_physics, hr_params,
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
    )
    
    train_losses.append(avg_losses["total"])
    val_losses.append(val_loss)
    all_loss_components.append(avg_losses)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = model

    global_epoch += 1
    if (epoch + 1) % 100 == 0 or epoch == 0:
        print(f"Epoch {global_epoch}/{total_epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | "
              f"Phys: {avg_losses['phys']:.4f} | Cons: {avg_losses['conservative']:.4f} | Diss: {avg_losses['dissipative']:.4f} | H_Loss: {avg_losses['hamiltonian']:.4f}")

# ==============================================================================
# STAGE 3: FINE-TUNE ALL NETWORKS
# ==============================================================================
print("\n" + "="*50)
print(f"STARTING STAGE 3: Fine-tuning All Networks for {epochs_stage3} epochs.")
print("="*50)

# Filter to train all networks
trainable_filter_s3 = eqx.filter(model, eqx.is_array) # All True

lr_schedule = optax.linear_schedule(
    init_value=lr_stage3_initial,
    end_value=lr_stage3_end,
    transition_steps=epochs_stage3 * num_batches
)
optimizer_s3 = optax.adamw(learning_rate=lr_schedule)
opt_state = optimizer_s3.init(eqx.filter(model, eqx.is_array))

for epoch in range(epochs_stage3):
    key, shuffle_key = jax.random.split(key)
    perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])
    t_shuffled, s_shuffled, q_shuffled, s_dot_shuffled, H_shuffled = \
        t_train_norm[perm], s_train_norm[perm], q_train_norm[perm], s_dot_train_norm[perm], H_train_norm[perm]

    epoch_losses = {k: 0.0 for k in ["total", "data_unified", "s_dot_autodiff", "s_dot_analytical", "phys", "conservative", "dissipative", "hamiltonian"]}
    
    for i in range(num_batches):
        start, end = i * batch_size, (i + 1) * batch_size
        t_b, s_b, q_b, s_dot_b, H_b = t_shuffled[start:end], s_shuffled[start:end], q_shuffled[start:end], s_dot_shuffled[start:end], H_shuffled[start:end]

        model, opt_state, train_loss_val, loss_comps = train_step(
            model, opt_state, optimizer_s3, trainable_filter_s3, t_b, s_b, q_b, s_dot_b, H_b,
            lambda_conservative_max, lambda_dissipative_max, lambda_physics_max, hr_params, # Full lambdas
            t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std,
            stage=3
        )
        for k in epoch_losses:
            if k in loss_comps:
                epoch_losses[k] += loss_comps[k]
    
    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}
    val_loss = evaluate_model(
        model, t_val_norm, s_val_norm, q_val_norm, s_dot_val_norm, H_val_norm,
        lambda_conservative_max, lambda_dissipative_max, lambda_physics_max, hr_params,
        t_mean, t_std, s_mean, s_std, q_mean, q_std, s_dot_mean, s_dot_std, H_mean, H_std
    )
    
    train_losses.append(avg_losses["total"])
    val_losses.append(val_loss)
    all_loss_components.append(avg_losses)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = model
    
    best_model = model # Keep the final model from fine-tuning

    global_epoch += 1
    if (epoch + 1) % 100 == 0 or epoch == 0:
        log_str = (
            f"Epoch {global_epoch}/{total_epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | "
            f"Data: {avg_losses['data_unified']:.4f} | "
            f"Phys: {avg_losses['phys']:.4f} | "
            f"Cons: {avg_losses['conservative']:.4f} | Diss: {avg_losses['dissipative']:.4f} | "
            f"H_Loss: {avg_losses['hamiltonian']:.4f}"
        )
        print(log_str)

print("\nTraining finished.")
print(f"Best validation loss achieved: {best_val_loss:.6f}")



# ==============================================================================
# 5. VISUALIZATION AND ANALYSIS
# ==============================================================================

import os
output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'temp/')
os.makedirs(output_dir, exist_ok=True)

print("\nGenerating visualization plots...")
# Use the full, unsplit, time-ordered data for coherent plots
t_test = t.reshape(-1, 1)
s_test, q_test, s_dot_test, H_analytical_vis = s, q, s_dot_true, H_analytical
t_test_norm = normalize(t_test, t_mean, t_std)

# --- Get all model predictions for the full dataset ---
all_states_pred_norm = jax.vmap(best_model.state_net)(t_test_norm)
q_pred_norm = all_states_pred_norm[:, :10]
s_pred_norm = all_states_pred_norm[:, 10:]

s_pred = denormalize(s_pred_norm, s_mean, s_std)
q_pred = denormalize(q_pred_norm, q_mean, q_std)

# --- Calculate all derivatives for comparison ---
grad_H_norm = jax.vmap(jax.grad(best_model.hamiltonian_net))(s_pred_norm)
J_norm = jax.vmap(best_model.j_net)(s_pred_norm)
R_norm = jax.vmap(best_model.dissipation_net)(s_pred_norm)
s_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
s_dot_from_structure = s_dot_from_structure_norm * s_std

f_c_batch_vis = jax.vmap(f_c_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
f_d_batch_vis = jax.vmap(f_d_fn, in_axes=(0, 0, None))(s_pred, q_pred, hr_params)
s_dot_from_equations = f_c_batch_vis + f_d_batch_vis

# Need to compute autodiff for the s-slice of the StateNN's output
get_s_slice_autodiff_grad = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[1]
s_dot_autodiff_norm = jax.vmap(get_s_slice_autodiff_grad, in_axes=(None, 0))(best_model.state_net, t_test_norm)
s_dot_autodiff = s_dot_autodiff_norm * (s_std / (t_std + 1e-8))


# --- Plot 1: Learned vs Analytical Hamiltonian ---
print("Comparing learned Hamiltonian with analytical solution...")
H_learned_norm = jax.vmap(best_model.hamiltonian_net)(s_pred_norm)
# The learned Hamiltonian might be flipped in sign, so we check correlation and align it.
correlation = jnp.corrcoef(H_analytical_vis.flatten(), H_learned_norm.flatten())[0, 1]
sign = jnp.sign(correlation)
H_learned_signed = sign * H_learned_norm
H_learned_aligned = H_learned_signed - jnp.mean(H_learned_signed) + jnp.mean(H_analytical_vis)

split_start = int(len(t_test) * 0)
split_end = int(len(t_test) * 1)

plt.figure(figsize=(12, 7))
plt.plot(t_test[split_start:split_end], H_analytical_vis[split_start:split_end], label='Analytical Hamiltonian',
         color='blue')
plt.plot(t_test[split_start:split_end], H_learned_aligned[split_start:split_end], label='Learned Hamiltonian (Aligned)',
         color='red')
plt.title("Time Evolution of Hamiltonians", fontsize=16)
plt.xlabel("Time", fontsize=14)
plt.ylabel("Hamiltonian Value", fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.savefig(os.path.join(output_dir, 'hamiltonian_comparison.png'), dpi=300)
plt.tight_layout()

# --- Plot 2: Training, Validation, and Physics Losses ---
# Extract individual loss components for plotting
phys_losses = [d['phys'] for d in all_loss_components]
conservative_losses = [d['conservative'] for d in all_loss_components]
dissipative_losses = [d['dissipative'] for d in all_loss_components]
hamiltonian_losses = [d['hamiltonian'] for d in all_loss_components]

plt.figure(figsize=(14, 8))
plt.plot(train_losses, label='Total Training Loss', color='dodgerblue', linewidth=2)
plt.plot(val_losses, label='Total Validation Loss', color='tomato', linewidth=2)
plt.plot(hamiltonian_losses, label='Hamiltonian Loss', color='gold', alpha=0.8)
plt.plot(phys_losses, label='Physics Structure Loss', color='darkorange', alpha=0.8)
plt.plot(conservative_losses, label='Conservative Loss', color='forestgreen', alpha=0.7)
plt.plot(dissipative_losses, label='Dissipative Loss', color='mediumorchid', alpha=0.7)

# Add vertical lines to mark stage transitions
stage1_end = epochs_stage1
stage2_end = epochs_stage1 + epochs_stage2
plt.axvline(x=stage1_end, color='grey', linewidth=2, label='End Stage 1')
plt.axvline(x=stage2_end, color='black', linewidth=2, label='End Stage 2')

plt.yscale('log')
plt.title('Training, Validation, and Physics Losses Over Epochs', fontsize=16)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss (Log Scale)', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'training_losses.png'), dpi=300)
plt.tight_layout()

# --- Plot 3: Derivative Comparison (Physics Fidelity) ---
fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 12), sharex=True)
state_labels_s_dot = [r'$\dot{e}_x$', r'$\dot{e}_y$', r'$\dot{e}_z$', r'$\dot{e}_u$', r'$\dot{e}_\phi$']
fig.suptitle("Derivative Fidelity Comparison", fontsize=18, y=0.99)

for i in range(s_test.shape[1]):
    axes[i].plot(t_test[split_start:split_end], s_dot_test[split_start:split_end, i], label='True Derivative',
                 color='green', linewidth=3, alpha=0.8)
    axes[i].plot(t_test[split_start:split_end], s_dot_from_structure[split_start:split_end, i], label='sPHNN Structure',
                 color='red')
    axes[i].plot(t_test[split_start:split_end], s_dot_from_equations[split_start:split_end, i],
                 label='Analytical Eq. (f_c+f_d)', color='purple')
    axes[i].plot(t_test[split_start:split_end], s_dot_autodiff[split_start:split_end, i], label='Autodiff',
                 color='orange')

    axes[i].set_ylabel(state_labels_s_dot[i], fontsize=14)
    axes[i].grid(True)
    axes[i].legend(loc='upper right')

axes[-1].set_xlabel("Time", fontsize=14)
fig.savefig(os.path.join(output_dir, 'derivative_fidelity.png'), dpi=300)
plt.tight_layout(rect=[0, 0, 1, 0.97])

# --- Plot 4: Error System State Trajectories (s) ---
fig, axes = plt.subplots(s_test.shape[1], 1, figsize=(12, 10), sharex=True)
state_labels_error = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
fig.suptitle("Error System State 's' Prediction: True vs. Predicted", fontsize=18, y=0.99)
for i in range(s_test.shape[1]):
    axes[i].plot(t_test[split_start:split_end], s_test[split_start:split_end, i], 'b', label='True State', alpha=0.9)
    axes[i].plot(t_test[split_start:split_end], s_pred[split_start:split_end, i], 'r', label='Predicted State')
    axes[i].set_ylabel(state_labels_error[i], fontsize=14)
    axes[i].grid(True)
    axes[i].legend(loc='upper right')
axes[-1].set_xlabel("Time", fontsize=14)
fig.savefig(os.path.join(output_dir, 'error_state_s_prediction.png'), dpi=300)
plt.tight_layout(rect=[0, 0, 1, 0.97])

# --- Plot 5: HR System State Trajectories (q) ---
fig, axes = plt.subplots(q_test.shape[1], 1, figsize=(12, 18), sharex=True)
state_labels_q = [
    r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\phi_1$',
    r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$', r'$\phi_2$'
]
fig.suptitle("HR System State 'q' Prediction: True vs. Predicted", fontsize=18, y=0.99)
for i in range(q_test.shape[1]):
    axes[i].plot(t_test[split_start:split_end], q_test[split_start:split_end, i], 'b', label='True State', alpha=0.9)
    axes[i].plot(t_test[split_start:split_end], q_pred[split_start:split_end, i], 'r', label='Predicted State')
    axes[i].set_ylabel(state_labels_q[i], fontsize=14)
    axes[i].grid(True)
    axes[i].legend(loc='upper right')
axes[-1].set_xlabel("Time", fontsize=14)
fig.savefig(os.path.join(output_dir, 'hr_state_q_prediction.png'), dpi=300)
plt.tight_layout(rect=[0, 0, 1, 0.97])

plt.close('all')

print("All visualization plots have been generated and saved.")


```

#### File: `load_model_and_run.py`
```python
"""
load_model_and_run.py
---------------------
This script loads a pre-trained sPHNN-PINN Equinox model and the original
simulation data to perform inference and generate a full set of visualization plots.

It faithfully reproduces the analysis and plotting section from the main
`pH_PINN.py` script, allowing for consistent evaluation of the trained model's
performance on a specific simulation run.

To Run:
1. Make sure you have a saved model at 'results/PINN Model/sphnn_pinn.eqx'.
2. Make sure the data file exists at 'results/PINN Data/error_system_data.pkl'.
3. From the project's root directory, execute:
   python -m src.sph_pinn.load_model_and_run
"""

import os
import sys
import pickle
import matplotlib.pyplot as plt

import jax
import jax.numpy as jnp
import equinox as eqx

# Ensure the project root is in the Python path to allow for module imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.hr_model.model import DEFAULT_PARAMS
# Import the necessary components from the training script
from src.sph_pinn.pH_PINN import (
    Combined_sPHNN_PINN,
    generate_data,
    normalize,
    denormalize,
    f_c_fn,
    f_d_fn
)

# Enable 64-bit precision for JAX computations.
jax.config.update("jax_enable_x64", True)


def main():
    """
    Main function to load the model, prepare data, run predictions, and
    generate visualization plots.
    """
    # ==============================================================================
    # 1. SETUP PATHS AND CONFIGURATION
    # ==============================================================================
    print("Setting up paths and loading configuration...")

    # --- Define base directory for the project root ---
    project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..')

    # --- Paths for model, data, and plot outputs ---
    model_path = os.path.join(project_root, 'results', 'PINN Model', 'sphnn_pinn.eqx')
    data_path = os.path.join(project_root, 'results', 'PINN Data', 'error_system_data.pkl')
    plot_output_dir = os.path.join(project_root, 'results', 'temp')
    os.makedirs(plot_output_dir, exist_ok=True)

    # --- Recreate the exact nn_config used for training to build the model skeleton ---
    # This must match the configuration in pH_PINN.py
    nn_config = {
        "state_out_size": 15, "state_width": 256, "state_depth": 3,
        "state_mapping_size": 32, "state_scale": 300, "state_dim": 5,
        "h_width": 256, "h_depth": 3, "h_epsilon": 0.525,
        "d_width": 32, "d_depth": 3, "j_width": 128, "j_depth": 3,
        "activation": jax.nn.softplus,
    }
    hr_params = DEFAULT_PARAMS.copy()

    # --- PRNG Key for model initialization ---
    key = jax.random.PRNGKey(42)

    # ==============================================================================
    # 2. LOAD MODEL AND DATA
    # ==============================================================================
    print(f"Loading pre-trained model from: {model_path}")

    # --- Create a "skeleton" model with the correct architecture ---
    skeleton_model = Combined_sPHNN_PINN(key=key, config=nn_config)

    # --- Load the saved weights into the skeleton ---
    try:
        loaded_model = eqx.tree_deserialise_leaves(model_path, skeleton_model)
        print("Model loaded successfully.")
    except FileNotFoundError:
        print(f"ERROR: Model file not found at '{model_path}'.")
        print("Please ensure the model has been trained and saved correctly.")
        return

    # --- Load the full dataset ---
    t, e, x, e_dot_true, H_analytical = generate_data(data_path)
    if t is None:
        sys.exit("Exiting: Data loading failed.")

    # ==============================================================================
    # 3. PREPARE DATA FOR PREDICTION
    # ==============================================================================
    print("Preparing data for inference...")

    # --- Recreate the train/validation split to get the *exact* normalization stats ---
    # This ensures that denormalization is consistent with the training phase.
    validation_split = 0.2
    num_samples = e.shape[0]
    perm = jax.random.permutation(key, num_samples)
    split_idx = int(num_samples * (1 - validation_split))

    t_shuffled = t[perm].reshape(-1, 1)
    e_shuffled = e[perm]

    t_train, _ = jnp.split(t_shuffled, [split_idx])
    e_train, _ = jnp.split(e_shuffled, [split_idx])

    # --- Calculate normalization stats from the training portion ONLY ---
    t_mean, t_std = jnp.mean(t_train), jnp.std(t_train)
    e_mean, e_std = jnp.mean(e_train, axis=0), jnp.std(e_train, axis=0)
    # Note: x_mean/std are also needed for denormalizing the predicted x states.
    x_shuffled = x[perm]
    x_train, _ = jnp.split(x_shuffled, [split_idx])
    x_mean, x_std = jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)

    # ==============================================================================
    # 4. RUN PREDICTION AND ANALYSIS
    # ==============================================================================
    # --- Select a single simulation run from the original data for visualization ---
    run_to_visualize_idx = 0
    print(f"\nRunning predictions and analysis for simulation run #{run_to_visualize_idx + 1}...")

    with open(data_path, 'rb') as f:
        all_runs = pickle.load(f)
    vis_data = all_runs[run_to_visualize_idx]

    # --- Prepare test arrays for the selected run ---
    t_test = jnp.asarray(vis_data['t']).reshape(-1, 1)
    e_test = jnp.vstack([vis_data['e_x'], vis_data['e_y'], vis_data['e_z'], vis_data['e_u'], vis_data['e_phi']]).T
    x_test = jnp.vstack([vis_data['x1'], vis_data['y1'], vis_data['z1'], vis_data['u1'], vis_data['phi1'],
                         vis_data['x2'], vis_data['y2'], vis_data['z2'], vis_data['u2'], vis_data['phi2']]).T
    e_dot_test = jnp.vstack(
        [vis_data['d_e_x'], vis_data['d_e_y'], vis_data['d_e_z'], vis_data['d_e_u'], vis_data['d_e_phi']]).T
    H_analytical_vis = jnp.asarray(vis_data['Hamiltonian'])

    # --- Normalize the time data for model input ---
    t_test_norm = normalize(t_test, t_mean, t_std)

    # --- Perform Predictions ---
    all_states_pred_norm = jax.vmap(loaded_model.state_net)(t_test_norm)
    x_pred_norm = all_states_pred_norm[:, :10]
    e_pred_norm = all_states_pred_norm[:, 10:]

    # --- Denormalize predictions to get physical values ---
    e_pred = denormalize(e_pred_norm, e_mean, e_std)
    x_pred = denormalize(x_pred_norm, x_mean, x_std)

    # --- Calculate physics-derived quantities from the model's output ---
    grad_H_norm = jax.vmap(jax.grad(loaded_model.hamiltonian_net))(e_pred_norm)
    grad_H_raw = grad_H_norm / (e_std + 1e-8)

    J_phys = jax.vmap(loaded_model.j_net)(e_pred)
    R_phys = jax.vmap(loaded_model.dissipation_net)(e_pred)
    e_dot_from_structure = jax.vmap(lambda j, r, g: (j - r) @ g)(J_phys, R_phys, grad_H_raw)

    f_c_batch_vis = jax.vmap(f_c_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    f_d_batch_vis = jax.vmap(f_d_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    e_dot_from_equations = f_c_batch_vis + f_d_batch_vis

    get_e_slice_autodiff_grad = lambda net, t: jax.jvp(lambda tau: net(tau)[10:], (t,), (jnp.ones_like(t),))[1]
    e_dot_autodiff_norm = jax.vmap(get_e_slice_autodiff_grad, in_axes=(None, 0))(loaded_model.state_net, t_test_norm)
    e_dot_autodiff = e_dot_autodiff_norm * (e_std / (t_std + 1e-8))

    # ==============================================================================
    # 5. VISUALIZATION (replicated from pH_PINN.py)
    # ==============================================================================
    print("Generating visualization plots...")

    # --- Plot 1: Learned vs Analytical Hamiltonian ---
    H_learned_norm = jax.vmap(loaded_model.hamiltonian_net)(e_pred_norm)
    H_learned_flipped = (-1) * H_learned_norm
    H_learned_aligned = H_learned_flipped - jnp.mean(H_learned_flipped) + jnp.mean(H_analytical_vis)

    plt.figure(figsize=(12, 7))
    plt.plot(t_test[:10000], H_analytical_vis[:10000], label='Analytical Hamiltonian', color='blue')
    plt.plot(t_test[:10000], H_learned_aligned[:10000], label='Learned Hamiltonian (Aligned)', color='red',
             linestyle='--')
    plt.title("Time Evolution of Hamiltonians", fontsize=16)
    plt.xlabel("Time", fontsize=14)
    plt.ylabel("Hamiltonian Value", fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.savefig(os.path.join(plot_output_dir, 'inference_hamiltonian_comparison.png'), dpi=300)
    plt.tight_layout()

    # --- Plot 2: Derivative Comparison (Physics Fidelity) ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 12), sharex=True)
    state_labels_e_dot = [r'$\dot{e}_x$', r'$\dot{e}_y$', r'$\dot{e}_z$', r'$\dot{e}_u$', r'$\dot{e}_\phi$']
    fig.suptitle("Derivative Fidelity Comparison (Inference)", fontsize=18, y=0.99)

    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[:10000], e_dot_test[:10000, i], label='True Derivative', color='green', linewidth=3,
                     alpha=0.8)
        axes[i].plot(t_test[:10000], e_dot_from_structure[:10000, i], label='sPHNN Structure', color='red',
                     linestyle='--')
        axes[i].plot(t_test[:10000], e_dot_from_equations[:10000, i], label='Analytical Eq. (f_c+f_d)', color='purple',
                     linestyle=':')
        axes[i].plot(t_test[:10000], e_dot_autodiff[:10000, i], label='Autodiff', color='orange', linestyle='-.')
        axes[i].set_ylabel(state_labels_e_dot[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')

    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(plot_output_dir, 'inference_derivative_fidelity.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- Plot 3: Error System State e(t) ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 10), sharex=True)
    state_labels_e = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    fig.suptitle("Error System State 'e' Prediction (Inference)", fontsize=18, y=0.99)
    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[:10000], e_test[:10000, i], 'b', label='True State', alpha=0.9)
        axes[i].plot(t_test[:10000], e_pred[:10000, i], 'r', label='Predicted State', linestyle='--')
        axes[i].set_ylabel(state_labels_e[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(plot_output_dir, 'inference_error_state_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- Plot 4: HR System State x(t) ---
    fig, axes = plt.subplots(x_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_x = [r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\phi_1$',
                      r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$', r'$\phi_2$']
    fig.suptitle("HR System State 'x' Prediction (Inference)", fontsize=18, y=0.99)
    for i in range(x_test.shape[1]):
        axes[i].plot(t_test[:10000], x_test[:10000, i], 'b', label='True State', alpha=0.9)
        axes[i].plot(t_test[:10000], x_pred[:10000, i], 'r', label='Predicted State', linestyle='--')
        axes[i].set_ylabel(state_labels_x[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(plot_output_dir, 'inference_hr_state_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    plt.close('all')
    print(f"\n‚úÖ All inference plots have been saved to '{plot_output_dir}'")


if __name__ == "__main__":
    main()

```

#### File: `optimize_hyperparams.py`
```python
"""
optimize_hyperparams.py
-----------------------
This script uses the Optuna framework to perform an automated hyperparameter
search for the Combined_PH_PINN model defined in pH_PINN.py.
[cite_start][cite: 418]
It defines an "objective" function that Optuna repeatedly calls with different
hyperparameter combinations.
[cite_start][cite: 419] Each call (a "trial") trains the model for a
fixed number of epochs and returns the best validation loss.
[cite_start][cite: 419] Optuna uses
these results to intelligently search for the optimal set of hyperparameters.
[cite_start][cite: 420]
Results of the study are saved to a SQLite database file (optimize_hyperparams.db)
in the 'results/PINN Data/' directory, allowing the optimization to be paused
and resumed.
[cite_start][cite: 421]
To run this script:
1. Make sure you have Optuna and its storage dependencies installed:
   pip install optuna "optuna[storages]"
   [cite_start][cite: 422]
2. Place this file in the `src/sph_pinn/` directory.
[cite_start][cite: 422]
3. Run from the root directory of your project:
   python -m src.sph_pinn.optimize_hyperparams --objective val_loss
   OR
   python -m src.sph_pinn.optimize_hyperparams --objective h_loss
   [cite_start][cite: 423]

To view the results dashboard after running (run from project root):
   optuna-dashboard "sqlite:///results/PINN Data/optimize_hyperparams.db"
"""
import os
import sys
import jax
import jax.numpy as jnp
import equinox as eqx
import optax
import optuna
import argparse

# Ensure the script can find other modules in the project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# --- Import necessary components from your existing files ---
from src.hr_model.model import DEFAULT_PARAMS
from src.sph_pinn.pH_PINN import (
    Combined_PH_PINN,
    generate_data,
    normalize,
    train_step,
    loss_fn as evaluate_model # Use loss_fn for evaluation as it returns components
)

# JAX configuration
jax.config.update("jax_enable_x64", True)


def objective(trial, epochs_per_trial, static_data, objective_metric):
    """
    The main objective function that Optuna will minimize.
    Args:
        trial (optuna.Trial): An Optuna trial object used to suggest hyperparameters.
        epochs_per_trial (int): The number of epochs to train for during each trial.
        static_data (dict): A dictionary containing all pre-processed data and stats.
        objective_metric (str): The metric to optimize ('val_loss' or 'h_loss').
    Returns:
        float: The best value of the chosen objective metric achieved during the trial.
    """
    # --- 1. Suggest Hyperparameters from the Search Space ---
    # Retrieve the fixed master key from static_data
    master_key = static_data['master_key']

    # Use the fixed key for model initialization and create a subkey for epoch shuffling
    model_key, epoch_key = jax.random.split(master_key)


    # StateNN Fourier Features
    mapping_size = trial.suggest_categorical("mapping_size", [32, 64, 128, 256, 512])
    scale = trial.suggest_float("scale", 10, 1000, log=True)

    # Network Architectures (width and depth for each component)
    state_width = trial.suggest_categorical("state_width", [128, 256, 512, 1024])
    state_depth = trial.suggest_int("state_depth", 2, 6)
    h_width = trial.suggest_categorical("h_width", [32, 64, 128, 256, 512])
    h_depth = trial.suggest_int("h_depth", 1, 4)
    d_width = trial.suggest_categorical("d_width", [2, 4, 8, 16, 32, 64])
    d_depth = trial.suggest_int("d_depth", 1, 4)
    j_width = trial.suggest_categorical("j_width", [2, 4, 8, 16, 32, 64])
    j_depth = trial.suggest_int("j_depth", 1, 3)
    epsilon = trial.suggest_float("epsilon", 0.01, 5)

    # Optimizer
    initial_learning_rate = trial.suggest_float("initial_learning_rate", 1e-5, 1e-2, log=True)
    decay_steps = trial.suggest_int("decay_steps", 500, 3000)

    # Training and Loss
    batch_size = trial.suggest_categorical("batch_size", [2000, 4000, 8000, 16000, 32000, 64000])
    lambda_conservative_max = trial.suggest_float("lambda_conservative_max", 0.1, 30)
    lambda_dissipative_max = trial.suggest_float("lambda_dissipative_max", 0.1, 30)
    lambda_physics_max = trial.suggest_float("lambda_physics_max", 0.1, 30)
    lambda_j_structure_max = trial.suggest_float("lambda_j_structure_max", 0.1, 30)
    lambda_r_structure_max = trial.suggest_float("lambda_r_structure_max", 0.1, 30)
    lambda_phys_res_max = trial.suggest_float("lambda_phys_res_max", 0.1, 30)
    lambda_warmup_epochs = trial.suggest_int("lambda_warmup_epochs", 500, 3000)

    # --- 2. Build Model and Optimizer with Suggested Values ---
    nn_config = {
        "state_net": {
            "out_size": static_data['x_dim'] + static_data['e_dim'],
            "hidden_sizes": [state_width] * state_depth,
            "fourier_features": {"in_size": 1, "mapping_size": mapping_size, "scale": scale}
        },
        "hamiltonian_net": {
            "hidden_sizes": [h_width] * h_depth, "epsilon": epsilon
        },
        "dissipation_net": {
            "hidden_sizes": [d_width] * d_depth
        },
        "j_net": {
            "hidden_sizes": [j_width] * j_depth
        },
        "activation": jax.nn.softplus,
    }

    model = Combined_PH_PINN(key=model_key, config=nn_config, state_dim=static_data['e_dim'])

    lr_schedule = optax.linear_schedule(
        init_value=initial_learning_rate, end_value=1e-5, transition_steps=decay_steps
    )
    optimizer = optax.adamw(learning_rate=lr_schedule)
    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))

    # --- 3. Run the Training Loop ---
    best_val_loss = jnp.inf
    best_h_loss = jnp.inf
    num_batches = static_data['t_train_norm'].shape[0] // batch_size
    if num_batches == 0: num_batches = 1

    for epoch in range(epochs_per_trial):
        warmup = jnp.minimum(1.0, (epoch + 1) / lambda_warmup_epochs)
        lambdas = {
            "lambda_conservative": lambda_conservative_max * warmup,
            "lambda_dissipative": lambda_dissipative_max * warmup,
            "lambda_physics": lambda_physics_max * warmup,
            "lambda_j_structure": lambda_j_structure_max * warmup,
            "lambda_r_structure": lambda_r_structure_max * warmup,
            "lambda_phys_res": lambda_phys_res_max * warmup,
        }

        # Split the key for shuffling, ensuring a different shuffle per epoch
        # but the sequence is the same for every trial
        epoch_key, shuffle_key = jax.random.split(epoch_key)
        perm = jax.random.permutation(shuffle_key, static_data['t_train_norm'].shape[0])
        t_s, e_s, x_s, edot_s, xdot_s, H_s = (
            static_data[k][perm] for k in ['t_train_norm', 'e_train_norm', 'x_train_norm',
                                           'e_dot_train_norm', 'x_dot_train_norm', 'H_train_norm']
        )

        for i in range(num_batches):
            start, end = i * batch_size, (i + 1) * batch_size
            t_b, e_b, x_b, edot_b, xdot_b, H_b = (
                arr[start:end] for arr in [t_s, e_s, x_s, edot_s, xdot_s, H_s]
            )

            model, opt_state, _, _ = train_step(
                model, opt_state, optimizer, t_b, e_b, x_b, edot_b, xdot_b, H_b,
                **lambdas, **static_data['static_params']
            )

        # --- Evaluate and update best loss ---
        # NOTE: Assumes evaluate_model can return components, like train_step
        val_loss, val_components = evaluate_model(
            model, static_data['t_val_norm'], static_data['e_val_norm'],
            static_data['x_val_norm'], static_data['e_dot_val_norm'],
            static_data['x_dot_val_norm'], static_data['H_val_norm'],
            **lambdas, **static_data['static_params']
        )
        val_h_loss = val_components['hamiltonian']

        if val_loss < best_val_loss:
            best_val_loss = val_loss
        if val_h_loss < best_h_loss:
            best_h_loss = val_h_loss

    # --- 4. Return the Final Metric to Optuna ---
    if objective_metric == 'h_loss':
        return float(best_h_loss)
    return float(best_val_loss)


def main():
    """Main execution block to set up and run the Optuna study."""
    # --- 0. Argument Parsing ---
    parser = argparse.ArgumentParser(description="Run Optuna hyperparameter optimization for the pH-PINN model.")
    parser.add_argument(
        '--objective',
        type=str,
        default='h_loss',
        choices=['val_loss', 'h_loss'],
        help="The objective metric to minimize ('val_loss' or 'h_loss')."
    )
    args = parser.parse_args()
    print(f"Starting optimization with objective: {args.objective}")

    # --- 1. Load and Prepare Data (Done Once) ---
    print("Loading and preparing data for optimization...")
    data_path = os.path.join(os.path.dirname(__file__), '..', '..', 'results', 'PINN Data', 'error_system_data.pkl')
    t, e, x, e_dot, x_dot, H, _ = generate_data(data_path)
    if t is None:
        sys.exit("Exiting: Data loading failed.")

    # Define a single master seed for all operations
    master_seed = 42
    master_key = jax.random.PRNGKey(master_seed)

    validation_split = 0.2
    num_samples = e.shape[0]

    # Use the master key for the initial train/val split
    split_key, _ = jax.random.split(master_key)
    perm = jax.random.permutation(split_key, num_samples)

    data_arrays = [t[perm].reshape(-1, 1), e[perm], x[perm], e_dot[perm], x_dot[perm], H[perm]]

    split_idx = int(num_samples * (1 - validation_split))
    train_data, val_data = zip(*(jnp.split(arr, [split_idx]) for arr in data_arrays))

    t_train, e_train, x_train, e_dot_train, x_dot_train, H_train = train_data
    t_val, e_val, x_val, e_dot_val, x_dot_val, H_val = val_data

    # --- Calculate and package normalization statistics ---
    stats = { #
        't': (jnp.mean(t_train), jnp.std(t_train)),
        'e': (jnp.mean(e_train, axis=0), jnp.std(e_train, axis=0)),
        'x': (jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)),
        'e_dot': (jnp.mean(e_dot_train, axis=0), jnp.std(e_dot_train, axis=0)),
        'x_dot': (jnp.mean(x_dot_train, axis=0), jnp.std(x_dot_train, axis=0)),
        'H': (jnp.mean(H_train), jnp.std(H_train)),
    }

    # --- Package all data and parameters for the objective function ---
    static_data = {
        'master_key': master_key, # Pass the master key to the objective
        'e_dim': e_train.shape[1], 'x_dim': x_train.shape[1],
        't_train_norm': normalize(t_train, *stats['t']),
        'e_train_norm': normalize(e_train, *stats['e']),
        'x_train_norm': normalize(x_train, *stats['x']),
        'e_dot_train_norm': normalize(e_dot_train, *stats['e_dot']),
        'x_dot_train_norm': normalize(x_dot_train, *stats['x_dot']),
        'H_train_norm': normalize(H_train, *stats['H']),
        't_val_norm': normalize(t_val, *stats['t']),
        'e_val_norm': normalize(e_val, *stats['e']),
        'x_val_norm': normalize(x_val, *stats['x']),
        'e_dot_val_norm': normalize(e_dot_val, *stats['e_dot']),
        'x_dot_val_norm': normalize(x_dot_val, *stats['x_dot']),
        'H_val_norm': normalize(H_val, *stats['H']),
        'static_params': {
            'hr_params': {**DEFAULT_PARAMS, 'ge': 0.62},
            'I_ext': jnp.array([0.8, 0.8]),
            'xi': jnp.array([[0, 1], [1, 0]]),
            't_mean': stats['t'][0], 't_std': stats['t'][1],
            'e_mean': stats['e'][0], 'e_std': stats['e'][1],
            'x_mean': stats['x'][0], 'x_std': stats['x'][1],
            'e_dot_mean': stats['e_dot'][0], 'e_dot_std': stats['e_dot'][1],
            'x_dot_mean': stats['x_dot'][0], 'x_dot_std': stats['x_dot'][1],
            'H_mean': stats['H'][0], 'H_std': stats['H'][1],
        }
    }

    # --- 2. Create and Run the Optuna Study ---
    print("\nStarting Optuna hyperparameter search...")
    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data')
    os.makedirs(results_dir, exist_ok=True)
    db_path = os.path.join(results_dir, "optimize_hyperparams.db")
    storage_name = f"sqlite:///{db_path}"
    # Use different study names to avoid conflicts
    study_name = f"sphnn_pinn_optimization_{args.objective}"

    study = optuna.create_study(
        study_name=study_name, storage=storage_name,
        direction="minimize", load_if_exists=True
    )

    objective_with_args = lambda trial: objective(trial, epochs_per_trial=100, static_data=static_data, objective_metric=args.objective)
    study.optimize(objective_with_args, n_trials=1)

    # --- 3. Print and Save the Results ---
    print("\nOptimization finished.")
    print(f"Study results are saved in: {storage_name}")
    print(f"To view dashboard, run: optuna-dashboard {storage_name}")

    best_trial = study.best_trial
    print("\n" + "="*40)
    print("         Best Trial Found")
    print("="*40)
    print(f"  Value (Best {args.objective}): {best_trial.value:.6f}")
    print("  Best Hyperparameters: ")
    for key, value in best_trial.params.items():
        print(f"    '{key}': {value},")
    print("="*40)


if __name__ == "__main__":
    main()
```

#### File: `pH_PINN.py`
```python
"""
This script implements a Physics-Informed Port-Hamiltonian Neural Network.
The model learns the underlying physical structure of an error-feedback system
by training a composite model that simultaneously predicts the system's state
and enforces the port-Hamiltonian structure as a physics constraint.

The model is composed of two main parts:
1.  State Prediction Network: An MLP with Fourier features (StateNN)
    learns the combined state trajectories x(t) and e(t).
2.  Port-Hamiltonian Networks: Three networks that define the error system's
    dynamics based on the predicted state e_pred:
      e_dot = (J(e,x1,u1,u2,phi1) - R(e,x1,u1,u2,phi1)) * grad_e(H(e,x1,u1,u2,phi1))
    - HamiltonianNN (H): A convex network learning the system's energy; takes e AND (x1,u1,u2,phi1)
      but autodiff is done only w.r.t. e (the four extra inputs are constants in grad).
    - DynamicJ_NN (J): An MLP learning the conservative dynamics; depends on e and (x1,u1,u2,phi1).
    - DissipationNN (R): An MLP learning the dissipative dynamics; depends on e and (x1,u1,u2,phi1).

The training loss is a combination of:
- Data Fidelity Loss: MSE between predicted states (e_pred, x_pred) and true data.
- Physics Residual Loss: Enforces that the time derivatives of the full state
  (x, e) from the StateNN (via autodiff) match the analytical vector fields.
- Conservative Loss: Enforces that the learned Hamiltonian is invariant
  under the conservative flow (Lie derivative is zero).
- Dissipative Loss: Enforces that the time derivative of the Hamiltonian
  is correctly described by the dissipative flow.
- Physics Structure Loss: Enforces that the output of the learned PHPINN
  structure matches the analytical vector field for the error dynamics.
"""

import jax, jax.numpy as jnp
import numpy as np
import equinox as eqx
import optax
import matplotlib.pyplot as plt
import sys
from src.hr_model.model import DEFAULT_PARAMS
import os
from pathlib import Path
import pickle
import optuna
import copy


# JAX configuration to use 64-bit precision.
jax.config.update("jax_enable_x64", True)


# ==============================================================================
# 1. NEURAL NETWORK DEFINITIONS
# ==============================================================================

class FourierFeatures(eqx.Module):
    """Encodes a 1D input into a higher-dimensional space using Fourier features."""
    b_matrix: jax.Array
    output_size: int = eqx.field(static=True)

    def __init__(self, key, config: dict):
        in_size = config['in_size']
        mapping_size = config['mapping_size']
        scale = config['scale']
        n_pairs = mapping_size // 2
        self.b_matrix = jax.random.normal(key, (n_pairs, in_size)) * scale
        self.output_size = n_pairs * 2

    def __call__(self, t):
        if t.ndim == 1:
            t = t[None, :]
        t_proj = t @ self.b_matrix.T
        return jnp.concatenate([jnp.sin(t_proj), jnp.cos(t_proj)], axis=-1).squeeze()


class StateNN(eqx.Module):
    """An MLP with Fourier Features to approximate the combined state [x(t), e(t)]."""
    layers: list

    def __init__(self, key, config: dict):
        fourier_config = config['fourier_features']
        out_size = config['out_size']
        hidden_sizes = config['hidden_sizes']

        # keys: 1 for Fourier + (#hidden + 1) for linears
        keys = jax.random.split(key, len(hidden_sizes) + 2)
        fourier_key, layer_keys = keys[0], keys[1:]

        fourier_layer = FourierFeatures(fourier_key, config=fourier_config)

        layers = [fourier_layer]
        in_dim = fourier_layer.output_size

        # hidden layers
        for i, h in enumerate(hidden_sizes):
            layers.append(eqx.nn.Linear(in_dim, h, key=layer_keys[i]))
            in_dim = h

        # output head
        layers.append(eqx.nn.Linear(in_dim, out_size, key=layer_keys[-1]))
        self.layers = layers

    def __call__(self, t):
        x_out = self.layers[0](t)
        for layer in self.layers[1:-1]:
            x_out = jax.nn.tanh(layer(x_out))
        return self.layers[-1](x_out)


# --- PHPINN Component Networks (from PHPINN implementation) ---

class _FICNN(eqx.Module):
    """Fully Input Convex Neural Network with variable hidden sizes."""
    w_layers: list    # list of Linear(in_size, h_i)
    u_layers: list    # list of Linear(h_{i-1}, h_i) with nonnegative weights
    final_layer: eqx.nn.Linear
    activation: callable = eqx.field(static=True)

    def __init__(self, key, in_size: int, out_size: int, hidden_sizes: list):
        assert len(hidden_sizes) >= 1, "FICNN needs at least one hidden layer."
        self.activation = jax.nn.softplus

        L = len(hidden_sizes)
        # keys: L for W, (L-1) for U, 1 for final
        keys = jax.random.split(key, 2 * L)
        w_keys = keys[:L]
        u_keys = keys[L:2 * L - 1]
        final_key = keys[-1]

        # Input skip connections into each hidden layer
        self.w_layers = [eqx.nn.Linear(in_size, hidden_sizes[i], key=w_keys[i]) for i in range(L)]

        # Layer-to-layer connections (constrained to be nonnegative)
        self.u_layers = []
        for i in range(1, L):
            self.u_layers.append(eqx.nn.Linear(hidden_sizes[i - 1], hidden_sizes[i], use_bias=False, key=u_keys[i - 1]))

        # Final convex head (no bias)
        self.final_layer = eqx.nn.Linear(hidden_sizes[-1], out_size, use_bias=False, key=final_key)

    def __call__(self, e_and_ctx):
        z = self.activation(self.w_layers[0](e_and_ctx))
        for i, u in enumerate(self.u_layers, start=1):
            u_nonneg = eqx.tree_at(lambda l: l.weight, u, jnp.abs(u.weight))
            z = self.activation(u_nonneg(z) + self.w_layers[i](e_and_ctx))
        return self.final_layer(z)[0]



class HamiltonianNN(eqx.Module):
    """
    Learns a convex Hamiltonian function H([e, ctx]) with a guaranteed minimum at e0 (ctx acts as a parameter).
    ctx = [x1, u1, u2, phi1] (all normalized).
    """
    ficnn: _FICNN
    x0: jax.Array
    epsilon: float = eqx.field(static=True)
    input_dim: int = eqx.field(static=True)  # e_dim + 4

    def __init__(self, key, in_size_with_ctx, e_dim, hidden_sizes, x0, epsilon):
        self.ficnn = _FICNN(key, in_size_with_ctx, out_size=1, hidden_sizes=hidden_sizes)
        self.x0 = x0  # shape (e_dim,)
        self.epsilon = epsilon
        self.input_dim = in_size_with_ctx

    def __call__(self, e_in, ctx_in):
        # Concatenate e and context for the FICNN input
        x_in = jnp.concatenate([e_in, ctx_in])  # shape (e_dim+4,)
        x0_ext = jnp.concatenate([self.x0, ctx_in])  # anchor uses the same ctx so H(e=0,ctx)=0

        f_x = self.ficnn(x_in)
        f_x0 = self.ficnn(x0_ext)
        grad_f_x0 = jax.grad(self.ficnn)(x0_ext)  # grad wrt the full input

        # Normalization term to set H(e0,ctx)=0 and grad_e H(e0,ctx)=0 (ctx part cancels since ctx_in - ctx_in = 0)
        f_norm = f_x0 + jnp.dot(grad_f_x0, x_in - x0_ext)

        # Regularization term to ensure a strict minimum in e-space
        f_reg = self.epsilon * jnp.sum((e_in - self.x0) ** 2)
        return f_x - f_norm + f_reg

class DissipationNN(eqx.Module):
    """
    Sparse (non-symmetric) dissipation matrix R(e, ctx) with only:
      diagonals:  R11, R22, R33, R44, R55
      off-diags:  R43, R45, R53, R54   (NOTE: non-symmetric; R34,R35 are forced zero)

    Output head predicts 9 scalars in this order:
      [R11, R22, R33, R44, R55, R43, R45, R53, R54]
    """
    layers: list
    activation: callable
    state_dim: int = eqx.field(static=True)
    input_dim: int  # e_dim + 4

    def __init__(self, key, state_dim, input_dim, hidden_sizes, activation):
        assert state_dim == 5, "Sparse DissipationNN assumes state_dim == 5."
        self.state_dim = state_dim
        self.input_dim = input_dim
        self.activation = activation

        if len(hidden_sizes) == 0:
            # no hidden layers
            self.layers = [eqx.nn.Linear(self.input_dim, 9, key=key)]
        else:
            keys = jax.random.split(key, len(hidden_sizes) + 1)
            self.layers = [eqx.nn.Linear(self.input_dim, hidden_sizes[0], key=keys[0])]
            for i in range(1, len(hidden_sizes)):
                self.layers.append(eqx.nn.Linear(hidden_sizes[i - 1], hidden_sizes[i], key=keys[i]))
            self.layers.append(eqx.nn.Linear(hidden_sizes[-1], 9, key=keys[-1]))

    def __call__(self, e, ctx):
        z = jnp.concatenate([e, ctx])

        # hidden layers
        for layer in self.layers[:-1]:
            z = self.activation(layer(z))

        # head: 9 parameters
        params = self.layers[-1](z)  # shape (9,)

        # unpack
        diag_vals = params[:5]            # R11,R22,R33,R44,R55
        off_vals  = params[5:]            # R43,R45,R53,R54 (in this exact order)

        # build R with two scatter writes
        R = jnp.zeros((5, 5), dtype=params.dtype)

        # set diagonal
        idx = jnp.arange(5)
        R = R.at[idx, idx].set(diag_vals)

        # set allowed off-diagonals: (3,2),(3,4),(4,2),(4,3)
        rows = jnp.array([3, 3, 4, 4])
        cols = jnp.array([2, 4, 2, 3])
        R = R.at[rows, cols].set(off_vals)

        return R


class DynamicJ_NN(eqx.Module):
    """
    Sparse/skew-symmetric J with only the first row learned:
      J[0, 1:] = [j12, j13, j14, j15] (predicted)
      J[i, 0]  = -J[0, i]             (skew)
      diag(J)  = 0
      all other entries = 0
    """
    layers: list
    state_dim: int = eqx.field(static=True)
    input_dim: int  # e_dim + 4
    activation: callable

    def __init__(self, key, state_dim, input_dim, hidden_sizes, activation):
        assert state_dim == 5, "DynamicJ_NN (sparse) assumes state_dim == 5."
        self.state_dim = state_dim
        self.input_dim = input_dim
        self.activation = activation

        out_dim = 4  # j12, j13, j14, j15
        if len(hidden_sizes) == 0:
            self.layers = [eqx.nn.Linear(self.input_dim, out_dim, key=key)]
        else:
            keys = jax.random.split(key, len(hidden_sizes) + 1)
            self.layers = [eqx.nn.Linear(self.input_dim, hidden_sizes[0], key=keys[0])]
            for i in range(1, len(hidden_sizes)):
                self.layers.append(eqx.nn.Linear(hidden_sizes[i - 1], hidden_sizes[i], key=keys[i]))
            self.layers.append(eqx.nn.Linear(hidden_sizes[-1], out_dim, key=keys[-1]))

    def __call__(self, e, ctx):
        # Concatenate normalized inputs, same convention as before
        z = jnp.concatenate([e, ctx])

        # Forward pass
        for layer in self.layers[:-1]:
            z = self.activation(layer(z))
        row_tail = self.layers[-1](z)  # shape (4,) -> [j12, j13, j14, j15]

        # Build J with two scatter writes (fast, JAX-friendly)
        J = jnp.zeros((5, 5), dtype=row_tail.dtype)
        J = J.at[0, 1:].set(row_tail)   # first row, columns 2..5
        J = J.at[1:, 0].set(-row_tail)  # first column, rows 2..5 (skew)

        # Diagonal already zero; everything else zero by construction
        return J


# --- The Combined Model ---

class Combined_PH_PINN(eqx.Module):
    """Main model combining a unified state predictor and PHPINN structure."""
    state_net: StateNN
    hamiltonian_net: HamiltonianNN
    dissipation_net: DissipationNN
    j_net: DynamicJ_NN

    def __init__(self, key, config: dict, state_dim: int):
        state_key, h_key, d_key, j_key = jax.random.split(key, 4)

        # Extract sub-configs
        state_net_config = config['state_net']
        h_net_config = config['hamiltonian_net']
        d_net_config = config['dissipation_net']
        j_net_config = config['j_net']
        activation_fn = config['activation']

        # The equilibrium point for the normalized error system is the origin.
        x0_norm = jnp.zeros(state_dim)

        # Input dimensions (normalized inputs): e_dim + 4 context features (x1,u1,u2,phi1)
        input_dim_with_ctx = state_dim + 4

        self.state_net = StateNN(key=state_key, config=state_net_config)
        self.hamiltonian_net = HamiltonianNN(
            h_key,
            in_size_with_ctx=input_dim_with_ctx,
            e_dim=state_dim,
            hidden_sizes=h_net_config['hidden_sizes'],
            x0=x0_norm,
            epsilon=h_net_config['epsilon'],
        )
        self.dissipation_net = DissipationNN(
            d_key,
            state_dim=state_dim,
            input_dim=input_dim_with_ctx,
            hidden_sizes=d_net_config['hidden_sizes'],
            activation=activation_fn,
        )
        self.j_net = DynamicJ_NN(
            j_key,
            state_dim=state_dim,
            input_dim=input_dim_with_ctx,
            hidden_sizes=j_net_config['hidden_sizes'],
            activation=activation_fn,
        )


# ==============================================================================
# 2. DATA HANDLING
# ==============================================================================

def generate_data(file_path: str):
    """
    Loads and prepares training data from a pre-generated pickle file containing
    multiple simulation runs.
    """
    print(f"Loading simulation data from {file_path}...")
    try:
        with open(file_path, 'rb') as f:
            # The file contains a list of result dictionaries
            all_runs_results = pickle.load(f)
    except FileNotFoundError:
        print(f"Error: Data file not found at {file_path}")
        print("Please run 'generate_data_for_PINN.py' to create the data file.")
        return None, None, None, None, None, None

    # Initialize lists to hold data from all runs
    # TEST MODIFICATION: Added all_x_dot list
    all_t, all_e, all_x, all_e_dot, all_x_dot, all_H, all_dHdt  = [], [], [], [], [], [], []

    # Process each simulation run
    for i, results in enumerate(all_runs_results):
        print(f"  ... processing run {i + 1}/{len(all_runs_results)}")

        # Extract data for the current run
        t = jnp.asarray(results['t'])
        e = jnp.vstack([
            results['e_x'], results['e_y'], results['e_z'],
            results['e_u'], results['e_phi']
        ]).T
        x = jnp.vstack([
            results['x1'], results['y1'], results['z1'], results['u1'], results['phi1'],
            results['x2'], results['y2'], results['z2'], results['u2'], results['phi2']
        ]).T
        e_dot_true = jnp.vstack([
            results['d_e_x'], results['d_e_y'], results['d_e_z'],
            results['d_e_u'], results['d_e_phi']
        ]).T
        H_analytical = jnp.asarray(results['Hamiltonian'])
        dHdt = jnp.asarray(results['dHdt'])

        # Stack the true derivatives for the x states
        x_dot_true = jnp.vstack([
            results['d_x1'], results['d_y1'], results['d_z1'], results['d_u1'], results['d_phi1'],
            results['d_x2'], results['d_y2'], results['d_z2'], results['d_u2'], results['d_phi2']
        ]).T

        # Append to the main lists
        all_t.append(t)
        all_e.append(e)
        all_x.append(x)
        all_e_dot.append(e_dot_true)
        all_H.append(H_analytical)
        all_dHdt.append(dHdt)
        all_x_dot.append(x_dot_true)

    # Concatenate all runs into single arrays
    final_t = jnp.concatenate(all_t)
    final_e = jnp.concatenate(all_e)
    final_x = jnp.concatenate(all_x)
    final_e_dot = jnp.concatenate(all_e_dot)
    final_H = jnp.concatenate(all_H)
    final_dHdt = jnp.concatenate(all_dHdt)
    final_x_dot = jnp.concatenate(all_x_dot)

    print("Data loading and aggregation complete.")
    return final_t, final_e, final_x, final_e_dot, final_x_dot, final_H, final_dHdt


def normalize(data, mean, std):
    """Normalizes data using pre-computed statistics."""
    return (data - mean) / (std + 1e-8)


def denormalize(data, mean, std):
    """Denormalizes data using pre-computed statistics."""
    return data * std + mean


def load_best_config_from_study(default_config, objective='h_loss'):
    """
    Loads the best hyperparameters from a completed Optuna study and updates
    the default training configuration.

    Args:
        default_config (dict): The default TRAIN_CONFIG dictionary.
        objective (str): The objective of the study to load ('val_loss' or 'h_loss').

    Returns:
        dict: The updated configuration dictionary with the best hyperparameters.
    """
    # Define the path to the Optuna database
    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data')  #
    db_path = os.path.join(results_dir, "optimize_hyperparams.db")  #
    storage_name = f"sqlite:///{db_path}"  #
    study_name = f"sphnn_pinn_optimization_{objective}"  #

    print("=" * 60)
    print(f"Attempting to load best hyperparameters for study '{study_name}'...")
    print(f"Database: {db_path}")
    print("=" * 60)

    try:
        study = optuna.load_study(study_name=study_name, storage=storage_name)  #
        best_params = study.best_trial.params
        print("‚úÖ Successfully loaded best hyperparameters from study.")
    except Exception as e:
        print(f"‚ùå WARNING: Could not load Optuna study. Reason: {e}")
        print("         Falling back to the default TRAIN_CONFIG.")
        return default_config

    # Create a deep copy to avoid modifying the original default config
    new_config = copy.deepcopy(default_config)

    # --- Update TRAIN_CONFIG with loaded hyperparameters ---

    # Update top-level keys that map directly
    direct_mapping_keys = [
        'batch_size', 'initial_learning_rate', 'decay_steps', 'lambda_warmup_epochs',
        'lambda_conservative_max', 'lambda_dissipative_max', 'lambda_physics_max',
        'lambda_j_structure_max', 'lambda_r_structure_max', 'lambda_phys_res_max'
    ]
    for key in direct_mapping_keys:
        if key in best_params:
            new_config[key] = best_params[key]

    # Update nested network architecture keys explicitly
    if 'mapping_size' in best_params:
        new_config['nn']['state_net']['fourier_features']['mapping_size'] = best_params['mapping_size']  #
    if 'scale' in best_params:
        new_config['nn']['state_net']['fourier_features']['scale'] = best_params['scale']  #
    if 'epsilon' in best_params:
        new_config['nn']['hamiltonian_net']['epsilon'] = best_params['epsilon']  #

    # Handle network hidden layer structures (built from width and depth)
    if 'state_width' in best_params and 'state_depth' in best_params:
        new_config['nn']['state_net']['hidden_sizes'] = [best_params['state_width']] * best_params['state_depth']  #
    if 'h_width' in best_params and 'h_depth' in best_params:
        new_config['nn']['hamiltonian_net']['hidden_sizes'] = [best_params['h_width']] * best_params['h_depth']  #
    if 'd_width' in best_params and 'd_depth' in best_params:
        new_config['nn']['dissipation_net']['hidden_sizes'] = [best_params['d_width']] * best_params['d_depth']  #
    if 'j_width' in best_params and 'j_depth' in best_params:
        new_config['nn']['j_net']['hidden_sizes'] = [best_params['j_width']] * best_params['j_depth']  #

    print("‚úÖ TRAIN_CONFIG has been updated with optimized hyperparameters.")
    return new_config

# ==============================================================================
# 3. TRAINING LOGIC
# ==============================================================================

# --- Helper functions for the physics-based loss terms ---

def _alpha(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [2 * m - 1., -1., -1., 2 * m - 1., -1., -1., 2 * m - 1.]
    return jnp.select(conds, choices, default=-1.)


def _beta(u1, u2, m):
    """Helper function for the dissipative field f_d."""
    conds = [
        jnp.logical_and(u1 >= 1, jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(u1 >= 1, u2 <= -1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 >= 1),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), jnp.logical_and(u2 > -1, u2 < 1)),
        jnp.logical_and(jnp.logical_and(u1 > -1, u1 < 1), u2 <= -1),
        jnp.logical_and(u1 <= -1, u2 >= 1),
        jnp.logical_and(u1 <= -1, jnp.logical_and(u2 > -1, u2 < 1)),
    ]
    choices = [
        2 * m * (u1 - 1), -4 * m, -2 * m * (u1 - 1), 0.,
        -2 * m * (u1 + 1), 4 * m, 2 * m * (u1 + 1),
    ]
    return jnp.select(conds, choices, default=0.)


def f_c_fn(e, x, hr_params):
    """Calculates the conservative vector field f_c(e)."""
    e_x, e_y, e_u, e_phi = e[0], e[1], e[3], e[4]
    x1, u1 = x[0], x[3]

    k, f, rho, d, r, s = \
        hr_params['k'], hr_params['f'], hr_params['rho'], hr_params['d'], hr_params['r'], hr_params['s']

    return jnp.array([
        e_y + 2 * k * f * u1 * x1 * e_u + rho * x1 * e_phi,
        -2 * d * x1 * e_x,
        r * s * e_x,
        e_x,
        e_x
    ])


def f_d_fn(e, x, hr_params):
    """Calculates the dissipative vector field f_d(e)."""
    e_x, e_y, e_z, e_u, e_phi = e[0], e[1], e[2], e[3], e[4]
    x1, u1, phi1, u2 = x[0], x[3], x[4], x[8]

    a, b, k, h, f, rho, g_e, r, q_param, m = \
        hr_params['a'], hr_params['b'], hr_params['k'], hr_params['h'], \
            hr_params['f'], hr_params['rho'], hr_params['ge'], hr_params['r'], \
            hr_params['q'], hr_params['m']

    N_val = -3 * a * x1 ** 2 + 2 * b * x1 + k * h + k * f * u1 ** 2 + rho * phi1 - 2 * g_e
    alpha_val = _alpha(u1, u2, m)
    beta_val = _beta(u1, u2, m)

    return jnp.array([
        N_val * e_x,
        -e_y,
        -r * e_z,
        alpha_val * e_u + beta_val,
        -q_param * e_phi
    ])


def hr_vector_field(t, state, N, params, I_ext, xi):
    """
    Calculates the time derivatives for a network of N coupled Hindmarsh-Rose neurons.
    """
    # Reshape the flat state vector into a 2D array (N neurons x 5 variables)
    state_matrix = state.reshape((N, 5))
    x, y, z, u, phi = state_matrix.T

    # Electrical Coupling
    x_diff = x[jnp.newaxis, :] - x[:, jnp.newaxis]
    electrical_coupling = params['ge'] * jnp.sum(jnp.asarray(xi) * x_diff, axis=1)

    # --- Calculate derivatives ---
    dxdt = (y - (params['a'] * x ** 3) + (params['b'] * x ** 2)
            + (params['k'] * (params['h'] + (params['f'] * (u ** 2))) * x)
            + (params['rho'] * phi * x) + I_ext
            + electrical_coupling
            )
    dydt = params['c'] - (params['d'] * x ** 2) - y
    dzdt = params['r'] * (params['s'] * (x + params['x0']) - z)
    dudt = -u + (params['m'] * (jnp.abs(u + 1.0) - jnp.abs(u - 1.0))) + x
    dphidt = x - (params['q'] * phi)

    # Assign calculated derivative vectors to the output matrix
    d_state_dt_matrix = jnp.zeros_like(state_matrix).at[:, 0].set(dxdt).at[:, 1].set(dydt) \
        .at[:, 2].set(dzdt).at[:, 3].set(dudt) \
        .at[:, 4].set(dphidt)

    return d_state_dt_matrix.flatten()


# In the loss_fn function signature

@eqx.filter_jit
def loss_fn(model: Combined_PH_PINN, t_batch_norm, e_true_batch_norm, x_true_batch_norm, e_dot_true_batch_norm,
            x_dot_true_batch_norm,
            H_true_batch_norm,
            lambda_conservative: float, lambda_dissipative: float, lambda_physics: float,
            lambda_j_structure: float,
            lambda_r_structure: float,
            lambda_phys_res: float,
            hr_params: dict,
            I_ext: jax.Array, xi: jax.Array,
            t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean, H_std):
    """Calculates the composite data and new physics-based losses."""

    # --- Part 1: State Prediction and Unified Data Fidelity Loss ---
    all_states_pred_norm = jax.vmap(model.state_net)(t_batch_norm)
    all_states_true_norm = jnp.concatenate([x_true_batch_norm, e_true_batch_norm], axis=1)
    data_loss = jnp.mean((all_states_pred_norm - all_states_true_norm) ** 2)

    x_pred_batch_norm = all_states_pred_norm[:, :10]
    e_pred_batch_norm = all_states_pred_norm[:, 10:]

    e_pred = denormalize(e_pred_batch_norm, e_mean, e_std)
    x_pred = denormalize(x_pred_batch_norm, x_mean, x_std)

    # Build normalized context inputs (x1,u1,u2,phi1) from normalized x_pred
    x1n = x_pred_batch_norm[:, 0]
    u1n = x_pred_batch_norm[:, 3]
    u2n = x_pred_batch_norm[:, 8]
    phi1n = x_pred_batch_norm[:, 4]
    ctx_batch_norm = jnp.stack([x1n, u1n, u2n, phi1n], axis=1)  # shape (B,4)

    # --- Part 2: Physics Calculations ---
    # --- e-derivatives from Autodiff ---
    get_autodiff_grad_e_slice = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[1]
    e_dot_autodiff_norm = jax.vmap(get_autodiff_grad_e_slice, in_axes=(None, 0))(model.state_net, t_batch_norm)
    e_dot_autodiff = e_dot_autodiff_norm * (e_std / (t_std + 1e-8))

    # --- x-derivatives from Autodiff ---
    get_autodiff_grad_x_slice = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[:10],
                                                       (t,), (jnp.ones_like(t),))[1]
    x_dot_autodiff_norm = jax.vmap(get_autodiff_grad_x_slice, in_axes=(None, 0))(model.state_net, t_batch_norm)
    x_dot_autodiff = x_dot_autodiff_norm * (x_std / (t_std + 1e-8))

    # --- Calculate derivatives from Analytical Equations ---
    f_c_batch = jax.vmap(f_c_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    f_d_batch = jax.vmap(f_d_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    e_dot_diss_cons = f_c_batch + f_d_batch

    # --- Analytical derivatives for x from the HR vector field ---
    vmapped_hr_vector_field = jax.vmap(hr_vector_field, in_axes=(None, 0, None, None, None, None))
    x_dot_vectorfield = vmapped_hr_vector_field(None, x_pred, 2, hr_params, I_ext, xi)

    # --- Part 3: Loss Components ---
    # Physics Residual Losses (error and HR states)
    physics_residual_loss1 = jnp.mean((e_dot_autodiff - e_dot_diss_cons) ** 2)
    physics_residual_loss2 = jnp.mean((x_dot_autodiff - x_dot_vectorfield) ** 2)
    physics_residual_loss = physics_residual_loss1 + physics_residual_loss2

    # PH Structure Losses
    # grad only w.r.t. e (ctx is treated as constant via closure)
    def grad_H_single(e_norm, ctx_norm):
        return jax.grad(lambda ee: model.hamiltonian_net(ee, ctx_norm))(e_norm)

    grad_H_norm = jax.vmap(grad_H_single)(e_pred_batch_norm, ctx_batch_norm)
    grad_H = grad_H_norm / (e_std + 1e-8)

    J_norm = jax.vmap(model.j_net)(e_pred_batch_norm, ctx_batch_norm)
    R_norm = jax.vmap(model.dissipation_net)(e_pred_batch_norm, ctx_batch_norm)
    e_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
    e_dot_from_structure = e_dot_from_structure_norm * e_std
    loss_phys = jnp.mean((e_dot_autodiff - e_dot_from_structure) ** 2)

    # Conservative Loss
    lie_derivative = jax.vmap(jnp.dot)(grad_H, f_c_batch)
    loss_conservative = jnp.mean(lie_derivative ** 2)

    # Conservative Structure Fidelity Loss (J*gradH vs. f_c)
    j_grad_h_norm = jax.vmap(lambda j, g: j @ g)(J_norm, grad_H_norm)
    j_grad_h = j_grad_h_norm * e_std  # Denormalize using the same convention as loss_phys
    loss_j_structure = jnp.mean((f_c_batch - j_grad_h) ** 2)

    # Dissipative Loss
    dHdt_from_autodiff = jax.vmap(jnp.dot)(grad_H, e_dot_autodiff)
    dHdt_from_equations = jax.vmap(jnp.dot)(grad_H, f_d_batch)
    loss_dissipative = jnp.mean((dHdt_from_autodiff - dHdt_from_equations) ** 2)

    # Dissipative Structure Fidelity Loss (-R*gradH vs. f_d) ---
    r_grad_h_norm = jax.vmap(lambda r, g: -r @ g)(R_norm, grad_H_norm)
    r_grad_h = r_grad_h_norm * e_std  # Denormalize to physical units
    loss_r_structure = jnp.mean((f_d_batch - r_grad_h) ** 2)

    # Hamiltonian Loss (for monitoring only)
    H_pred_norm = jax.vmap(lambda e, ctx: model.hamiltonian_net(e, ctx))(e_pred_batch_norm, ctx_batch_norm)
    H_pred = denormalize(H_pred_norm, H_mean, H_std)
    H_true = denormalize(H_true_batch_norm, H_mean, H_std)

    H_pred_aligned = (-1) * H_pred - jnp.mean((-1) * H_pred) + jnp.mean(H_true)  # flip the sign
    loss_hamiltonian = jnp.mean((H_pred_aligned - H_true) ** 2)

    # --- Part 4: Total Loss ---
    total_loss = (data_loss
                  + (lambda_conservative * loss_conservative)
                  + (lambda_dissipative * loss_dissipative)
                  + (lambda_physics * loss_phys)
                  + (lambda_j_structure * loss_j_structure)
                  + (lambda_r_structure * loss_r_structure)
                  + (lambda_phys_res * physics_residual_loss))

    loss_components = {
        "total": total_loss,
        "data_unified": data_loss,
        "physics_residual": physics_residual_loss,
        "phys": loss_phys,
        "conservative": loss_conservative,
        "dissipative": loss_dissipative,
        "j_structure": loss_j_structure,
        "r_structure": loss_r_structure,
        "hamiltonian": loss_hamiltonian,
    }
    return total_loss, loss_components


@eqx.filter_jit
def train_step(model, opt_state, optimizer, t_batch_norm, e_batch_norm, x_batch_norm, e_dot_batch_norm,
               x_dot_batch_norm,
               H_batch_norm,
               lambda_conservative, lambda_dissipative, lambda_physics,
               lambda_j_structure,
               lambda_r_structure,
               lambda_phys_res,
               hr_params, I_ext, xi,
               t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean,
               H_std):
    """Performs a single training step."""
    (loss_val, loss_components), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(
        model, t_batch_norm, e_batch_norm, x_batch_norm, e_dot_batch_norm,
        x_dot_batch_norm,
        H_batch_norm,
        lambda_conservative, lambda_dissipative, lambda_physics,
        lambda_j_structure,
        lambda_r_structure,
        lambda_phys_res,
        hr_params, I_ext, xi,
        t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean, H_std
    )
    updates, opt_state = optimizer.update(grads, opt_state, model)
    model = eqx.apply_updates(model, updates)
    return model, opt_state, loss_val, loss_components

@eqx.filter_jit
def evaluate_model(model, t_batch_norm, e_batch_norm, x_batch_norm, e_dot_batch_norm,
                   x_dot_batch_norm,
                   H_batch_norm,
                   lambda_conservative, lambda_dissipative, lambda_physics,
                   lambda_j_structure,
                   lambda_r_structure,
                   lambda_phys_res,
                   hr_params, I_ext, xi,
                   t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean,
                   H_std):
    """Calculates the loss for the validation set."""
    # This function now returns both the total loss and the components dictionary
    loss_val, loss_components = loss_fn(
        model, t_batch_norm, e_batch_norm, x_batch_norm, e_dot_batch_norm,
        x_dot_batch_norm,
        H_batch_norm,
        lambda_conservative, lambda_dissipative, lambda_physics,
        lambda_j_structure,
        lambda_r_structure,
        lambda_phys_res,
        hr_params, I_ext, xi,
        t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean, H_std
    )
    return loss_val, loss_components
# ==============================================================================
# 4. MAIN EXECUTION LOGIC
# ==============================================================================
def main():
    """Main function to run the training and evaluation."""

    # ==========================================================================
    # --- Centralized Training and Model Configuration ---
    # ==========================================================================

    TRAIN_CONFIG = {
        # --- General Setup ---
        "seed": 42,
        "data_file_path": os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data/',
                                       'error_system_data.pkl'),
        "output_dir": os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'temp/'),

        # --- Training Hyperparameters ---
        "batch_size": 64000,
        "validation_split": 0.2,
        "epochs": 100,

        # --- Optimizer and Learning Rate Schedule ---
        "initial_learning_rate": 0.0029742742447583428,
        "end_learning_rate": 1e-5,
        "decay_steps": 1321,

        # --- Physics Loss Hyperparameters ---
        'lambda_conservative_max': 26.67464802508848,
        'lambda_dissipative_max': 4.106828123315793,
        'lambda_physics_max': 29.326026352964497,
        'lambda_j_structure_max': 9.540936935607018,
        'lambda_r_structure_max': 2.397980403924165,
        'lambda_phys_res_max': 5.519988732552705,
        'lambda_warmup_epochs': 683,

        # --- System Parameters ---
        "hr_params": {
            **DEFAULT_PARAMS,
            'ge': 0.62,
        },
        "I_ext": jnp.array([0.8, 0.8]),
        "xi": jnp.array([[0, 1], [1, 0]]),

        # --- Visualization ---
        "run_to_visualize_idx": 0,
        "vis_start_ratio": 0,
        "vis_end_ratio": 0.5,

        # --- Neural Network Architectures ---
        "nn": {
            "state_net": {
                "out_size": 15,  # Will be updated dynamically based on data shape (x_dim + e_dim)
                "hidden_sizes": [512, 512, 512, 512],
                "fourier_features": {
                    "in_size": 1,
                    "mapping_size": 128,
                    "scale": 56.30314996019258
                }
            },
            "hamiltonian_net": {
                "hidden_sizes": [32, 32],
                "epsilon": 2.243072009175816
            },
            "dissipation_net": {
                "hidden_sizes": [32, 32, 32, 32],
            },
            "j_net": {
                "hidden_sizes": [2],
            },
            "activation": jax.nn.softplus,
        }
    }

    # --- Overwrite config with best params from Optuna study if enabled ---
    read_config_from_study = True
    if read_config_from_study:
        # Load the study that optimized for Hamiltonian loss ('h_loss')
        TRAIN_CONFIG = load_best_config_from_study(TRAIN_CONFIG, objective='val_loss')

    # ==========================================================================

    # --- Setup ---
    key = jax.random.PRNGKey(TRAIN_CONFIG["seed"])
    model_key, data_key = jax.random.split(key)
    hr_params = TRAIN_CONFIG["hr_params"]
    I_ext = TRAIN_CONFIG["I_ext"]
    xi = TRAIN_CONFIG["xi"]

    # --- Generate and Prepare Data ---
    # TEST MODIFICATION: Capture x_dot_true from generate_data

    t, e, x, e_dot_true, x_dot_true, H_analytical, dHdt_analytical = generate_data(TRAIN_CONFIG["data_file_path"])
    if t is None:
        sys.exit("Exiting: Data loading failed.")

    num_samples = e.shape[0]
    perm = jax.random.permutation(data_key, num_samples)
    # TEST MODIFICATION: Add x_dot_true to the shuffled variables
    t_shuffled, e_shuffled, x_shuffled, e_dot_shuffled, x_dot_shuffled, H_shuffled = \
        t[perm], e[perm], x[perm], e_dot_true[perm], x_dot_true[perm], H_analytical[perm]
    t_shuffled = t_shuffled.reshape(-1, 1)

    split_idx = int(num_samples * (1 - TRAIN_CONFIG["validation_split"]))
    t_train, t_val = jnp.split(t_shuffled, [split_idx])
    e_train, e_val = jnp.split(e_shuffled, [split_idx])
    x_train, x_val = jnp.split(x_shuffled, [split_idx])
    e_dot_train, e_dot_val = jnp.split(e_dot_shuffled, [split_idx])
    # TEST MODIFICATION: Split x_dot_true
    x_dot_train, x_dot_val = jnp.split(x_dot_shuffled, [split_idx])
    H_train, H_val = jnp.split(H_shuffled, [split_idx])

    # --- Normalize Data (using ONLY training set statistics) ---
    t_mean, t_std = jnp.mean(t_train), jnp.std(t_train)
    e_mean, e_std = jnp.mean(e_train, axis=0), jnp.std(e_train, axis=0)
    x_mean, x_std = jnp.mean(x_train, axis=0), jnp.std(x_train, axis=0)
    e_dot_mean, e_dot_std = jnp.mean(e_dot_train, axis=0), jnp.std(e_dot_train, axis=0)
    # TEST MODIFICATION: Calculate norm stats for x_dot
    x_dot_mean, x_dot_std = jnp.mean(x_dot_train, axis=0), jnp.std(x_dot_train, axis=0)
    H_mean, H_std = jnp.mean(H_train), jnp.std(H_train)

    t_train_norm = normalize(t_train, t_mean, t_std)
    e_train_norm = normalize(e_train, e_mean, e_std)
    x_train_norm = normalize(x_train, x_mean, x_std)
    e_dot_train_norm = normalize(e_dot_train, e_dot_mean, e_dot_std)
    # TEST MODIFICATION: Normalize x_dot_train
    x_dot_train_norm = normalize(x_dot_train, x_dot_mean, x_dot_std)
    H_train_norm = normalize(H_train, H_mean, H_std)

    t_val_norm = normalize(t_val, t_mean, t_std)
    e_val_norm = normalize(e_val, e_mean, e_std)
    x_val_norm = normalize(x_val, x_mean, x_std)
    e_dot_val_norm = normalize(e_dot_val, e_dot_mean, e_dot_std)
    # TEST MODIFICATION: Normalize x_dot_val
    x_dot_val_norm = normalize(x_dot_val, x_dot_mean, x_dot_std)
    H_val_norm = normalize(H_val, H_mean, H_std)

    # --- Initialize Model ---
    e_dim = e_train.shape[1]
    x_dim = x_train.shape[1]

    # Update network config with data-dependent shapes
    nn_config = TRAIN_CONFIG["nn"]
    nn_config["state_net"]["out_size"] = e_dim + x_dim

    model = Combined_PH_PINN(key=model_key, config=nn_config, state_dim=e_dim)

    # --- Training Loop ---
    batch_size = TRAIN_CONFIG["batch_size"]
    epochs = TRAIN_CONFIG["epochs"]
    num_batches = t_train_norm.shape[0] // batch_size
    if num_batches == 0 and t_train_norm.shape[0] > 0:
        print(f"Warning: batch_size ({batch_size}) > num samples. Setting num_batches to 1.")
        num_batches = 1

    lr_schedule = optax.linear_schedule(
        init_value=TRAIN_CONFIG["initial_learning_rate"],
        end_value=TRAIN_CONFIG["end_learning_rate"],
        transition_steps=TRAIN_CONFIG["decay_steps"]
    )
    optimizer = optax.adamw(learning_rate=lr_schedule)
    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))

    train_losses, val_losses = [], []
    phys_losses, conservative_losses, dissipative_losses, hamiltonian_losses = [], [], [], []
    best_model, best_val_loss, best_h_loss = model, jnp.inf, jnp.inf
    data_unified_losses, physics_residual_losses = [], []
    j_structure_losses, r_structure_losses = [], []

    print(f"Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        # Loss weight warmup schedule
        warmup_factor = jnp.minimum(1.0, (epoch + 1) / TRAIN_CONFIG["lambda_warmup_epochs"])
        current_lambda_conservative = TRAIN_CONFIG["lambda_conservative_max"] * warmup_factor
        current_lambda_dissipative = TRAIN_CONFIG["lambda_dissipative_max"] * warmup_factor
        current_lambda_physics = TRAIN_CONFIG["lambda_physics_max"] * warmup_factor
        current_lambda_j_structure = TRAIN_CONFIG["lambda_j_structure_max"] * warmup_factor
        current_lambda_r_structure = TRAIN_CONFIG["lambda_r_structure_max"] * warmup_factor
        current_lambda_phys_res = TRAIN_CONFIG["lambda_phys_res_max"] * warmup_factor

        key, shuffle_key = jax.random.split(key)
        perm = jax.random.permutation(shuffle_key, t_train_norm.shape[0])
        # TEST MODIFICATION: Add x_dot_shuffled
        t_shuffled, e_shuffled, x_shuffled, e_dot_shuffled, x_dot_shuffled, H_shuffled = \
            t_train_norm[perm], e_train_norm[perm], x_train_norm[perm], \
                e_dot_train_norm[perm], x_dot_train_norm[perm], H_train_norm[perm]

        # Initialize epoch loss accumulators
        epoch_losses = {k: 0.0 for k in
                        ["total", "data_unified", "physics_residual", "phys", "conservative", "dissipative",
                         "j_structure", "r_structure", "hamiltonian"]}

        for i in range(num_batches):
            start, end = i * batch_size, (i + 1) * batch_size
            # TEST MODIFICATION: Add x_dot_b
            t_b, e_b, x_b = t_shuffled[start:end], e_shuffled[start:end], x_shuffled[start:end]
            e_dot_b, x_dot_b, H_b = e_dot_shuffled[start:end], x_dot_shuffled[start:end], H_shuffled[start:end]

            # TEST MODIFICATION: Pass new data to train_step
            model, opt_state, train_loss_val, loss_comps = train_step(
                model, opt_state, optimizer, t_b, e_b, x_b, e_dot_b, x_dot_b, H_b,
                current_lambda_conservative, current_lambda_dissipative, current_lambda_physics,
                current_lambda_j_structure,
                current_lambda_r_structure,
                current_lambda_phys_res,
                hr_params, I_ext, xi,
                t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean, H_std
            )
            for k in epoch_losses:
                if k in loss_comps:
                    epoch_losses[k] += loss_comps[k]

        # Calculate average losses for the epoch
        avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}

        # TEST MODIFICATION: Pass new data to evaluate_model
        val_loss, val_loss_comps = evaluate_model(
            model, t_val_norm, e_val_norm, x_val_norm, e_dot_val_norm, x_dot_val_norm, H_val_norm,
            current_lambda_conservative, current_lambda_dissipative, current_lambda_physics,
            current_lambda_j_structure,
            current_lambda_r_structure,
            current_lambda_phys_res,
            hr_params, I_ext, xi,
            t_mean, t_std, e_mean, e_std, x_mean, x_std, e_dot_mean, e_dot_std, x_dot_mean, x_dot_std, H_mean, H_std
        )

        # Append all losses for plotting
        train_losses.append(avg_losses["total"])
        val_losses.append(val_loss)
        phys_losses.append(avg_losses["phys"])
        conservative_losses.append(avg_losses["conservative"])
        dissipative_losses.append(avg_losses["dissipative"])
        hamiltonian_losses.append(avg_losses["hamiltonian"])
        data_unified_losses.append(avg_losses["data_unified"])
        physics_residual_losses.append(avg_losses["physics_residual"])
        j_structure_losses.append(avg_losses["j_structure"])
        r_structure_losses.append(avg_losses["r_structure"])

        # --- UPDATED MODEL SAVING LOGIC ---
        # Get the Hamiltonian loss from the validation set results
        val_h_loss = val_loss_comps['hamiltonian']

        # Save the model if the validation Hamiltonian loss is the best we've seen
        if val_h_loss < best_h_loss:
            best_h_loss = val_h_loss
            best_val_loss = val_loss  # Also save the corresponding total val loss
            best_model = model

        if (epoch + 1) % 1 == 0 or epoch == 0:
            log_str = (
                f"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_losses['total']:.4f} | Val Loss: {val_loss:.4f} | "
                f"H_Loss: {avg_losses['hamiltonian']:.4f} | "
                f"Data: {avg_losses['data_unified']:.4f} | PhysRes: {avg_losses['physics_residual']:.4f} | "
                f"PH: {avg_losses['phys']:.4f} | J_Struct: {avg_losses['j_structure']:.4f} | "
                f"R_Struct: {avg_losses['r_structure']:.4f} | "
                f"Cons: {avg_losses['conservative']:.4f} | Diss: {avg_losses['dissipative']:.4f}"
            )
            print(log_str)

    print("Training finished.")
    print(f"Best validation loss achieved: {best_val_loss:.6f}")

    # ==============================================================================
    # 5. VISUALIZATION AND ANALYSIS
    # ==============================================================================
    output_dir = TRAIN_CONFIG["output_dir"]
    os.makedirs(output_dir, exist_ok=True)

    run_to_visualize_idx = TRAIN_CONFIG["run_to_visualize_idx"]
    print(f"\nGenerating visualization plots for simulation run #{run_to_visualize_idx + 1}...")

    # Load the data again to isolate a single run for clean plotting
    with open(TRAIN_CONFIG["data_file_path"], 'rb') as f:
        all_runs = pickle.load(f)

    # Ensure the chosen index is valid
    if run_to_visualize_idx >= len(all_runs):
        print(
            f"Error: 'run_to_visualize_idx' ({run_to_visualize_idx}) is out of bounds. Max is {len(all_runs) - 1}. Setting to 0.")
        run_to_visualize_idx = 0

    vis_results = all_runs[run_to_visualize_idx]

    # Use the selected run's data for all subsequent plotting
    t_test = jnp.asarray(vis_results['t']).reshape(-1, 1)
    e_test = jnp.vstack([
        vis_results['e_x'], vis_results['e_y'], vis_results['e_z'],
        vis_results['e_u'], vis_results['e_phi']
    ]).T
    x_test = jnp.vstack([
        vis_results['x1'], vis_results['y1'], vis_results['z1'], vis_results['u1'], vis_results['phi1'],
        vis_results['x2'], vis_results['y2'], vis_results['z2'], vis_results['u2'], vis_results['phi2']
    ]).T
    e_dot_test = jnp.vstack([
        vis_results['d_e_x'], vis_results['d_e_y'], vis_results['d_e_z'],
        vis_results['d_e_u'], vis_results['d_e_phi']
    ]).T
    H_analytical_vis = jnp.asarray(vis_results['Hamiltonian'])

    # Get x_dot_true from the data
    x_dot_test = jnp.vstack([
        vis_results['d_x1'], vis_results['d_y1'], vis_results['d_z1'], vis_results['d_u1'], vis_results['d_phi1'],
        vis_results['d_x2'], vis_results['d_y2'], vis_results['d_z2'], vis_results['d_u2'], vis_results['d_phi2']
    ]).T

    # Normalize the visualization data using the previously computed training statistics
    t_test_norm = normalize(t_test, t_mean, t_std)

    # --- Get all model predictions for the full dataset ---
    all_states_pred_norm = jax.vmap(best_model.state_net)(t_test_norm)
    x_pred_norm = all_states_pred_norm[:, :10]
    e_pred_norm = all_states_pred_norm[:, 10:]

    e_pred = denormalize(e_pred_norm, e_mean, e_std)
    x_pred = denormalize(x_pred_norm, x_mean, x_std)

    # Build normalized context for viz
    x1n_v = x_pred_norm[:, 0]
    u1n_v = x_pred_norm[:, 3]
    u2n_v = x_pred_norm[:, 8]
    phi1n_v = x_pred_norm[:, 4]
    ctx_norm_v = jnp.stack([x1n_v, u1n_v, u2n_v, phi1n_v], axis=1)

    # --- Calculate all derivatives for comparison ---
    # Autodiff derivatives
    get_e_slice_autodiff_grad = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[10:], (t,), (jnp.ones_like(t),))[1]
    e_dot_autodiff_norm = jax.vmap(get_e_slice_autodiff_grad, in_axes=(None, 0))(best_model.state_net, t_test_norm)
    e_dot_autodiff = e_dot_autodiff_norm * (e_std / (t_std + 1e-8))

    get_x_slice_autodiff_grad = lambda net, t: jax.jvp(lambda t_scalar: net(t_scalar)[:10], (t,), (jnp.ones_like(t),))[1]
    x_dot_autodiff_norm = jax.vmap(get_x_slice_autodiff_grad, in_axes=(None, 0))(best_model.state_net, t_test_norm)
    x_dot_autodiff = x_dot_autodiff_norm * (x_std / (t_std + 1e-8))

    # Analytical derivatives from predicted states
    vmapped_hr_vector_field = jax.vmap(hr_vector_field, in_axes=(None, 0, None, None, None, None))
    x_dot_from_vectorfield_vis = vmapped_hr_vector_field(None, x_pred, 2, hr_params, I_ext, xi)
    f_c_batch_vis = jax.vmap(f_c_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    f_d_batch_vis = jax.vmap(f_d_fn, in_axes=(0, 0, None))(e_pred, x_pred, hr_params)
    e_dot_from_equations = f_c_batch_vis + f_d_batch_vis

    # PH structure derivative
    def grad_H_single_v(e_norm, ctx_norm):
        return jax.grad(lambda ee: best_model.hamiltonian_net(ee, ctx_norm))(e_norm)

    grad_H_norm = jax.vmap(grad_H_single_v)(e_pred_norm, ctx_norm_v)
    J_norm = jax.vmap(best_model.j_net)(e_pred_norm, ctx_norm_v)
    R_norm = jax.vmap(best_model.dissipation_net)(e_pred_norm, ctx_norm_v)
    e_dot_from_structure_norm = jax.vmap(lambda j, r, g: (j - r) @ g)(J_norm, R_norm, grad_H_norm)
    e_dot_from_structure = e_dot_from_structure_norm * e_std

    # --- Plot 1: Learned vs Analytical Hamiltonian ---
    print("Comparing learned Hamiltonian with analytical solution...")
    H_learned_norm = jax.vmap(lambda e, ctx: best_model.hamiltonian_net(e, ctx))(e_pred_norm, ctx_norm_v)
    H_learned_flipped = (-1) * H_learned_norm
    H_learned_aligned = H_learned_flipped - jnp.mean(H_learned_flipped) + jnp.mean(H_analytical_vis)

    split_start = int(len(t_test) * TRAIN_CONFIG["vis_start_ratio"])
    split_end = int(len(t_test) * TRAIN_CONFIG["vis_end_ratio"])

    plt.figure(figsize=(12, 7))
    plt.plot(t_test[split_start:split_end], H_analytical_vis[split_start:split_end], label='Analytical Hamiltonian',
             color='blue')
    plt.plot(t_test[split_start:split_end], H_learned_aligned[split_start:split_end],
             label='Learned Hamiltonian (Aligned)', color='red')
    plt.title("Time Evolution of Hamiltonians", fontsize=16)
    plt.xlabel("Time", fontsize=14)
    plt.ylabel("Hamiltonian Value", fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, 'hamiltonian_comparison.png'), dpi=300)
    plt.tight_layout()

    # --- Plot 2: Training, Validation, and Physics Losses ---
    # --- Plot 2: Training, Validation, and Physics Losses ---
    plt.figure(figsize=(14, 9))
    plt.plot(train_losses, label='Total Training Loss', color='black', linewidth=2.5)
    plt.plot(val_losses, label='Total Validation Loss', color='firebrick', linewidth=2.5)

    # Data and Physics Residuals
    plt.plot(data_unified_losses, label='Data Fidelity Loss', color='dodgerblue')
    plt.plot(physics_residual_losses, label='Physics Residual Loss', color='darkorange')

    # PH Structure Losses
    plt.plot(phys_losses, label='PH Structure Loss (phys)', color='purple', alpha=0.9)
    plt.plot(j_structure_losses, label='J Structure Loss', color='brown', alpha=0.7)
    plt.plot(r_structure_losses, label='R Structure Loss', color='magenta', alpha=0.7)

    # Core Physics Losses
    plt.plot(conservative_losses, label='Conservative Loss', color='green', alpha=0.9)
    plt.plot(dissipative_losses, label='Dissipative Loss', color='darkcyan', alpha=0.9)

    # Monitoring Loss
    plt.plot(hamiltonian_losses, label='Hamiltonian Loss (Monitor)', color='gold')

    plt.yscale('log')
    plt.title('Training, Validation, and All Physics Losses Over Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (Log Scale)', fontsize=12)
    plt.legend(fontsize=10, loc='upper right', ncol=2)
    plt.grid(True, which="both", ls="--", alpha=0.6)
    plt.savefig(os.path.join(output_dir, 'training_losses_detailed.png'), dpi=300)
    plt.tight_layout()
    # --- Plot 3: Derivative Comparison (Physics Fidelity) for Error States ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 12), sharex=True)
    state_labels_e_dot = [r'$\dot{e}_x$', r'$\dot{e}_y$', r'$\dot{e}_z$', r'$\dot{e}_u$', r'$\dot{e}_\phi$']
    fig.suptitle("Error Derivative Fidelity Comparison (e_dot)", fontsize=18, y=0.99)

    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], e_dot_test[split_start:split_end, i], label='True Derivative',
                     color='green', linewidth=2, alpha=0.8)
        axes[i].plot(t_test[split_start:split_end], e_dot_autodiff[split_start:split_end, i], label='Autodiff',
                     color='orange')
        axes[i].plot(t_test[split_start:split_end], e_dot_from_equations[split_start:split_end, i],
                     label='Analytical Eq.', color='purple')
        axes[i].plot(t_test[split_start:split_end], e_dot_from_structure[split_start:split_end, i],
                     label='PH Structure', color='red')

        axes[i].set_ylabel(state_labels_e_dot[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')

    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'error_derivative_fidelity.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- Plot 4: Derivative Comparison (Physics Fidelity) for HR States ---
    fig, axes = plt.subplots(x_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_x_dot = [
        r'$\dot{x}_1$', r'$\dot{y}_1$', r'$\dot{z}_1$', r'$\dot{u}_1$', r'$\dot{\phi}_1$',
        r'$\dot{x}_2$', r'$\dot{y}_2$', r'$\dot{z}_2$', r'$\dot{u}_2$', r'$\dot{\phi}_2$'
    ]
    fig.suptitle("HR Derivative Fidelity Comparison (x_dot)", fontsize=18, y=0.99)

    for i in range(x_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], x_dot_test[split_start:split_end, i], label='True Derivative',
                     color='green', linewidth=2, alpha=0.8)
        axes[i].plot(t_test[split_start:split_end], x_dot_autodiff[split_start:split_end, i], label='Autodiff',
                     color='orange')
        axes[i].plot(t_test[split_start:split_end], x_dot_from_vectorfield_vis[split_start:split_end, i],
                     label='Analytical Eq.', color='purple')

        axes[i].set_ylabel(state_labels_x_dot[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')

    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'hr_derivative_fidelity.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- Plot 5: Error System State Trajectories (e) ---
    fig, axes = plt.subplots(e_test.shape[1], 1, figsize=(12, 10), sharex=True)
    state_labels_error = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']
    fig.suptitle("Error System State 'e' Prediction: True vs. Predicted", fontsize=18, y=0.99)
    for i in range(e_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], e_test[split_start:split_end, i], 'b', label='True State',
                     alpha=0.9)
        axes[i].plot(t_test[split_start:split_end], e_pred[split_start:split_end, i], 'r', label='Predicted State')
        axes[i].set_ylabel(state_labels_error[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'error_state_e_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- Plot 6: HR System State Trajectories (x) ---
    fig, axes = plt.subplots(x_test.shape[1], 1, figsize=(12, 18), sharex=True)
    state_labels_x = [
        r'$x_1$', r'$y_1$', r'$z_1$', r'$u_1$', r'$\phi_1$',
        r'$x_2$', r'$y_2$', r'$z_2$', r'$u_2$', r'$\phi_2$'
    ]
    fig.suptitle("HR System State 'x' Prediction: True vs. Predicted", fontsize=18, y=0.99)
    for i in range(x_test.shape[1]):
        axes[i].plot(t_test[split_start:split_end], x_test[split_start:split_end, i], 'b', label='True State',
                     alpha=0.9)
        axes[i].plot(t_test[split_start:split_end], x_pred[split_start:split_end, i], 'r', label='Predicted State')
        axes[i].set_ylabel(state_labels_x[i], fontsize=14)
        axes[i].grid(True)
        axes[i].legend(loc='upper right')
    axes[-1].set_xlabel("Time", fontsize=14)
    fig.savefig(os.path.join(output_dir, 'hr_state_x_prediction.png'), dpi=300)
    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # === dH/dt comparison plot ==============================================
    # === Plot 7: dH/dt (analytical vs three predictions) ====================
    # Pull analytical dH/dt from the selected run
    dHdt_analytical_vis = jnp.asarray(vis_results['dHdt'])  # shape [T]

    # Gradient of H wrt e, holding ctx fixed (work in *normalized* coordinates)
    def grad_H_wrt_e(e_norm, ctx_norm):
        return jax.grad(lambda ee: best_model.hamiltonian_net(ee, ctx_norm))(e_norm)  # [5]

    grad_H_norm = jax.vmap(grad_H_wrt_e)(e_pred_norm, ctx_norm_v)  # [T, 5]
    # Convert gradient to physical units: dH/de_phys = dH/de_norm * (de_norm/de_phys)
    # with e_norm = (e_phys - Œº)/œÉ  ->  de_norm/de_phys = 1/œÉ  ->  dH/de_phys = (dH/de_norm)/œÉ
    grad_H = grad_H_norm / (e_std + 1e-8)  # [T, 5]

    # Compose the three predicted dH/dt variants
    dHdt_pred_true = jnp.einsum('ti,ti->t', grad_H, e_dot_test)  # ‚àáH ¬∑ eÃá_true
    dHdt_pred_structure = jnp.einsum('ti,ti->t', grad_H, e_dot_from_structure)  # ‚àáH ¬∑ eÃá_struct
    dHdt_pred_autodiff = jnp.einsum('ti,ti->t', grad_H, e_dot_autodiff)  # ‚àáH ¬∑ eÃá_autodiff
    dHdt_pred_pH = -jnp.einsum('ti,tij,tj->t', grad_H_norm, R_norm, grad_H_norm) # -‚àáH.T . R . ‚àáH

    plt.figure(figsize=(12, 6))
    plt.plot(t_test[split_start:split_end], np.asarray(dHdt_analytical_vis[split_start:split_end]), label=r'$\dot H$ (analytical)')
    # plt.plot(t_test[split_start:split_end], np.asarray(dHdt_pred_true[split_start:split_end])*(-1), label=r'$\nabla H \cdot \dot e_{\mathrm{true}}$')
    # plt.plot(t_test[split_start:split_end], np.asarray(dHdt_pred_structure[split_start:split_end])*(-1), label=r'$\nabla H \cdot \dot e_{\mathrm{structure}}$')
    # plt.plot(t_test[split_start:split_end], np.asarray(dHdt_pred_autodiff[split_start:split_end])*(-1), label=r'$\nabla H \cdot \dot e_{\mathrm{autodiff}}$')
    plt.plot(t_test[split_start:split_end], np.asarray(dHdt_pred_pH[split_start:split_end])*(-1), label=r'$-(\nabla H)^\top R\,\nabla H$')
    plt.xlabel('Time');
    plt.ylabel(r'$\dot H$')
    plt.title(r'$\dot H$: analytical vs. predictions')
    plt.legend(loc='best', frameon=False)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'dHdt_comparison.png'), dpi=300)
    # ======================================================================

    # --- Plot 8 & 9: Mean J and R matrices over the whole dataset ------------
    print("Computing mean J and R matrices over the whole dataset...")

    # Use the model across the entire normalized time set (train + val)
    t_all_norm = jnp.concatenate([t_train_norm, t_val_norm], axis=0)

    # Predict [x, e] for all timestamps
    all_states_pred_norm_full = jax.vmap(best_model.state_net)(t_all_norm)
    x_pred_norm_all = all_states_pred_norm_full[:, :10]
    e_pred_norm_all = all_states_pred_norm_full[:, 10:]

    # Build ctx = [x1, u1, u2, phi1] from normalized x predictions
    x1n_all  = x_pred_norm_all[:, 0]
    u1n_all  = x_pred_norm_all[:, 3]
    u2n_all  = x_pred_norm_all[:, 8]
    phi1n_all = x_pred_norm_all[:, 4]
    ctx_norm_all = jnp.stack([x1n_all, u1n_all, u2n_all, phi1n_all], axis=1)  # [T,4]

    # Evaluate J and R for all samples
    J_stack = jax.vmap(best_model.j_net)(e_pred_norm_all, ctx_norm_all)           # [T, 5, 5]
    R_stack = jax.vmap(best_model.dissipation_net)(e_pred_norm_all, ctx_norm_all) # [T, 5, 5]

    # Mean across the dataset
    J_mean = jnp.mean(J_stack, axis=0)  # [5,5]
    R_mean = jnp.mean(R_stack, axis=0)  # [5,5]

    # Plot helpers
    var_labels = [r'$e_x$', r'$e_y$', r'$e_z$', r'$e_u$', r'$e_\phi$']

    def plot_matrix_with_numbers(mat, title, filename):
        plt.figure(figsize=(6.2, 5.4))
        im = plt.imshow(np.asarray(mat), interpolation='nearest', aspect='equal')  # default colormap
        plt.title(title, fontsize=14)
        plt.xticks(np.arange(5), var_labels, rotation=0)
        plt.yticks(np.arange(5), var_labels)
        cbar = plt.colorbar(im, fraction=0.046, pad=0.04)
        cbar.ax.set_ylabel('Mean value', rotation=90, va='center')

        # Add numeric annotations in each cell
        mat_np = np.asarray(mat)
        vmin, vmax = float(np.min(mat_np)), float(np.max(mat_np))
        mid = (vmin + vmax) / 2.0
        for i in range(mat_np.shape[0]):
            for j in range(mat_np.shape[1]):
                val = mat_np[i, j]
                # Choose annotation color for contrast
                txt_color = 'white' if val > mid else 'black'
                plt.text(j, i, f"{val:.3g}", ha='center', va='center', color=txt_color, fontsize=10)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, filename), dpi=300)

    plot_matrix_with_numbers(J_mean, "Mean J matrix (dataset-wide)", "J_mean_matrix.png")
    plot_matrix_with_numbers(R_mean, "Mean R matrix (dataset-wide)", "R_mean_matrix.png")
    # -------------------------------------------------------------------------

    plt.close('all')
    print(f"All plots saved to {output_dir}")

    # ===== SAVE SPECIFIC PLOTTING DATA =====

    def _to_numpy_safe(x):
        """Convert JAX/NumPy array-likes to plain NumPy (no copy if not needed)."""
        import jax.numpy as jnp
        import numpy as np
        if isinstance(x, jnp.ndarray):
            return np.asarray(x)
        return np.asarray(x)

    # Determine repo root and output path
    OUT_PATH = os.path.join(Path(__file__).resolve().parents[2] / "results" / "PINN Data", "pinn_plot_data.pkl")

    print(f"\nSaving specific data required for plots to {OUT_PATH}...")

    # Explicitly define the payload with only the data used in the plots
    payload = {
        # --- Time Vector & Plotting Range ---
        't_test': _to_numpy_safe(t_test),
        'split_start': split_start,
        'split_end': split_end,

        # --- Loss Histories ---
        'train_losses': _to_numpy_safe(train_losses),
        'val_losses': _to_numpy_safe(val_losses),
        'hamiltonian_losses': _to_numpy_safe(hamiltonian_losses),
        'phys_losses': _to_numpy_safe(phys_losses),
        'conservative_losses': _to_numpy_safe(conservative_losses),
        'dissipative_losses': _to_numpy_safe(dissipative_losses),
        'data_unified_losses': _to_numpy_safe(data_unified_losses),
        'physics_residual_losses': _to_numpy_safe(physics_residual_losses),
        'j_structure_losses': _to_numpy_safe(j_structure_losses),
        'r_structure_losses': _to_numpy_safe(r_structure_losses),

        # --- Hamiltonian Plot Data (Plot 1) ---
        'H_analytical_vis': _to_numpy_safe(H_analytical_vis),
        'H_learned_aligned': _to_numpy_safe(H_learned_aligned),

        # --- Derivative Fidelity Plot Data (Plots 3 & 4) ---
        'e_dot_test': _to_numpy_safe(e_dot_test),
        'e_dot_autodiff': _to_numpy_safe(e_dot_autodiff),
        'e_dot_from_equations': _to_numpy_safe(e_dot_from_equations),
        'e_dot_from_structure': _to_numpy_safe(e_dot_from_structure),
        'x_dot_test': _to_numpy_safe(x_dot_test),
        'x_dot_autodiff': _to_numpy_safe(x_dot_autodiff),
        'x_dot_from_vectorfield_vis': _to_numpy_safe(x_dot_from_vectorfield_vis),

        # --- State Trajectory Plot Data (Plots 5 & 6) ---
        'e_test': _to_numpy_safe(e_test),
        'e_pred': _to_numpy_safe(e_pred),
        'x_test': _to_numpy_safe(x_test),
        'x_pred': _to_numpy_safe(x_pred),

        # --- dH/dt Comparison Plot Data (Plot 7) ---
        'dHdt_analytical_vis': _to_numpy_safe(dHdt_analytical_vis),
        'dHdt_pred_true': _to_numpy_safe(dHdt_pred_true),
        'dHdt_pred_structure': _to_numpy_safe(dHdt_pred_structure),
        'dHdt_pred_autodiff': _to_numpy_safe(dHdt_pred_autodiff),
        'dHdt_pred_pH': _to_numpy_safe(dHdt_pred_pH),
    }

    # Save the curated data to the pickle file
    with open(OUT_PATH, "wb") as f:
        pickle.dump(payload, f)

    print(f"‚úÖ Saved plotting data -> {OUT_PATH}")
    # ===== END SAVE BLOCK =====


if __name__ == "__main__":
    main()


```

#### File: `read_best_hyperparams.py`
```python
import optuna
import os
import argparse


def main():
    """
    Loads a specified Optuna study from a SQLite database file, prints the
    details of the best trial, and saves them to a text file.
    """
    # --- 1. Argument Parsing ---
    parser = argparse.ArgumentParser(
        description="Load an Optuna study and print the best hyperparameters."
    )
    parser.add_argument(
        '--objective',
        type=str,
        default='h_loss',
        choices=['val_loss', 'h_loss'],
        help="The objective metric of the study to load ('val_loss' or 'h_loss')."
    )
    args = parser.parse_args()

    # --- 2. Define Study Details ---
    # These must match the values used in your optimize_hyperparams.py script.
    results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'results', 'PINN Data')
    db_name = 'optimize_hyperparams.db'
    db_path = os.path.join(results_dir, db_name)
    storage_name = f"sqlite:///{db_path}"

    # Dynamically set the study name based on the chosen objective
    study_name = f"sphnn_pinn_optimization_{args.objective}"

    # --- 3. Load the Study ---
    print(f"Loading study '{study_name}' from {storage_name}...")
    try:
        study = optuna.load_study(study_name=study_name, storage=storage_name)
    except KeyError:
        print(f"\nError: Study '{study_name}' not found in the database.")
        print(f"Please ensure you have run the optimization with '--objective {args.objective}'")
        print(f"And that the database file exists at: {db_path}")
        return
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        return

    # --- 4. Get the Best Trial and Print Results ---
    best = study.best_trial

    print("\n" + "=" * 50)
    print(f"        Best Trial Found for '{study_name}'")
    print("=" * 50)
    print(f"  Trial Number: {best.number}")
    print(f"  Best Value ({args.objective}): {best.value:.6f}")
    print("\n  Best Hyperparameters:")
    for key, value in best.params.items():
        print(f"    '{key}': {value},")
    print("=" * 50)

    # --- 5. Save the Best Hyperparameters to a File ---
    output_txt_path = os.path.join(results_dir, f"best_hyperparams_{args.objective}.txt")
    with open(output_txt_path, 'w') as f:
        f.write(f"Best Hyperparameter Optimization Results for Study: '{study_name}'\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Best Value ({args.objective}): {best.value}\n\n")
        f.write("Best Hyperparameters:\n")
        for key, value in best.params.items():
            f.write(f"    '{key}': {value},\n")

    print(f"\n‚úÖ Best hyperparameters have been saved to: {output_txt_path}")


if __name__ == "__main__":
    main()

```

#### File: `test_gpu.py`
```python
import jax
import sys

print(f"JAX running on: {jax.default_backend()}")
if jax.default_backend() != 'gpu':
    print("WARNING: JAX is not using the GPU. Check your JAX installation and CUDA setup.", file=sys.stderr)
else:
    print("‚úÖ JAX is using the GPU.")

# You can also list all available devices
print("Available devices:")
for device in jax.devices():
    print(f"- {device}")

```

================================================================================
## Folder: src/sph_pinn/.ipynb_checkpoints
------------------------------------------------------------